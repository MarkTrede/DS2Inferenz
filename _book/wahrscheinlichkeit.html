<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Kapitel 3 Wahrscheinlichkeit | Data Science 2: Statistische Inferenz</title>
<meta name="author" content="Mark Trede">
<meta name="description" content="3.1 Definition Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung (Funktion) angegeben. Eine Abbildung \(P:...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Kapitel 3 Wahrscheinlichkeit | Data Science 2: Statistische Inferenz">
<meta property="og:type" content="book">
<meta property="og:description" content="3.1 Definition Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung (Funktion) angegeben. Eine Abbildung \(P:...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kapitel 3 Wahrscheinlichkeit | Data Science 2: Statistische Inferenz">
<meta name="twitter:description" content="3.1 Definition Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung (Funktion) angegeben. Eine Abbildung \(P:...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Data Science 2: Statistische Inferenz</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled"><li><a class="" href="index.html"><span class="header-section-number">Kapitel 3</span> Wahrscheinlichkeit</a></li></ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="wahrscheinlichkeit" class="section level1" number="3">
<h1>
<span class="header-section-number">Kapitel 3</span> Wahrscheinlichkeit<a class="anchor" aria-label="anchor" href="#wahrscheinlichkeit"><i class="fas fa-link"></i></a>
</h1>
<div id="def:wahrscheinlichkeit" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Definition<a class="anchor" aria-label="anchor" href="#def:wahrscheinlichkeit"><i class="fas fa-link"></i></a>
</h2>
<p>Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch
die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine
Abbildung (Funktion) angegeben.</p>
<blockquote>
<p>Eine Abbildung <span class="math inline">\(P: A \mapsto P(A)\)</span>
heißt <strong>Wahrscheinlichkeitsmaß</strong>, wenn gilt</p>
<ul>
<li><p>Nichtnegativität: <span class="math inline">\(P(A)\ge 0\)</span> für alle Ereignisse <span class="math inline">\(A\)</span></p></li>
<li><p>Normierung: <span class="math inline">\(P(\Omega)=1\)</span></p></li>
<li><p>Additivität: Für disjunkte Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span>
(d.h. <span class="math inline">\(A\cap B=\emptyset\)</span>) ist <span class="math inline">\(P(A\cup B)=P(A)+P(B)\)</span></p></li>
</ul>
</blockquote>
<p>Diese Eigenschaften, die eine Wahrscheinlichkeitsabbildung erfüllen
muss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten
hergeleitet werden, daher spricht man auch von Axiomen. Etwas allgemeiner
und präziser wurden diese Axiome 1933 von Andrey Kolmogorov
für eine saubere mathematische Fundierung der Wahrscheinlichkeitstheorie
eingeführt.</p>
<p>Aus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum Beispiel</p>
<ul>
<li><p>Monotonität: Wenn <span class="math inline">\(A\subseteq B\)</span>, dann ist <span class="math inline">\(P(A)\le P(B)\)</span></p></li>
<li><p>Komplementärereignis: <span class="math inline">\(P(\bar A)=1-P(A)\)</span></p></li>
<li><p>Additionssatz: <span class="math inline">\(P(A\cup B)=P(A)+P(B)-P(A\cap B)\)</span></p></li>
</ul>
</div>
<div id="venn" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Venn-Diagramme<a class="anchor" aria-label="anchor" href="#venn"><i class="fas fa-link"></i></a>
</h2>
<p>Ereignisse lassen sich - wie andere Mengen in der Mengenlehre auch - in
Form von Venn-Diagrammen darstellen. Wenn die Ergebnismenge <span class="math inline">\(\Omega\)</span>
durch ein Rechteck repräsentiert wird, dann kann man Ereignisse
als Teilmengen des Rechtecks darstellen. In dem folgenden Bild ist zum
Beispiel das Ereignis <span class="math inline">\(A\)</span> der kleine Kreis in der Mitte.
<img src="02-wahrscheinlichkeit_files/figure-html/unnamed-chunk-1-1.png" width="95%"></p>
<p>In diesem Venn-Diagramm wird deutlich, dass die Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(C\)</span>
nicht gleichzeitig eintreten können, <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> hingegen schon, denn
ihre Schnittmenge ist nicht leer. Außerdem
erkennt man, dass <span class="math inline">\(A\)</span> eine kleinere Wahrscheinlichkeit hat als <span class="math inline">\(B\)</span> und <span class="math inline">\(C\)</span>.
Der Additionssatz <span class="math inline">\(P(A\cup B)=P(A)+P(B)-P(A\cap B)\)</span>
wird in dieser grafischen Sichtweise sehr einfach klar:
Die Wahrscheinlichkeit von <span class="math inline">\(A\cup B\)</span> (also <span class="math inline">\(A\)</span> oder <span class="math inline">\(B\)</span>) ergibt sich als
die Summe aus den beiden Kreisflächen <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> abzüglich der Fläche
der Schnittmenge, die sonst doppelt gezählt werden würde.</p>
</div>
<div id="laplace" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Laplace-Experimente<a class="anchor" aria-label="anchor" href="#laplace"><i class="fas fa-link"></i></a>
</h2>
<p>Eine besonders einfache Art von Zufallsvorgängen sind die sogenannten
Laplace-Experimente.</p>
<blockquote>
<p>Ein Zufallsvorgang heißt <strong>Laplace-Experiment</strong>, wenn es nur endlich viele
Ergebnisse gibt (d.h. wenn <span class="math inline">\(|\Omega|=n\)</span>) und wenn alle
Elementarereignisse als gleich wahrscheinlich angenommen werden können.</p>
</blockquote>
<div class="inline-figure">Beachten Sie, dass es sich
dabei um eine Aussage handelt, die aus unserem Alltagswissen herrührt,
nicht aus mathematischen Überlegungen oder Herleitungen.
<img src="images/kronkorken.jpg" align="right" width="20%">
Ein typisches, einfaches Beispiel für ein Laplace-Experiment sind Würfelwürfe.
Es ist aus unserem Alltagswissen heraus plausibel, davon auszugehen,
dass alle Augenzahlen eines Würfels gleich wahrscheinlich sind. Auch
bei einer Münze ist es naheliegend, dass die Wahrscheinlichkeit für
Kopf und die Wahrscheinlichkeit für Zahl gleich sind. Hingegen würde
man beim Werfen eines Kronkorkens nicht unbedingt vermuten, dass beide Seiten
mit der gleichen Wahrscheinlichkeit oben liegen. Hier liegt also kein
Laplace-Experiment vor.</div>
<p>Bei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein
Ergebnis <span class="math inline">\(A\)</span> eintritt leicht zu ermitteln. Sie beträgt
<span class="math display">\[
P(A)=\frac{|A|}{|\Omega|},
\]</span>
also die Anzahl der Ergebnisse in <span class="math inline">\(A\)</span> dividiert durch die Anzahl aller
Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss man
also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist manchmal
sehr einfach, kann aber bei großen Mengen kompliziert sein. In solchen
Fällen hilft der Teilbereich der Mathematik weiter, den man “Kombinatorik”
nennt. Wir gehen in diesem Kurs jedoch nicht näher auf
kombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei
denen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.
Die Anzahl der Ergebnisse in <span class="math inline">\(\Omega\)</span> beträgt 6. Sei <span class="math inline">\(A\)</span> das
Ereignis “Eine gerade Zahl wird geworfen,” also <span class="math inline">\(A=\{2,4,6\}\)</span>.
Dann ist
<span class="math display">\[
\begin{align*}
P(A)&amp;=\frac{|A|}{|\Omega|}\\
&amp;=\frac{3}{6}\\
&amp;=0.5.
\end{align*}
\]</span></p></li>
<li>
<p>Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine Rolle,
wie man die Ergebnismenge festlegt. Wir wählen
<span class="math display">\[
\begin{align*}
\Omega=\{&amp;11,12,13,14,15,16,\\
&amp;21,22,23,24,25,26,\\
&amp;31,32,33,34,35,36,\\
&amp;41,42,43,44,45,46,\\
&amp;51,52,53,54,55,56,\\
&amp;61,62,63,64,65,66\}
\end{align*}
\]</span>
weil es sich dann um ein Laplace-Experiment handelt. Die Ergebnisse
“24” und “42” sind zwar nicht unterscheidbar, wenn die Würfel gleich
aussehen, aber wir können den zuerst geworfenen Würfel (oder den
weiter links liegenden) als ersten Würfel bezeichnen. Dass alle 36
Ergebnisse gleich wahrscheinlich sind, können wir mit unserem
Alltagswissen begründen, nicht aus der Mathematik heraus.</p>
<p>Sei <span class="math inline">\(A\)</span> das Ereignis “Die Augenzahlen der beiden Würfel unterscheiden
sich um 2,” d.h. <span class="math inline">\(A=\{13,24,31,35,42,46,53,64\}\)</span>. Dann gilt
<span class="math display">\[
\begin{align*}
P(A)&amp;=\frac{|A|}{|\Omega|}\\
&amp;=\frac{8}{36}\\
&amp;=\frac{2}{9}\\
&amp;\approx 0.2222.
\end{align*}
\]</span>
Als Ergebnismenge wäre auch
<span class="math display">\[
\begin{align*}
\Omega=\{
&amp;11,12,13,14,15,16,\\
&amp;22,23,24,25,26,\\
&amp;33,34,35,36,\\
&amp;44,45,46,\\
&amp;55,56,\\
&amp;66\}
\end{align*}
\]</span>
möglich gewesen, aber dann wäre die Annahme eines Laplace-Experiments
nicht plausibel gewesen. Die Elementarereignisse “11” und “12” wären
beispielsweise nicht gleich wahrscheinlich (ein Pasch tritt
erfahrungsgemäß seltener auf).</p>
<p>Die Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere
Vorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge
so gewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte
man das tun.</p>
</li>
</ul>
</div>
<div id="bedingt" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Bedingte Wahrscheinlichkeit<a class="anchor" aria-label="anchor" href="#bedingt"><i class="fas fa-link"></i></a>
</h2>
<p>Manchmal gibt es begrenzte Informationen über einen Zufallsvorgang.
Dann kennt man nicht das realisierte Ergebnis, kann aber die Menge der
möglichen Ergebnisse eingrenzen. Dadurch ändern sich die
Wahrscheinlichkeiten für Ereignisse.</p>
<blockquote>
<p>Wir betrachten zwei Ereignsse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> mit <span class="math inline">\(P(B)&gt;0\)</span>.
Dann heißt
<span class="math display">\[
P(A|B)=\frac{P(A\cap B)}{P(B)}
\]</span>
die <strong>bedingte Wahrscheinlichkeit</strong> von <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span></p>
</blockquote>
<p>Die Notation <span class="math inline">\(A|B\)</span> steht nicht für ein bestimmtes Ereignis, sondern
zeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten,
nämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über
Wahrscheinlichkeiten ist es nicht immer einfach (aber wichtig!),
zwischen der Wahrscheinlichkeit <span class="math inline">\(P(A\cap B)\)</span> und der bedingten
Wahrscheinlichkeit <span class="math inline">\(P(A|B)\)</span> zu unterscheiden. Wenn man <span class="math inline">\(P(A\cap B)\)</span>
meint, spricht man von der Wahrscheinlichkeit für <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span>.
Wenn man <span class="math inline">\(P(A|B)\)</span> meint, sagt man <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span>, oder: <span class="math inline">\(A\)</span> wenn <span class="math inline">\(B\)</span>,
oder: <span class="math inline">\(A\)</span> unter der Bedingung <span class="math inline">\(B\)</span>. Wenn man ausdrücklich angeben
möchte, dass eine Wahrscheinlichkeit <em>keine</em> bedingte Wahrscheinlichkeit
ist, nennt man sie auch eine unbedingte Wahrscheinlichkeit.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse
<span class="math inline">\(A=\{2,4,6\}\)</span> (“gerade Zahl”), und <span class="math inline">\(B=\{4,5,6\}\)</span> (“eine Zahl größer als 3”).
Die bedingte Wahrscheinlichkeit von <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span> beträgt
<span class="math display">\[
\begin{align*}
P(A|B)&amp;=\frac{P(A\cap B)}{P(B)}\\
&amp;=\frac{P(\{4,6\})}{P(\{4,5,6\})}\\
&amp;=\frac{2}{3}
\end{align*}
\]</span>
und die bedingte Wahrscheinlichkeit von <span class="math inline">\(B\)</span> gegeben <span class="math inline">\(A\)</span> lautet
<span class="math display">\[
\begin{align*}
P(B|A)&amp;=\frac{P(B\cap A)}{P(A)}\\
&amp;=\frac{P(\{4,6\})}{P(\{2,4,6\})}\\
&amp;=\frac{2}{3}.
\end{align*}
\]</span></p></li>
<li><p>Zwei Würfel werden geworfen. Sei <span class="math inline">\(A\)</span> das Ereignis “die Augensumme ist 10.”
Sei <span class="math inline">\(B\)</span> das Ereignis “einer der beiden Würfel (oder beide) zeigt eine 2.”
Dann ist
<span class="math display">\[
\begin{align*}
P(A|B)&amp;=\frac{P(A\cap B)}{P(B)}\\
&amp;=\frac{P(\{\})}{P(\{21,22,23,24,25,26,12,32,42,52,62\})}\\
&amp;=0.
\end{align*}
\]</span>
Die bedingte Wahrscheinlichkeit für das Ereignis <span class="math inline">\(C\)</span>: “Augensumme ist 5”
unter der Bedingung <span class="math inline">\(B\)</span> ist
<span class="math display">\[
\begin{align*}
P(C|B)&amp;=\frac{P(C\cap B)}{P(B)}\\
&amp;=\frac{P(\{23,32\})}{P(\{21,22,23,24,25,26,12,32,42,52,62\})}\\
&amp;=\frac{2}{11}.
\end{align*}
\]</span></p></li>
<li><p>Bei einer Umfrage werden Personen zufällig ausgewählt und befragt. Sei <span class="math inline">\(A\)</span>
das Ereignis “die befragte Person ist weiblich.” Sei <span class="math inline">\(B\)</span> das Ereignis
“die befragte Person arbeitet in Teilzeit.” Zwischen der (unbedingten)
Wahrscheinlichkeit <span class="math inline">\(P(B)\)</span> und der bedingten Wahrscheinlichkeit <span class="math inline">\(P(B|A)\)</span>
besteht ein Unterschied. Die Wahrscheinlichkeit <span class="math inline">\(P(B)\)</span> steht dafür, dass
eine zufällig ausgewählte Person in Teilzeit arbeitet, und diese Person
kann männlich oder weiblich sein. Dagegen ist <span class="math inline">\(P(B|A)\)</span> die
Wahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit
arbeitet.</p></li>
</ul>
<p>In den Medien werden häufig bedingte Wahrscheinlichkeiten berichtet,
ohne dass das explizit erwähnt wird. Oftmals sind die berichteten
bedingten Wahrscheinlichkeiten gar nicht die, für die man sich
eigentlich interessiert, weil die Bedingung und das Bedingte
quasi falsch herum angeordnet sind. Man interessiert sich für <span class="math inline">\(P(A|B)\)</span>,
aber berichtet wird <span class="math inline">\(P(B|A)\)</span>.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>In den Medien wird berichtet, dass ein Corona-Schnelltest mit einer
Wahrscheinlichkeit von xxx Prozent ein falsch positives Ergebnis liefert.
Falsch positiv bedeutet, die Wahrscheinlichkeit wird unter der Bedingung
angegeben, dass die getestete Person negativ ist. Wenn man Test an sich
durchführt, interessiert man sich dagegen für die bedingte
Wahrscheinlichkeit, infiziert zu sein, obwohl der Test negativ ausfällt,
und natürlich auch für die bedingte Wahrscheinlichkeit, tatsächlich
infiziert zu sein, wenn der Test positiv ausfällt.</p></li>
<li><p>In den Medien erfährt man, dass XX Prozent der Studierenden ein
“akademisches Elternhaus” haben. Es handelt sich um die bedingte
Wahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person
studiert. Für die Bewertung der Chancengleichheit ist hingegen die
bedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert, wenn
es aus einem akademischen Elternhaus stammt - und zwar im Vergleich
zu der bedingten Wahrscheinlichkeit, dass ein Kind studiert, wenn
es aus einem nicht-akademischen Elternhaus stammt.</p></li>
</ul>
<p>Wie kann man eine bedingte Wahrscheinlichkeit “umdrehen?” Wie verhält sich
<span class="math inline">\(P(B|A)\)</span> zu <span class="math inline">\(P(A|B)\)</span>? Das sehen wir im folgenden Abschnitt <a href="wahrscheinlichkeit.html#bayes">3.6</a>.</p>
<p>Bedingte Wahrscheinlichkeiten sind auch nützlich, um die Wahrscheinlichkeit
zu berechnen, dass mehrere Ereignisse gemeinsam passieren. Dazu formen wir
die Definition der bedingten Wahrscheinlichkeit einfach um. Aus
<span class="math display">\[
P(A|B)=\frac{P(A\cap B)}{P(B)}
\]</span>
wird
<span class="math display">\[
P(A\cap B)={P(B)P(A|B)}
\]</span>
Das lässt sich auf mehr als zwei Ereignisse erweitern:
<span class="math display">\[
\begin{align*}
P(A\cap B \cap C)&amp;=P(C)P(B|C)P(A|B\cap C)\\
P(A\cap B\cap C\cap D)&amp;=P(D)P(C|D)P(B|C\cap D)P(A|B\cap C\cap D)
\end{align*}
\]</span>
Diese Formeln sind bei näherer Betrachtung intuitiv: Die Wahrscheinlichkeit,
dass <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> und <span class="math inline">\(C\)</span> gemeinsam eintreten, ergibt sich, indem man zuerst
eine der unbedingten Wahrscheinlichkeiten nimmt (z.B. <span class="math inline">\(P(C)\)</span>).
Nun ist <span class="math inline">\(C\)</span> quasi eingetreten und wir arbeiten unter der Bedingung <span class="math inline">\(C\)</span>
weiter. Die Wahrscheinlichkeit <span class="math inline">\(P(C)\)</span> wird jetzt mit der
bedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,
also <span class="math inline">\(P(B|C)\)</span>. Nun sind <span class="math inline">\(B\)</span> und <span class="math inline">\(C\)</span> eingetreten, und auf der nächsten
Stufe multiplizieren wir deshalb mit <span class="math inline">\(P(A|B\cap C)\)</span>. Eine andere
Reihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B.
<span class="math display">\[
P(A\cap B\cap C)=P(A)P(B|A)P(C|A\cap B).
\]</span></p>
</div>
<div id="total" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Totale Wahrscheinlichkeit<a class="anchor" aria-label="anchor" href="#total"><i class="fas fa-link"></i></a>
</h2>
<p>Aus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte
Wahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum <span class="math inline">\(\Omega\)</span>
in eine Partition. Unter einer Partition versteht man eine Zerlegung
in disjunkte Mengen <span class="math inline">\(A_1, A_2, \ldots, A_n\)</span>, so dass die Vereinigungsmenge
der <span class="math inline">\(A_1,\ldots, A_n\)</span> wieder <span class="math inline">\(\Omega\)</span> ergibt. Die “Partitionierung eines
Rinds” könnte etwa so aussehen:</p>
<div class="inline-figure"><img src="images/partitionrind.jpg" width="100%"></div>
<p>Das Rind wird also vollständig Teilmengen zerlegt, und alle Teilmengen
zusammen ergeben wieder das gesamte Rind.</p>
<p>Nun nehmen wir an, dass für jedes Ereignis <span class="math inline">\(A_i\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, die
bedingte Wahrscheinlichkeit <span class="math inline">\(P(B|A_i\)</span> gegeben ist, wobei <span class="math inline">\(B\)</span> irgendein
Ereignis ist. Dann gilt für die unbedingte Wahrscheinlichkeit von <span class="math inline">\(B\)</span>:
<span class="math display">\[
P(B)=\sum_{i=1}^n P(B|A_i)P(A_i).
\]</span>
Die unbedingte Wahrscheinlichkeit ergibt sich also als gewichtete Summe
aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch die
Wahrscheinlichkeiten der bedingenden Ereignisse.</p>
<p><strong>Beispiele:</strong></p>
</div>
<div id="bayes" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> Satz von Bayes<a class="anchor" aria-label="anchor" href="#bayes"><i class="fas fa-link"></i></a>
</h2>
<p>Der Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten
<span class="math inline">\(P(A|B)\)</span> und <span class="math inline">\(P(B|A)\)</span> zueinander in Beziehung. Es gilt
<span class="math display">\[
P(A|B)=\frac{P(B|A)P(A)}{P(B)}.
\]</span>
Um die Bedingung und das Bedingte zu vertauschen, braucht man also beide
unbedingte Wahrscheinlichkeiten. Die Herleitung des Satzes von Bayes
ergibt sich aus der Definition der bedingten Wahrscheinlichkeit.
Wegen
<span class="math display">\[
\begin{align*}
P(A|B) &amp;= \frac{P(A\cap B)}{P(B)}\\
P(B|A) &amp;= \frac{P(A\cap B)}{P(A)}
\end{align*}
\]</span>
gilt
<span class="math display">\[
P(A|B)P(B)=P(B|A)P(A).
\]</span>
Daraus folgt unmittelbar der Satz von Bayes.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="empty"></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Science 2: Statistische Inferenz</strong>" was written by Mark Trede. It was last built on 2021-12-29.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
