# Wahrscheinlichkeit {#sec-wahrscheinlichkeit}

## Definition {#sec-definition}

Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die
Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung
(Funktion) angegeben.

:::: callout-note
## Definition: Wahrscheinlichkeit
Eine Abbildung $P: A \mapsto P(A)$ heißt **Wahrscheinlichkeit**
(engl. probability), wenn gilt

- Nichtnegativität: $P(A)\ge 0$ für alle Ereignisse $A$

- Normierung: $P(\Omega)=1$

- Additivität: Für disjunkte Ereignisse $A$ und $B$ (d.h.
$A\cap B=\emptyset$) ist 
$$
P(A\cup B)=P(A)+P(B).
$$
:::

Diese Eigenschaften, die eine Wahrscheinlichkeit erfüllen
muss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten
hergeleitet werden, daher spricht man auch von **Axiomen**. Etwas
allgemeiner und präziser wurden diese Axiome 1933 
von Andrey Kolmogorov (1903-1987) für eine saubere mathematische 
Fundierung der Wahrscheinlichkeitstheorie eingeführt.

Aus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum
Beispiel:

-   Komplementärereignis: $P(\bar A)=1-P(A)$

-   Additionssatz: $P(A\cup B)=P(A)+P(B)-P(A\cap B)$

-   Monotonität: Wenn $A\subseteq B$, dann ist $P(A)\le P(B)$.

## Laplace-Experimente {#sec-laplace}

Eine besonders einfache Art von Zufallsvorgängen sind die sogenannten
Laplace-Experimente.

::: callout-note
## Definition: Laplace-Experiment
Ein Zufallsvorgang heißt **Laplace-Experiment** (engl. Laplace experiment), 
wenn es nur endlich viele Ergebnisse gibt (d.h. wenn $|\Omega|=n$) und wenn 
alle Elementarereignisse als gleich wahrscheinlich angenommen werden können.
:::

<img src="images/AdobeStock_332174058b_Kronkorken.jpeg" align="right" width="35%"/>
Beachten Sie, dass es sich dabei um eine Aussage handelt, die aus
unserem Alltagswissen herrührt, nicht aus mathematischen Überlegungen
oder Herleitungen! Ein
typisches, einfaches Beispiel für ein Laplace-Experiment sind
Würfelwürfe. Es ist aus unserem Alltagswissen heraus plausibel, davon
auszugehen, dass alle Augenzahlen eines normalen Würfels gleich wahrscheinlich
sind. Auch bei einer Münze ist es naheliegend, dass die
Wahrscheinlichkeit für Kopf und die Wahrscheinlichkeit für Zahl gleich
sind. Hingegen würde man beim Werfen eines Kronkorkens nicht unbedingt
vermuten, dass beide Seiten mit der gleichen Wahrscheinlichkeit oben
liegen. Hier liegt also kein Laplace-Experiment vor.

Bei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein
Ergebnis $A$ eintritt, leicht zu ermitteln. Sie beträgt 
$$
P(A)=\frac{|A|}{|\Omega|},
$$ 
also die Anzahl der Ergebnisse in $A$ dividiert durch die Anzahl
aller Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss
man also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist
manchmal sehr einfach, kann aber bei großen Mengen kompliziert sein. In
solchen Fällen hilft der Teilbereich der Mathematik weiter, den man
"Kombinatorik" nennt. Wir gehen in diesem Kurs jedoch nicht näher auf
kombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei
denen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.

::: {.callout-tip collapse="true"}
## Beispiel: Ein Würfel als Laplace-Experiment
Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.
Die Anzahl der Ergebnisse in $\Omega$ beträgt 6. Sei $A$ das
Ereignis "Eine gerade Zahl wird geworfen", also $A=\{2,4,6\}$. Dann
ist 

\begin{align*}
P(A)&=\frac{|A|}{|\Omega|}\\
&=\frac{3}{6}\\
&=0.5.
\end{align*}

:::

::: {.callout-tip collapse="true"}
## Beispiel: Zwei Würfel als Laplace-Experiment
Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine
Rolle, wie man die Ergebnismenge festlegt. Wir wählen 

\begin{align*}
\Omega=\{&11,12,13,14,15,16,\\
&21,22,23,24,25,26,\\
&31,32,33,34,35,36,\\
&41,42,43,44,45,46,\\
&51,52,53,54,55,56,\\
&61,62,63,64,65,66\}
\end{align*}

weil es sich dann um ein Laplace-Experiment handelt. Die
Ergebnisse "24" und "42" sind zwar nicht unterscheidbar, wenn die
Würfel gleich aussehen, aber wir können den zuerst geworfenen Würfel
(oder z.B. den weiter links liegenden) als ersten Würfel bezeichnen. Dass
alle 36 Ergebnisse gleich wahrscheinlich sind, können wir (nur) mit
unserem Alltagswissen begründen, nicht aus der Mathematik heraus.

Sei $A$ das Ereignis "Die Augenzahlen der beiden Würfel
unterscheiden sich um 2", d.h. 
$$
A=\{13,24,31,35,42,46,53,64\}.
$$ 
Dann gilt 
\begin{align*}
P(A)&=\frac{|A|}{|\Omega|}\\
&=\frac{8}{36}\\
&=\frac{2}{9}\\
&\approx 0.2222.
\end{align*}

Da die beiden Würfel nicht unterscheidbar sind, wäre als Ergebnismenge auch 

\begin{align*}
\Omega=\{
&11,12,13,14,15,16,\\
&22,23,24,25,26,\\
&33,34,35,36,\\
&44,45,46,\\
&55,56,\\
&66\}
\end{align*}

möglich gewesen. Die Ergebnismenge hätte dann 21 Elemente. Die Annahme 
eines Laplace-Experiments wäre jedoch in diesem Fall nicht
mehr korrekt, denn die Elementarereignisse "11" und "12" sind
beispielsweise nicht gleich wahrscheinlich. Wir wissen aus unserer
Erfahrung, dass ein Pasch seltener auftritt. 
:::

Die Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere
Vorgehen **möglichst einfach und elegant** ist. Wenn die Ergebnismenge so
gewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte man
das tun.

## Bedingte Wahrscheinlichkeit {#sec-bedingt}

Manchmal gibt es begrenzte Informationen über einen Zufallsvorgang. Dann
kennt man zwar nicht das realisierte Ergebnis, kann aber die Menge der
möglichen Ergebnisse eingrenzen. Dadurch ändern sich die
Wahrscheinlichkeiten für Ereignisse.

::: callout-note
## Definition: Bedingte Wahrscheinlichkeit
Wir betrachten zwei Ereignsse $A$ und $B$ mit $P(B)>0$. Dann heißt 
$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$ 
die **bedingte Wahrscheinlichkeit** (engl. conditional probability) von $A$ gegeben $B$.
:::

Die Notation $A|B$ steht nicht für ein bestimmtes Ereignis, sondern
zeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten,
nämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über
Wahrscheinlichkeiten ist es nicht immer einfach (aber sehr wichtig!),
zwischen der Wahrscheinlichkeit $P(A\cap B)$ und der bedingten
Wahrscheinlichkeit $P(A|B)$ zu unterscheiden. Wenn man $P(A\cap B)$
meint, spricht man von der Wahrscheinlichkeit, dass $A$ und $B$
eintreten. Wenn man $P(A|B)$ meint, sagt man $A$ gegeben $B$, oder: $A$
wenn $B$, oder: $A$ unter der Bedingung $B$. Wenn man ausdrücklich
angeben möchte, dass eine Wahrscheinlichkeit *keine* bedingte
Wahrscheinlichkeit ist, nennt man sie auch eine unbedingte
Wahrscheinlichkeit (engl. unconditional probability).

::: {.callout-tip collapse="true"}
## Beispiel: Bedingte Wahrscheinlichkeit für einen Würfel
Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse
$A=\{2,4,6\}$ ("gerade Zahl"), und $B=\{3,4,5,6\}$ ("eine Zahl größer
als 2"). Die bedingte Wahrscheinlichkeit von $A$ gegeben $B$ beträgt

\begin{align*}
P(A|B)&=\frac{P(A\cap B)}{P(B)}\\
&=\frac{P(\{4,6\})}{P(\{3,4,5,6\})}\\
&=\frac{1}{2}
\end{align*}

und die bedingte Wahrscheinlichkeit von $B$ gegeben $A$ lautet 

\begin{align*}
P(B|A)&=\frac{P(B\cap A)}{P(A)}\\
&=\frac{P(\{4,6\})}{P(\{2,4,6\})}\\
&=\frac{2}{3}.
\end{align*}

:::

::: {.callout-tip collapse="true"}
## Beispiel: Bedingte Wahrscheinlichkeit bei zwei Würfeln
Zwei Würfel werden geworfen, ohne dass Sie es sehen können.
Sie fragen, ob eine Sechs geworfen wurde. Die Frage wird
wahrheitsgemäß bejaht. Wie groß ist die Wahrscheinlichkeit,
dass ein 6er-Pasch geworfen wurde? 

Sei $A$ das Ereignis "6er-Pasch" und $B$ das Ereignis
"mindest eine Sechs wurde geworfen", d.h.

\begin{align*}
A &= \{66\}\\
B &= \{16,26,36,46,56,61,62,63,64,65,66\}.
\end{align*}

Daher gilt

\begin{align*}
P(A|B)&=\frac{P(A\cap B)}{P(B)}\\
&=\frac{P(\{66\})}{P(\{16,26,36,46,56,61,62,63,64,65,66\})}\\
&=1/11.
\end{align*}

:::

::: {.callout-tip collapse="true"}
## Beispiel: Erwerbsstatus und Geschlecht

<img src="images/AdobeStock_172088861b_Umfrage.jpeg" align="right" width=50%" style="padding-left:15px;padding-top:5px;padding-bottom:5px;"/>
Bei einer Umfrage werden Personen zufällig ausgewählt und befragt.
Sei $A$ das Ereignis "die befragte Person ist weiblich". Sei $B$ das
Ereignis "die befragte Person arbeitet in Teilzeit". Zwischen der
(unbedingten) Wahrscheinlichkeit $P(B)$ und der bedingten
Wahrscheinlichkeit $P(B|A)$ besteht ein Unterschied. Die
Wahrscheinlichkeit $P(B)$ steht dafür, dass eine zufällig
ausgewählte Person in Teilzeit arbeitet, und diese Person kann
männlich oder weiblich sein. Dagegen ist $P(B|A)$ die
Wahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit
arbeitet.
:::

In den Medien werden manchmal bedingte Wahrscheinlichkeiten berichtet,
ohne dass das explizit erwähnt wird. Oftmals sind die berichteten
bedingten Wahrscheinlichkeiten gar nicht die, für die man sich
eigentlich interessiert, weil die Bedingung und das Bedingte quasi
falsch herum angeordnet sind. Man interessiert sich für $P(A|B)$, 
berichtet wird aber $P(B|A)$.

::: {.callout-tip collapse="true"}
## Beispiel: Medizinischer Schnelltest
<img src="images/AdobeStock_338942366b_Medizintest.jpeg" align="right" width=50%" style="padding-left:15px;padding-top:5px;"/>Es wird berichtet, dass 
ein medizinischer Schnelltest mit einer
Wahrscheinlichkeit von 99 Prozent eine infizierte Person korrekt als
infiziert (positiv) erkennt und ebenfalls mit einer Wahrscheinlichkeit von 99
Prozent eine nicht infizierte Person als nicht infiziert erkennt
(negativ). Sei $A$ das Ereignis "infiziert" und $B$ das Ereignis "Test positiv". 
Die angegebenen Wahrscheinlichkeiten sind bedingte Wahrscheinlichkeiten,
und zwar $P(B|A)=0.99$ und $P(\bar B|\bar A)=0.99$. Als Testanwender 
interessiert man sich jedoch eher für die
bedingte Wahrscheinlichkeit, tatsächlich infiziert zu sein, 
wenn der Test positiv ausfällt, also für $P(A|B)$. Oder auch für die 
bedingte Wahrscheinlichkeit infiziert zu sein, obwohl der Test
negativ ausfällt, also $P(A|\bar B)$.
:::

::: {.callout-tip collapse="true"}
## Beispiel: Studierneigung und Elternhaus
Es wird berichtet, dass 50 Prozent der Studierenden ein
"akademisches Elternhaus" haben (die Zahlen in diesem
Beispiel sind fiktiv). Es handelt sich um die bedingte
Wahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person
studiert. Hier sind die Ereignisse $A$: "Kind studiert", und
$B$: "akademisches Elternhaus". Gegeben ist $P(B|A)=0.5$.
Für die Bewertung der Chancengleichheit ist jedoch die
bedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert,
wenn es aus einem akademischen Elternhaus stammt, also
$P(A|B)$ - und zwar im Vergleich zu der bedingten Wahrscheinlichkeit, 
dass ein Kind studiert, das nicht aus einem akademischen Elternhaus stammt,
also $P(A|\bar B)$.
:::

Wie kann man eine bedingte Wahrscheinlichkeit "umdrehen"? Wie verhält
sich $P(B|A)$ zu $P(A|B)?$ Das sehen wir in Kürze im @sec-bayes.

Bedingte Wahrscheinlichkeiten sind auch nützlich, um die
Wahrscheinlichkeit zu berechnen, dass mehrere Ereignisse gemeinsam
passieren. Dazu formen wir die Definition der bedingten
Wahrscheinlichkeit einfach um. Aus 
$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$ 
wird 
$$
P(A\cap B)={P(B)P(A|B)}.
$$
Das lässt sich auf mehr als zwei Ereignisse erweitern: 

\begin{align*}
P(A\cap B \cap C)&=P(C)P(B|C)P(A|B\cap C)\\
P(A\cap B\cap C\cap D)&=P(D)P(C|D)P(B|C\cap D)P(A|B\cap C\cap D)
\end{align*}

Diese Formeln sind bei näherer Betrachtung intuitiv: Die
Wahrscheinlichkeit, dass $A$, $B$ und $C$ gemeinsam eintreten, ergibt
sich, indem man zuerst eine der unbedingten Wahrscheinlichkeiten nimmt
(z.B. $P(C)$). Nun ist $C$ quasi eingetreten und wir arbeiten unter der
Bedingung $C$ weiter. Die Wahrscheinlichkeit $P(C)$ wird jetzt mit der
bedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,
also $P(B|C)$. Nun sind $B$ und $C$ eingetreten, und auf der nächsten
Stufe multiplizieren wir deshalb mit $P(A|B\cap C)$. Eine andere
Reihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B. 
$$
P(A\cap B\cap C)=P(A)P(B|A)P(C|A\cap B).
$$
Für jede Reihenfolge gelangt man zum gleichen Ergebnis.

::: {.callout-tip collapse="true"}
## Beispiel: Kündigungswahrscheinlichkeiten
<img src="images/AdobeStock_19427911b_Sparbuch.jpeg" align="right" width=50%" style="padding-left:15px;padding-top:5px;"/>
Eine Bank möchte die Dauer ihrer Geschäftsbeziehungen modellieren. 
Dazu definiert sie die Ereignisse $A_d$: "die Geschäftsbeziehung besteht seit $d$ Jahren"
für $d=0,1,\ldots,D$. Die Bank kennt (aus Erfahrung) die
Wahrscheinlichkeit für eine Kündigung in Abhängigkeit von der Dauer
(und geht davon aus, dass sich diese Wahrscheinlichkeiten im
Laufe der Zeit nicht verändern).
Mit $k_{d,d+1}$ wird die Wahrscheinlichkeit bezeichnet, dass {{< var ein >}}
{{< var Kunde >}}, {{< var der >}} schon seit $d$ Jahren {{< var Kunde >}}
ist, innerhalb des nächsten Jahres kündigt. 
Es handelt sich also um die bedingten Wahrscheinlichkeiten
$$
k_{d,d+1}=1-P(A_{d+1}|A_d).
$$
Die Wahrscheinlichkeit, dass {{< var ein >}} {{< var Neukunde >}} ($d=0$) nach 5 Jahren immer
noch {{< var Kunde >}} ist, beträgt

\begin{align*}
P(A_5)&=P(A_5|A_4)P(A_4|A_3)P(A_3|A_2)P(A_2|A_1)P(A_1|A_0)\\
&=(1-k_{4,5})(1-k_{3,4})(1-k_{2,3})(1-k_{1,2})(1-k_{0,1}).
\end{align*}

:::

## Totale Wahrscheinlichkeit {#sec-total}

Aus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte
Wahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum
$\Omega$ in eine Partition. Unter einer **Partition** versteht man eine
Zerlegung in disjunkte Mengen $A_1, A_2, \ldots, A_n$, so dass die
Vereinigungsmenge der $A_1,\ldots, A_n$ wieder $\Omega$ ergibt. Die
"Partitionierung eines Rinds" könnte etwa so aussehen:

```{r echo=FALSE,out.width="60%",fig.align='center'}
knitr::include_graphics("images/AdobeStock_101998556b_ButcherGuide.jpeg")
```

Das Rind wird also vollständig in Teilmengen aufgeteilt, und alle Teilmengen
zusammen ergeben wieder das gesamte Rind.

::: callout-important
## Satz von der totalen Wahrscheinlichkeit

Sei $A_1,\ldots,A_n$ eine Partition des Ergebnisraums $\Omega$.
Für jedes Ereignis $A_i$, $i=1,\ldots,n$, sei
die bedingte Wahrscheinlichkeit $P(B|A_i)$ gegeben, 
wobei $B$ irgendein Ereignis ist. 
Dann gilt für die **unbedingte (totale) Wahrscheinlichkeit** von $B$:
$$
P(B)=\sum_{i=1}^n P(B|A_i)P(A_i).
$$ 
:::

Zur Begründung: Da $A_1, A_2, \ldots, A_n$ eine Partition ist, gilt

\begin{align*}
P(B) &= P((B\cap A_1)\cup (B\cap A_2)\cup\ldots\cup(B\cap A_n))\\
&= P(B\cap A_1)+P(B\cap A_2)+\ldots+P(B\cap A_n)\\
&= P(B|A_1)P(A_1)+P(B|A_2)P(A_2)+\ldots+P(B|A_n)P(A_n).
\end{align*}

Eine besonders simple Partition ist die Unterteilung in
$A$ und $\bar A$. Dann lautet die Formel
$$
P(B)=P(B|A)P(A)+P(B|\bar A)P(\bar A).
$$ 

Die unbedingte Wahrscheinlichkeit ergibt sich als gewichtete
Summe aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch
die Wahrscheinlichkeiten der bedingenden Ereignisse. Dieser Zusammenhang 
ist dann besonders nützlich, wenn die bedingten Wahrscheinlichkeiten
schon bekannt oder einfach zu ermitteln sind, die unbedingte 
Wahrscheinlichkeit jedoch schwierig zu finden ist.

::: {.callout-tip collapse="true"}
## Beispiel: Kreditausfall und Konjunktur
Eine Bank vergibt einen Kredit an ein Unternehmen. Das Unternehmen
zahlt den Kredit mit einer Wahrscheinlichkeit von 0.95 zurück,
wenn die konjunkturelle Lage positiv ist. Bei einer schwachen
Konjunktur wird der Kredit jedoch nur mit einer Wahrscheinlichkeit
von 0.8 zurückgezahlt. Die Wahrscheinlichkeit einer guten
Konjunktur sei 0.75 (und die Konjunktur kann nur gut oder
schlecht sein).

Als Ereignisse definiert man $A$: "gute Konjunktur" und
$B$: "Kredit wird zurückgezahlt". Gegeben sind die bedingten Wahrscheinlichkeiten
$P(B|A)=0.95$ und $P(B|\bar A)=0.8$. Außerdem ist $P(A)=0.75$
bekannt. Damit ergibt sich die Wahrscheinlichkeit einer
Rückzahlung als

\begin{align*}
P(B)&=P(B|A)P(A)+P(B|\bar A)P(\bar A)\\
&=0.95\cdot 0.75+0.8\cdot 0.25\\
&=0.9125
\end{align*}

:::

## Satz von Bayes {#sec-bayes}

Der Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten
$P(A|B)$ und $P(B|A)$ zueinander in Beziehung. 

::: callout-important
## Satz von Bayes

Für zwei Ereignisse $A$ und $B$ mit $P(B)>0$ gilt der **Satz von Bayes**
(engl. Bayes theorem)
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}.
$$ 
:::

Um die Bedingung und das Bedingte zu vertauschen, braucht man also
die beiden unbedingten Wahrscheinlichkeiten. Die Herleitung des Satzes von
Bayes ergibt sich aus der Definition der bedingten Wahrscheinlichkeit.
Wegen 

\begin{align*}
P(A|B) &= \frac{P(A\cap B)}{P(B)}\\
P(B|A) &= \frac{P(A\cap B)}{P(A)}
\end{align*}

gilt 
$$
P(A|B)P(B)=P(B|A)P(A).
$$ 
Daraus folgt durch Umstellen unmittelbar der Satz von Bayes.

::: {.callout-tip collapse="true"}
## Beispiel: Medizinischer Schnelltest
Wir betrachten wieder den medizinischen Schnelltest und die beiden
Ereignisse $A$: "Person ist infiziert", und $B$: "Test positiv". Neben
den beiden bedingten Wahrscheinlichkeiten $P(B|A)=0.99$ und 
$P(\bar B|\bar A)=0.99$ sei bekannt, dass $P(A)=0.001$, d.h. nur mit
einer Wahrscheinlichkeit von 0.1 Prozent ist eine zufällig aus der
Population ausgewählte Person infiziert. 

Wie groß ist Wahrscheinlichkeit, dass eine Person infiziert ist, bei
der der Schnelltest ein positives Ergebnis zeigt? Gesucht ist also
$P(A|B)$. Nach dem Satz von Bayes gilt
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}.
$$
Nach dem Satz der totalen Wahrscheinlichkeit berechnet man

\begin{align*}
P(B) &=P(B|A)P(A)+P(B|\bar A)P(\bar A)\\
&= P(B|A)P(A)+[1-P(\bar B|\bar A)][1-P(A)]\\
&= 0.99\cdot 0.001+[1-0.99]\cdot [1-0.001]\\
&= 0.01098.
\end{align*}

Die Wahrscheinlichkeit, dass eine Person mit einem positiven Testergebnis
tatsächlich infiziert ist, ist folglich immer noch sehr klein, obwohl der
Test scheinbar sehr genau arbeitet. 
:::

## Unabhängigkeit {#sec-unabhaengig1}

::: callout-note
## Definition: Unabhängigkeit

Zwei Ereignisse $A$ und $B$ heißen **unabhängig** 
(oder auch **stochastisch unabhängig**, 
engl. independent bzw. stochastically independent), wenn 
$$
P(A\cap B)=P(A)\cdot P(B).
$$ 
:::

Eine alternative (fast äquivalente) Definition der Unabhängigkeit
lautet so: Die Ereignisse $A$ und $B$ heißen unabhängig, wenn gilt 
$$
P(A|B)=P(A)
$$ 
bzw. wenn 
$$
P(B|A)=P(B).
$$ 
Die Definition über die bedingten Wahrscheinlichkeiten ist etwas
intuitiver. Wenn das Wissen darüber, ob $A$ eingetreten ist oder nicht,
für die Wahrscheinlichkeit von $B$ keine Rolle spielt (und umgekehrt),
dann sind $A$ und $B$ unabhängig.

Achten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln.
Zur Erinnerung: Die Ereignisse $A$ und $B$ sind disjunkt, wenn
$A\cap B=\emptyset$, so dass $P(A\cap B)=0$. Disjunkte Ereignisse können
also nicht beide zusammen eintreten. Wenn $A$ eintritt, kann man sicher
sein, dass $B$ nicht eintritt. Und wenn $B$ eintritt, kann man sicher
sein, dass $A$ nicht eintritt. Unabhängige Ereignisse können hingegen
sehr wohl zusammen eintreten.

::: {.callout-tip collapse="true"}
## Beispiel: Medizinischer Schnelltest
Wir betrachten wieder den medizinischen Schnelltest und die beiden
Ereignisse $A$: "Person ist infiziert", und $B$: "Test positiv".
Zur Erinnerung: Gegeben sind die beiden bedingten Wahrscheinlichkeiten
$P(B|A)=0.99$ und $P(\bar B|\bar A)=0.99$ sowie die
unbedingte Wahrscheinlichkeit $P(A)=0.001$. Außerdem haben wir
im letzten Abschnitt $P(B)=0.01098$ berechnet. 

Sind die Ereignisse $A$ und $B$ unabhängig? Die Antwort lautet "nein",
denn es gilt
$$
0.99=P(B|A)\neq P(B)=0.01098.
$$
Unabhängigkeit ist in diesem Fall natürlich auch nicht zu erwarten.
Ein Schnelltest, dessen Ergebnis keinen Einfluss auf die 
Einschätzung der Wahrscheinlichkeit einer Infektion hätte, wäre
vollkommen nutzlos.
:::
