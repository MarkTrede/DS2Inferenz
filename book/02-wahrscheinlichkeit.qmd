# Wahrscheinlichkeit {#wahrscheinlichkeit}

## Definition {#def:wahrscheinlichkeit}

Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die
Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung
(Funktion) angegeben.

::: callout-note
Eine Abbildung $P: A \mapsto P(A)$ heißt **Wahrscheinlichkeitsmaß**
(engl. probability mass) oder kurz Wahrscheinlichkeit, wenn gilt

-   Nichtnegativität: $P(A)\ge 0$ für alle Ereignisse $A$

-   Normierung: $P(\Omega)=1$

-   Additivität: Für disjunkte Ereignisse $A$ und $B$ (d.h.
    $A\cap B=\emptyset$) ist 
    $$
    P(A\cup B)=P(A)+P(B).
    $$
:::

Diese Eigenschaften, die eine Wahrscheinlichkeitsabbildung erfüllen
muss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten
hergeleitet werden, daher spricht man auch von **Axiomen**. Etwas
allgemeiner und präziser wurden diese Axiome 1933 von Andrey Kolmogorov (1903-1987)
für eine saubere mathematische Fundierung der Wahrscheinlichkeitstheorie
eingeführt.

Aus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum
Beispiel

-   Monotonität: Wenn $A\subseteq B$, dann ist $P(A)\le P(B)$

-   Komplementärereignis: $P(\bar A)=1-P(A)$

-   Additionssatz: $P(A\cup B)=P(A)+P(B)-P(A\cap B)$.

## Venn-Diagramme {#venn}

Ereignisse lassen sich - wie andere Mengen in der Mengenlehre auch - in
Form von Venn-Diagrammen darstellen. Wenn die Ergebnismenge $\Omega$
durch ein Rechteck repräsentiert wird, dann kann man Ereignisse als
Teilmengen des Rechtecks darstellen. In dem folgenden Bild ist zum
Beispiel das Ereignis $A$ der kleine Kreis in der Mitte.

```{r}
#| out.width: "95%"
#| echo: FALSE
  
library(plotrix)
par(mar=c(0.1,0.1,0.1,0.1))
plot(c(0,1),c(0,1),t="n", axes=F, xlab="",ylab="")
draw.circle(0.5,0.5,radius=0.1,col=NA)
draw.circle(0.65,0.65,radius=0.2,col=NA)
draw.circle(0.2,0.3,radius=0.2,col=NA)
box()
text(0.1,0.8,expression(Omega),cex=1.2)
text(0.5,0.5,"A",cex=1.2)
text(0.65,0.65,"B",cex=1.2)
text(0.2,0.3,"C",cex=1.2)
```

In diesem Venn-Diagramm wird deutlich, dass die Ereignisse $A$ und $C$
nicht gleichzeitig eintreten können, $A$ und $B$ hingegen schon, denn
ihre Schnittmenge ist nicht leer. Außerdem erkennt man, dass $A$ eine
kleinere Wahrscheinlichkeit hat als $B$ und $C$. Der Additionssatz
$P(A\cup B)=P(A)+P(B)-P(A\cap B)$ wird in dieser grafischen Sichtweise
sehr einfach klar: Die Wahrscheinlichkeit von $A\cup B$ (also $A$ oder
$B$) ergibt sich als die Summe aus den beiden Kreisflächen $A$ und $B$
abzüglich der Fläche der Schnittmenge, die sonst doppelt gezählt werden
würde.

## Laplace-Experimente {#laplace}

Eine besonders einfache Art von Zufallsvorgängen sind die sogenannten
Laplace-Experimente.

::: callout-note
Ein Zufallsvorgang heißt **Laplace-Experiment**, wenn es nur endlich
viele Ergebnisse gibt (d.h. wenn $|\Omega|=n$) und wenn alle
Elementarereignisse als gleich wahrscheinlich angenommen werden können.
:::

<img src="images/AdobeStock_332174058b.jpeg" align="right" width="35%"/>
Beachten Sie, dass es sich dabei um eine Aussage handelt, die aus
unserem Alltagswissen herrührt, nicht aus mathematischen Überlegungen
oder Herleitungen. Ein
typisches, einfaches Beispiel für ein Laplace-Experiment sind
Würfelwürfe. Es ist aus unserem Alltagswissen heraus plausibel, davon
auszugehen, dass alle Augenzahlen eines Würfels gleich wahrscheinlich
sind. Auch bei einer Münze ist es naheliegend, dass die
Wahrscheinlichkeit für Kopf und die Wahrscheinlichkeit für Zahl gleich
sind. Hingegen würde man beim Werfen eines Kronkorkens nicht unbedingt
vermuten, dass beide Seiten mit der gleichen Wahrscheinlichkeit oben
liegen. Hier liegt also kein Laplace-Experiment vor.

Bei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein
Ergebnis $A$ eintritt leicht zu ermitteln. Sie beträgt 
$$
P(A)=\frac{|A|}{|\Omega|},
$$ 
also die Anzahl der Ergebnisse in $A$ dividiert durch die Anzahl
aller Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss
man also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist
manchmal sehr einfach, kann aber bei großen Mengen kompliziert sein. In
solchen Fällen hilft der Teilbereich der Mathematik weiter, den man
"Kombinatorik" nennt. Wir gehen in diesem Kurs jedoch nicht näher auf
kombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei
denen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.

::: callout-tip
-   Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.
    Die Anzahl der Ergebnisse in $\Omega$ beträgt 6. Sei $A$ das
    Ereignis "Eine gerade Zahl wird geworfen", also $A=\{2,4,6\}$. Dann
    ist 
    $$
    \begin{align*}
    P(A)&=\frac{|A|}{|\Omega|}\\
    &=\frac{3}{6}\\
    &=0.5.
    \end{align*}
    $$

-   Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine
    Rolle, wie man die Ergebnismenge festlegt. Wir wählen 
    $$
    \begin{align*}
    \Omega=\{&11,12,13,14,15,16,\\
    &21,22,23,24,25,26,\\
    &31,32,33,34,35,36,\\
    &41,42,43,44,45,46,\\
    &51,52,53,54,55,56,\\
    &61,62,63,64,65,66\}
    \end{align*}
    $$ 
    weil es sich dann um ein Laplace-Experiment handelt. Die
    Ergebnisse "24" und "42" sind zwar nicht unterscheidbar, wenn die
    Würfel gleich aussehen, aber wir können den zuerst geworfenen Würfel
    (oder z.B. den weiter links liegenden) als ersten Würfel bezeichnen. Dass
    alle 36 Ergebnisse gleich wahrscheinlich sind, können wir (nur) mit
    unserem Alltagswissen begründen, nicht aus der Mathematik heraus.
    Sei $A$ das Ereignis "Die Augenzahlen der beiden Würfel
    unterscheiden sich um 2", d.h. 
    $$
    A=\{13,24,31,35,42,46,53,64\}.
    $$ 
    Dann gilt 
    $$
    \begin{align*}
    P(A)&=\frac{|A|}{|\Omega|}\\
    &=\frac{8}{36}\\
    &=\frac{2}{9}\\
    &\approx 0.2222.
    \end{align*}
    $$ 
    Da die beiden Würfel nicht unterscheidbar sind, wäre als Ergebnismenge auch 
    $$
    \begin{align*}
    \Omega=\{
    &11,12,13,14,15,16,\\
    &22,23,24,25,26,\\
    &33,34,35,36,\\
    &44,45,46,\\
    &55,56,\\
    &66\}
    \end{align*}
    $$ 
    möglich gewesen. Dann wäre aber die Annahme eines Laplace-Experiments nicht
    mehr korrekt, denn die Elementarereignisse "11" und "12" sind
    beispielsweise nicht gleich wahrscheinlich (wir wissen aus unserer
    Erfahrung, dass ein Pasch seltener auftritt).
:::

Die Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere
Vorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge so
gewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte man
das tun.

## Bedingte Wahrscheinlichkeit {#bedingt}

Manchmal gibt es begrenzte Informationen über einen Zufallsvorgang. Dann
kennt man zwar nicht das realisierte Ergebnis, kann aber die Menge der
möglichen Ergebnisse eingrenzen. Dadurch ändern sich die
Wahrscheinlichkeiten für Ereignisse.

::: callout-note
Wir betrachten zwei Ereignsse $A$ und $B$ mit $P(B)>0$. Dann heißt 
$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$ 
die **bedingte Wahrscheinlichkeit** (engl. conditional probability) von $A$ gegeben $B$.
:::

Die Notation $A|B$ steht nicht für ein bestimmtes Ereignis, sondern
zeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten,
nämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über
Wahrscheinlichkeiten ist es nicht immer einfach (aber sehr wichtig!),
zwischen der Wahrscheinlichkeit $P(A\cap B)$ und der bedingten
Wahrscheinlichkeit $P(A|B)$ zu unterscheiden. Wenn man $P(A\cap B)$
meint, spricht man von der Wahrscheinlichkeit, dass $A$ und $B$
eintreten. Wenn man $P(A|B)$ meint, sagt man $A$ gegeben $B$, oder: $A$
wenn $B$, oder: $A$ unter der Bedingung $B$. Wenn man ausdrücklich
angeben möchte, dass eine Wahrscheinlichkeit *keine* bedingte
Wahrscheinlichkeit ist, nennt man sie auch eine unbedingte
Wahrscheinlichkeit (engl. unconditional probability).

::: callout-tip
-   Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse
    $A=\{2,4,6\}$ ("gerade Zahl"), und $B=\{4,5,6\}$ ("eine Zahl größer
    als 3"). Die bedingte Wahrscheinlichkeit von $A$ gegeben $B$ beträgt
    $$
    \begin{align*}
    P(A|B)&=\frac{P(A\cap B)}{P(B)}\\
    &=\frac{P(\{4,6\})}{P(\{4,5,6\})}\\
    &=\frac{2}{3}
    \end{align*}
    $$ 
    und die bedingte Wahrscheinlichkeit von $B$ gegeben $A$ lautet 
    $$
    \begin{align*}
    P(B|A)&=\frac{P(B\cap A)}{P(A)}\\
    &=\frac{P(\{4,6\})}{P(\{2,4,6\})}\\
    &=\frac{2}{3}.
    \end{align*}
    $$

-   Zwei Würfel werden geworfen, ohne dass Sie es sehen können.
Sie fragen, ob eine Sechs geworfen wurde. Die Frage wird
wahrheitsgemäß bejaht. Wie groß ist die Wahrscheinlichkeit,
dass ein 6er-Pasch geworfen wurde? 

Sei $A$ das Ereignis "6er-Pasch" und $B$ das Ereignis
"mindest eine Sechs wurde geworfen", d.h.
$$
\begin{align*}
A &= \{66\}\\
B &= \{16,26,36,46,56,61,62,63,64,65,66\}.
\end{align*}
$$
Daher gilt
$$
\begin{align*}
P(A|B)&=\frac{P(A\cap B)}{P(B)}\\
&=\frac{P(\{66\})}{P(\{16,26,36,46,56,61,62,63,64,65,66\})}\\
&=1/11.
\end{align*}
$$ 

-   Bei einer Umfrage werden Personen zufällig ausgewählt und befragt.
    Sei $A$ das Ereignis "die befragte Person ist weiblich". Sei $B$ das
    Ereignis "die befragte Person arbeitet in Teilzeit". Zwischen der
    (unbedingten) Wahrscheinlichkeit $P(B)$ und der bedingten
    Wahrscheinlichkeit $P(B|A)$ besteht ein Unterschied. Die
    Wahrscheinlichkeit $P(B)$ steht dafür, dass eine zufällig
    ausgewählte Person in Teilzeit arbeitet, und diese Person kann
    männlich oder weiblich sein. Dagegen ist $P(B|A)$ die
    Wahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit
    arbeitet.
:::

In den Medien werden häufig bedingte Wahrscheinlichkeiten berichtet,
ohne dass das explizit erwähnt wird. Oftmals sind die berichteten
bedingten Wahrscheinlichkeiten gar nicht die, für die man sich
eigentlich interessiert, weil die Bedingung und das Bedingte quasi
falsch herum angeordnet sind. Man interessiert sich für $P(A|B)$, 
berichtet wird aber $P(B|A)$.

::: callout-tip
-   Es wird berichtet, dass ein medizinischer Schnelltest mit einer
    Wahrscheinlichkeit von 99 Prozent eine infizierte Person korrekt als
    infiziert (positiv) erkennt und ebenfalls mit einer Wahrscheinlichkeit von 99
    Prozent eine nicht infizierte Person als nicht infiziert erkennt
    (negativ). Sei $A$ das Ereignis "infiziert" und $B$ das Ereignis "Test positiv". 
    Die angegebenen Wahrscheinlichkeiten sind bedingte Wahrscheinlichkeiten,
    und zwar $P(B|A)=0.99$ und $P(\bar B|\bar A)=0.99$. Als Testanwender 
    interessiert man sich jedoch eher für die
    bedingte Wahrscheinlichkeit, tatsächlich infiziert zu sein, 
    wenn der Test positiv ausfällt, also für $P(A|B)$. Oder auch für die 
    bedingte Wahrscheinlichkeit infiziert zu sein, obwohl der Test
    negativ ausfällt, also $P(A|\bar B)$.

-   Es wird berichtet, dass 50 Prozent der Studierenden ein
    "akademisches Elternhaus" haben (die Zahlen in diesem
    Beispiel sind fiktiv). Es handelt sich um die bedingte
    Wahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person
    studiert. Hier sind die Ereignisse $A$: "Kind studiert", und
    $B$: "akademisches Elternhaus". Gegeben ist $P(B|A)=0.5$.
    Für die Bewertung der Chancengleichheit ist jedoch die
    bedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert,
    wenn es aus einem akademischen Elternhaus stammt, also
    $P(A|B)$ - und zwar im Vergleich zu der bedingten Wahrscheinlichkeit, 
    dass ein Kind studiert, das nicht aus einem akademischen Elternhaus stammt,
    also $P(A|\bar B)$.
:::

Wie kann man eine bedingte Wahrscheinlichkeit "umdrehen"? Wie verhält
sich $P(B|A)$ zu $P(A|B)$? Das sehen wir in Kürze im Abschnitt \@ref(bayes).

Bedingte Wahrscheinlichkeiten sind auch nützlich, um die
Wahrscheinlichkeit zu berechnen, dass mehrere Ereignisse gemeinsam
passieren. Dazu formen wir die Definition der bedingten
Wahrscheinlichkeit einfach um. Aus 
$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$ 
wird 
$$
P(A\cap B)={P(B)P(A|B)}.
$$
Das lässt sich auf mehr als zwei Ereignisse erweitern: 
$$
\begin{align*}
P(A\cap B \cap C)&=P(C)P(B|C)P(A|B\cap C)\\
P(A\cap B\cap C\cap D)&=P(D)P(C|D)P(B|C\cap D)P(A|B\cap C\cap D)
\end{align*}
$$ 
Diese Formeln sind bei näherer Betrachtung intuitiv: Die
Wahrscheinlichkeit, dass $A$, $B$ und $C$ gemeinsam eintreten, ergibt
sich, indem man zuerst eine der unbedingten Wahrscheinlichkeiten nimmt
(z.B. $P(C)$). Nun ist $C$ quasi eingetreten und wir arbeiten unter der
Bedingung $C$ weiter. Die Wahrscheinlichkeit $P(C)$ wird jetzt mit der
bedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,
also $P(B|C)$. Nun sind $B$ und $C$ eingetreten, und auf der nächsten
Stufe multiplizieren wir deshalb mit $P(A|B\cap C)$. Eine andere
Reihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B. 
$$
P(A\cap B\cap C)=P(A)P(B|A)P(C|A\cap B).
$$
Für jede Reihenfolge gelangt man zum gleichen Ergebnis.

::: callout-tip
Eine Bank möchte die Dauer ihrer Kundenbeziehungen modellieren. 
Dazu definiert sie die Ereignisse $A_t$: "die Kundenbeziehung besteht seit $t$ Jahren"
für $t=0,1,\ldots$. Die Bank kennt (aus Erfahrung) die
Kündigungswahrscheinlichkeiten in Abhängigkeit von der Kundendauer.
Mit $k_{t,t+1}$ wird die Wahrscheinlichkeit bezeichnet, dass ein
Kunde, der schon seit $t$ Jahren Kunde ist, innerhalb des nächsten
Jahres kündigt. Es handelt sich also um die bedingten Wahrscheinlichkeiten
$$
k_{t,t+1}=1-P(A_{t+1}|A_t).
$$
Die Wahrscheinlichkeit, dass ein Neukunde ($t=0$) nach 5 Jahren immer
noch Kunde ist, beträgt
$$
\begin{align*}
P(A_5)&=P(A_5|A_4)P(A_4|A_3)P(A_3|A_2)P(A_2|A_1)P(A_1|A_0)\\
&=(1-k_{4,5})(1-k_{3,4})(1-k_{2,3})(1-k_{1,2})(1-k_{0,1}).
\end{align*}
$$
:::

## Totale Wahrscheinlichkeit {#total}

Aus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte
Wahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum
$\Omega$ in eine Partition. Unter einer **Partition** versteht man eine
Zerlegung in disjunkte Mengen $A_1, A_2, \ldots, A_n$, so dass die
Vereinigungsmenge der $A_1,\ldots, A_n$ wieder $\Omega$ ergibt. Die
"Partitionierung eines Rinds" könnte etwa so aussehen:

```{r echo=FALSE,out.width="100%"}
knitr::include_graphics("images/AdobeStock_101998556b.jpeg")
```

Das Rind wird also vollständig Teilmengen zerlegt, und alle Teilmengen
zusammen ergeben wieder das gesamte Rind.

Nun nehmen wir an, dass für jedes Ereignis $A_i$, $i=1,\ldots,n$, die
bedingte Wahrscheinlichkeit $P(B|A_i)$ gegeben ist, wobei $B$ irgendein
Ereignis ist. Dann gilt für die unbedingte Wahrscheinlichkeit von $B$:
$$
P(B)=\sum_{i=1}^n P(B|A_i)P(A_i).
$$ 
Eine besonders simple Partitionierung ist die Unterteilung in
$A$ und $\bar A$. Dann lautet die Formel
$$
P(B)=P(B|A)P(A)+P(B|\bar A)P(\bar A).
$$ 

Die unbedingte Wahrscheinlichkeit ergibt sich als gewichtete
Summe aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch
die Wahrscheinlichkeiten der bedingenden Ereignisse. Dieser Zusammenhang 
ist dann besonders nützlich, wenn die bedingten Wahrscheinlichkeiten
schon bekannt oder einfach zu ermitteln sind, die unbedingte 
Wahrscheinlichkeit jedoch schwierig zu finden ist.

::: callout-tip
Eine Bank vergibt einen Kredit an ein Unternehmen. Das Unternehmen
zahlt den Kredit mit einer Wahrscheinlichkeit von 0.95 zurück,
wenn die konjunkturelle Lage positiv ist. Bei einer schwachen
Konjunktur wird der Kredit jedoch nur mit einer Wahrscheinlichkeit
von 0.8 zurückgezahlt. Die Wahrscheinlichkeit einer guten
Konjunktur sei 0.75 (und die Konjunktur kann nur gut oder
schlecht sein).

Als Ereignisse definiert man $A$: "gute Konjunktur" und
$B$: "Kredit wird zurückgezahlt". Gegeben sind die bedingten Wahrscheinlichkeiten
$P(B|A)=0.95$ und $P(B|\bar A)=0.8$. Außerdem ist $P(A)=0.75$
bekannt. Damit ergibt sich die Wahrscheinlichkeit einer
Rückzahlung als
$$
\begin{align*}
P(B)&=P(B|A)P(A)+P(B|\bar A)P(\bar A)\\
&=0.95\cdot 0.75+0.8\cdot 0.25\\
&=0.9125
\end{align*}
$$
:::

## Satz von Bayes {#bayes}

Der Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten
$P(A|B)$ und $P(B|A)$ zueinander in Beziehung. Es gilt 
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}.
$$ 
Um die Bedingung und das Bedingte zu vertauschen, braucht man also
beide unbedingte Wahrscheinlichkeiten. Die Herleitung des Satzes von
Bayes ergibt sich aus der Definition der bedingten Wahrscheinlichkeit.
Wegen 
$$
\begin{align*}
P(A|B) &= \frac{P(A\cap B)}{P(B)}\\
P(B|A) &= \frac{P(A\cap B)}{P(A)}
\end{align*}
$$ 
gilt 
$$
P(A|B)P(B)=P(B|A)P(A).
$$ 
Daraus folgt durch Umstellen unmittelbar der Satz von Bayes.

::: callout-tip
Wir betrachten wieder den medizinischen Schnelltest und die beiden
Ereignisse $A$: "Person ist infiziert", und $B$: "Test positiv". Neben
den beiden bedingten Wahrscheinlichkeiten $P(B|A)=0.99$ und 
$P(\bar B|\bar A)=0.99$ sei bekannt, dass $P(A)=0.001$, d.h. nur mit
einer Wahrscheinlichkeit von 0.1 Prozent ist eine zufällig aus der
Population ausgewählte Person infiziert. 

Wie groß ist Wahrscheinlichkeit, dass eine Person infiziert ist, bei
der der Schnelltest ein positives Ergebnis zeigt? Gesucht ist also
$P(A|B)$. Nach dem Satz von Bayes gilt
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}.
$$
Nach dem Satz der totalen Wahrscheinlichkeit berechnet man
$$
\begin{align*}
P(B) &=P(B|A)P(A)+P(B|\bar A)P(\bar A)\\
&= P(B|A)P(A)+[1-P(\bar B|\bar A)][1-P(A)]\\
&= 0.99\cdot 0.001+[1-0.99]\cdot [1-0.001]\\
&= 0.01098.
\end{align*}
$$
Die Wahrscheinlichkeit, dass eine Person mit einem positiven Testergebnis
tatsächlich infiziert ist, ist folglich immer noch sehr klein, obwohl der
Test scheinbar sehr genau arbeitet. 
:::

## Unabhängigkeit

Zwei Ereignisse $A$ und $B$ heißen unabhängig (oder auch stochastisch
unabhängig, engl. independent bzw. stochastically independent), wenn 
$$
P(A\cap B)=P(A)\cdot P(B).
$$ 
Eine alternative (fast äquivalente) Definition der Unabhängigkeit
lautet so: Die Ereignisse $A$ und $B$ heißen unabhängig, wenn gilt 
$$
P(A|B)=P(A)
$$ 
bzw. wenn 
$$
P(B|A)=P(B).
$$ 
Die Definition über die bedingten Wahrscheinlichkeiten ist etwas
intuitiver. Wenn das Wissen darüber, ob $A$ eingetreten ist oder nicht,
für die Wahrscheinlichkeit von $B$ keine Rolle spielt (und umgekehrt),
dann sind $A$ und $B$ unabhängig.

Achten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln.
Zur Erinnerung: Die Ereignisse $A$ und $B$ sind disjunkt, wenn
$A\cap B=\emptyset$, so dass $P(A\cap B)=0$. Disjunkte Ereignisse können
also nicht beide zusammen eintreten. Wenn $A$ eintritt, kann man sicher
sein, dass $B$ nicht eintritt. Und wenn $B$ eintritt, kann man sicher
sein, dass $A$ nicht eintritt. Unabhängige Ereignisse können hingegen
sehr wohl zusammen eintreten.

::: callout-tip
Wir betrachten wieder den medizinischen Schnelltest und die beiden
Ereignisse $A$: "Person ist infiziert", und $B$: "Test positiv".
Zur Erinnerung: Gegeben sind die beiden bedingten Wahrscheinlichkeiten
$P(B|A)=0.99$ und $P(\bar B|\bar A)=0.99$ sowie die
unbedingte Wahrscheinlichkeit $P(A)=0.001$. Außerdem haben wir
im letzten Abschnitt $P(B)=0.01098$ berechnet. 

Sind die Ereignisse $A$ und $B$ unabhängig? Die Antwort lautet "nein",
denn es gilt
$$
0.99=P(B|A)\neq P(B)=0.01098.
$$
Unabhängigkeit ist in diesem Fall natürlich auch nicht zu erwarten.
Ein Schnelltest, dessen Ergebnis keinen Einfluss auf die 
Einschätzung der Wahrscheinlichkeit einer Infektion hätte, wäre
vollkommen nutzlos.
:::

## Wahrscheinlichkeitsbäume

Ein Werkzeug, mit dem man allgemeine und etwas komplexere 
Wahrscheinlichkeits-Probleme übersichtlich bearbeiten kann,
sind Wahrscheinlichkeitsbäume. Die Ereignisse bilden die Äste 
des Baums. Die Wahrscheinlichkeiten an den Ästen sind
bedingte Wahrscheinlichkeiten, nur auf der ersten Stufe handelt
es sich um unbedingte Wahrscheinlichkeiten.

::: callout-tip
Als Beispiel betrachten wir den Erfolg einer Firma und den
Einsatz ihres Managements. Dazu definieren wir die beiden
Ereignisse $A$: "Die Firma hat Erfolg" und $B$:
"Das Management strengt sich an".

Gesucht sei die bedingte Wahrscheinlichkeit $P(B|A)=P(A\cap B)/P(A)$.
Für die Auszahlung von Boni an Manager ist es wichtig zu wissen,
ob sie eine gute Leistung erbracht haben. 

Wir wollen bestimmen, wie hoch
die Wahrscheinlichkeit ist, dass das Management sich tatsächlich für den
Erfolg angestrengt hat, wenn wir beobachten, dass die Firma erfolgreich
ist. Oder anders gesagt: Ist der Erfolg der Firma wirklich ein Indikator
für eine gute Leistung?

Wir nehmen an (auch wenn es in der Praxis unrealistisch ist, diese Werte
zu kennen): 
$$
\begin{align*}
P(A|B) &= 0.8 \\
P(A|\bar{B}) &= 0.5 \\
P(B) &= 0.6.
\end{align*}
$$ 
Es bietet sich an, den Baum auf der ersten Ebene in die beiden Ereignisse 
$B$ und $\bar B$ zu verzweigen, denn wir kennen bedingte
Wahrscheinlichkeiten, die diese beiden Ereignisse als Bedingung haben.
An die beiden Zweige schreiben wir die Ereignisse und ihre
Wahrscheinlichkeiten. Die Wahrscheinlichkeit von 0.4 für das Ereignis
$\bar B$ ergibt sich aus der Gegenwahrscheinlichkeit von $B$.

```{r echo=FALSE}
plot(c(0,1),c(0,1),t="n",xlab="",ylab="",axes=F)
points(c(0,0.4,0.4),c(0.5,0.2,0.8))
lines(c(0.4,0,0.4),c(0.2,0.5,0.8))
text(0.2,0.7,expression(B))
text(0.35,0.83,0.6)
text(0.35,0.17,0.4)
text(0.2,0.29,expression(bar(B)))
```

Auf der zweiten Stufe werden nun die bedingten Ereignisse und
Wahrscheinlichkeiten ergänzt, einmal unter der Bedingung, dass $B$
eingetreten ist, einmal unter der Bedingung, dass $\bar B$ eingetreten
ist. An die neuen Zweige werden die zugehörigen bedingten
Wahrscheinlichkeiten geschrieben. Dabei nutzen wir wiederum die
Gegenwahrscheinlichkeiten der bedingten Ereignisse.

```{r echo=FALSE}
plot(c(0,1),c(0,1),t="n",xlab="",ylab="",axes=F)
points(c(0,0.4,0.4,0.8,0.8,0.8,0.8),c(0.5,0.2,0.8,0,0.4,0.6,1))
lines(c(0.4,0,0.4),c(0.2,0.5,0.8))
lines(c(0.8,0.4,0.8),c(0,0.2,0.4))
lines(c(0.8,0.4,0.8),c(0.6,0.8,1))
text(0.2,0.7,expression(B))
text(0.2,0.29,expression(bar(B)))
text(0.6,0.04,expression(bar(A)))
text(0.6,0.35,expression(A))
text(0.6,0.65,expression(bar(A)))
text(0.6,0.95,expression(A))
text(c(0.35,0.35,0.75,0.75,0.75,0.75),
     c(0.3,0.7,0.93,0.67,0.33,0.07),
     c(0.4,0.6,0.8,0.2,0.5,0.5))
```

Die Wahrscheinlichkeiten für $A\cap B$, $A\cap \bar B$, $\bar A\cap B$
und $\bar A\cap\bar b$ ergeben sich nun, indem man entlang der Äste die
Wahrscheinlichkeiten multipliziert (wie in Abschnitt #bedingt
beschrieben). Man erhält 
$$
\begin{align*}
P(A\cap B)&=0.6\cdot 0.8 = 0.48\\
P(\bar A\cap B)&=0.6\cdot 0.2 = 0.12\\
P(A\cap \bar B)&=0.4\cdot 0.5 = 0.2\\
P(\bar A\cap \bar B)&=0.4\cdot 0.5 = 0.2
\end{align*}
$$ 
Die (unbedingten) Wahrscheinlichkeiten $P(A)$ und $P(\bar A)$ ergeben
sich, indem man die Wahrscheinlichkeiten aller Pfade aufaddiert, die
einen $A$-Ast (bzw. einen $\bar A$-Ast) haben. Es ergibt sich
$P(A)=0.68$ und $P(\bar A)=0.32$.

Wie hoch ist nun die Wahrscheinlichkeit $P(B|A)$, dass sich das
Management angestrengt hat, wenn die Firma tatsächlich 
erfolgreich war? Dazu rechnen wir
$$
P(B|A)=\frac{P(A\cup B)}{P(A)}=\frac{0.48}{0.68}=0.7059.
$$
Das Wissen, dass die Firma Erfolg hatte, führt also dazu, dass
die Anstrengung des Managements eine höhere Wahrscheinlichkeit 
hat als ohne dieses Wissen. Die bedingte Wahrscheinlichkeit
$P(B|A)$ ist in diesem Beispiel höher als die unbedingte
Wahrscheinlichkeit $P(B)$. Ob das als Nachweis für die guten
Leistungen des Managements ausreicht und die Auszahlung von
Boni rechtfertig, ist natürlich keine statistische Frage, sondern 
solltein der Verträgen des Managements festgelegt sein.
:::
