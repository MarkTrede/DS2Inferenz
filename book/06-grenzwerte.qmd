# Grenzwertsätze und Simulationen {#grenzwerte}

In diesem Kapitel gehen wir davon aus, dass eine Zufallsvariable 
immer wieder neu gezogen wird. Als Ausgangspunkt der Überlegungen
dient eine Folge von unabhängigen, identisch verteilten (engl. 
independent and identically distributed, i.i.d.) Zufallsvariablen 
$$
X_1,X_2,X_3,\ldots
$$
Man kann sich diese Folge als unabhängige Wiederholungen einer
Zufallsvariable $X$ vorstellen, z.B. als immer wieder neu
geworfene Würfel. Im folgenden bezeichnen wir den Erwartungswert
der Folgenelemente mit $E(X)=\mu$ und die Varianz mit
$Var(X)=\sigma^2$.

## Gesetz der großen Zahl

Aus den Elementen der Folge von Zufallsvariablen berechnen wir
für jedes $n$ das arithmetische Mittel (den Durchschnitt)
$$
\bar X_n=\frac{1}{n}\sum_{i=1}^n X_i.
$$
*Achtung*: Es handelt sich bei $\bar X_n$ um den Durchschnitt
von Zufallsvariablen, also quasi von Zahlen, deren Wert man noch
nicht kennt. Darum kennt man natürlich auch den Wert des Durchschnitts
noch nicht. Mit anderen Worten: Der Durchschnitt $\bar X_n$ ist
ebenfalls eine Zufallsvariable.

Da der Durchschnitt $\bar X_n$ eine Zufallsvariable ist, kann man
den Erwartungswert und die Varianz ausrechnen. Sie sind mit den
Rechenregeln aus Kapitel XXX leicht zu finden. Es gilt
$$
\begin{align*}
E(X) &= E\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \frac{1}{n}\sum_{i=1}^n E(X_i) \\
&= \frac{1}{n}\sum_{i=1}^n \mu \\
&= \mu
\end{align*}
$$
und
$$
\begin{align*}
Var(X) &= Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i) \\
&= \frac{1}{n^2}\sum_{i=1}^n \sigma^2 \\
&= \frac{\sigma^2}{n}.
\end{align*}
$$
Mit anderen Worten: Wenn man den Durchschnitt aus immer mehr
Folgeelementen bildet, dann ist der Erwartungswert immer $\mu$,
aber die Varianz wird mit steigendem $n$ immer kleiner. Der
Durchschnitt $\bar X_n$ liegt also mit steigendem $n$
"immer näher" am Erwartungswert $E(X)=\mu$ der Zufallsvariable $X$.

Diese Einsicht lässt sich mathematisch präzise formulieren.

::: callout-note
Das **Gesetz der großen Zahl** besagt, dass für
jedes (noch so kleine) $\epsilon >0$ gilt
$$
\lim_{n\to\infty} P(|\bar X_n-\mu|\ge \epsilon)=0.
$$
:::

Als alternative Notation findet man oft
$$
\text{plim}_{n\to\infty}\bar X_n=\mu,
$$
wobei man "plim" als "Wahrscheinlichkeits-Limes" oder engl.
"probability limit" spricht.

Es gibt neben diesem Gesetz der großen Zahl, das auch als
"schwaches Gesetz der großen Zahl" bezeichnet wird, noch weitere
Varianten, die aber im Kern ebenfalls aussagen, dass der Durchschnitt
in einem gewissen Sinn gegen den Erwartungswert konvergiert.

Das Gesetz der großen Zahl gilt nicht nur für den Erwartungswert,
sondern auch für andere wichtige Größen der Zufallsvariablen.
So konvergiert die Varianz der Folgeelemente gegen die Varianz
der Zufallsvariable; die empirische Verteilungsfunktion der Folgeelemente
konvergiert gegen die Verteilungsfunktion der Zufallsvariable;
die Quantile der Folgeelemente konvergieren gegen die Quantile
der Zufallsvariable; und das Histogramm der Folgeelmente konvergiert
gegen die Dichte der Zufallsvariable. 

## Monte-Carlo-Simulationen (I)

Das Gesetz der großen Zahl hat eine weitreichende 
Konsequenz: Um die Verteilung einer Zufallsvariable zu kennen, 
muss man nicht unbedingt die Verteilungsfunktion,
Dichte oder Wahrscheinlichkeitsfunktion kennen, sondern es
reicht aus, wenn man einen **Algorithmus** zur Verfügung hat, der viele 
unabhängige Ziehungen aus der Zufallsvariable liefert. Solche
Algorithmen gibt es - und viele davon sind in R (oder anderen
Programmiersprachen) implementiert. Die Ziehungen, die vom
Computer generiert werden, nennt man **Zufallszahlen** (engl.
random numbers). Verfahren, die auf solchen Algorithmen
beruhen, werden Monte-Carlo-Simulationen genannt, weil
das Casino von Monte Carlo früher als Inbegriff von 
Zufälligkeit galt.

Die R-Funktionen zum Erzeugen von Zufallszahlen haben alle 
die Form

`rVERTEILUNG(n, PARAMETER)`

Dabei gibt `n` an, wie viele Folgeelemente gezogen werden sollen.
Für die Standardverteilungen, die in Kapitel XXX behandelt wurden,
lauten die Funktionsnamen:

- `rbinom(n, size, prob)`
- `rpois(n, lambda)`
- `rnorm(n, mean, sd)`
- `rexp(n, rate)`

Für die Paretoverteilung gibt es keinen vorgefertigten Algorithmus.
Das Paket `distributionsrd` stellt jedoch die Funktion 

- `rpareto(n, k, xmin)`

bereit, mit der Zufallszahlen aus einer Paretoverteilung gezogen
werden können.

::: callout-tip
Dieses Beispiel zeigt, wie durch eine Monte-Carlo-Simulation
Kennzahlen einer Normalverteilung mit Erwartungswert 
$\mu=10$ und Standardabweichung $\sigma=3$ bestimmt werden 
können. Vorsicht: Die R-Funktionen zur Normalverteilung
erwarten als Parameter für die Streuung nicht die
Varianz, sondern die Standardabweichung.

Zuerst werden $n=100\,000$ Zufallszahlen gezogen
(Sie können die Zahl der Ziehungen durchaus noch
höher setzen) und in einen Vektor `x` geschrieben.
```{r}
n <- 100000
x <- rnorm(n, mean=10, sd=3)
```
Der Erwartungswert wird approximiert durch den Durchschnitt.
```{r}
mean(x)
```
Dieser Wert liegt sehr nahe an dem theoretischen Erwartungswert
$E(X)=\mu=10$.

Die Varianz der Zufallszahlen ist
```{r}
var(x)
```
Der Wert weicht nur wenig von der theoretischen Varianz
$Var(X)=\sigma^2=9$ ab. 

Als 0.75-Quantil erhält man
```{r}
quantile(x, 0.75)
```
Das theoretische 0.75-Quantil der $N(10,3^2)$ ist
```{r}
qnorm(0.75, mean=10, sd=3)
```
Auch hier ist die Approximation durch die Monte-Carlo-Simulation
also sehr präzise.

Der folgende Plot zeigt das Histogramm der Zufallszahlen
zusammen mit der theoretischen Dichtefunktion der Normalverteilung,
die als rote Linie zu sehen ist.
```{r}
hist(x, breaks=50, probability=TRUE)
curve(dnorm(x, mean=10, sd=3), add=TRUE, col="red")
```
Zur Erklärung: Der R-Befehl `hist` erzeugt ein Histogramm. Die Option
`breaks` gibt an, wie viele Intervalle gebildet werden sollen. 
Die Option `probability=TRUE` dient dazu, die Fläche unter der
Dichte auf 1 zu normieren, so dass das Histogramm quasi eine Dichte wird.
Die rote Linie wird durch die Funktion `curve` erzeugt, die wir
in diesem Kurs jedoch nicht weiter behandeln werden.
:::

Offensichtlich ist es nicht sinnvoll, eine Monte-Carlo-Simulation 
durchzuführen, um Erwartungswert, Varianz, Quantile und Histogramm einer 
Verteilung zu finden, für die man diese Größen auch einfach
theoretisch herleiten kann. Monte-Carlo-Simulationen sind 
immer dann extrem hilfreich, wenn es zwar nicht leicht ist, die
theoretischen Größen zu bestimmen, wenn man aber sehr leicht
und schnell Zufallszahlen erzeugen kann aus der Verteilung, die
einen interessiert. Das passiert beispielsweise, wenn wir uns
für eine Zufallsvariable interessieren, die keiner 
Standardverteilung folgt, die sich aber aus einer anderen
Zufallsvariablen ergibt, die wiederum einer Standardverteilung folgt.
Das nächste Beispiel zeigt das.

::: callout-tip
Die Zufallsvariable $X$ folge einer Normalverteilung mit
Erwartungswert $\mu=8$ und Standardabweichung $\sigma=0.5$.
Die Zufallsvariable $Y=e^X$ soll den Bruttomonatslohn von
Angestellten einer bestimmten Branche modellieren. Wir interessieren
uns nicht für den Brutto-, sondern den Nettolohn. Letzterer
ergibt sich, indem man die Abgaben und Steuern vom Bruttolohn 
subtrahiert. Die Höhe der Abgaben und Steuern werde durch 
folgende Funktion beschrieben:
$$
S(y)=\left\{ 
\begin{array}{ll}
0 & \text{ für }y\le 2000\\
0.35(y-2000) & \text{ für }y> 2000\\
\end{array}
\right.
$$
wobei $y$ der Bruttolohn ist. Es handelt sich also um eine "flat tax"
von 35 Prozent mit einem Freibetrag von 2000. 
Der Nettolohn wird modelliert durch die Zufallsvariable
$$
Z=Y-S(Y)
$$
Wie hoch ist der Erwartungswert des Nettolohns? Das lässt
sich recht einfach durch eine Monte-Carlo-Simulation
ermitteln. Die Anzahl der Zufallszahlen sei $n=1\,000\,000$.
```{r}
n <- 1000000
x <- rnorm(n, mean=8, sd=0.5)
y <- exp(x)
s <- ifelse(y <= 2000, 0, 0.35*(y-2000))
z <- y-s
mean(z)
```
Die Dichte des Nettolohns kann durch das Histogramm
approximiert werden. Diese Form der Dichte entspricht keiner
bekannten Standardverteilung.
```{r}
hist(z, probability=TRUE, breaks=150, xlim=c(0,10000))
```
:::

## Zentraler Grenzwertsatz

Der **zentrale Grenzwertsatz** (engl. central limit theorem) 
ist aus zwei Gründen relevant. Zum einen 
liefert er eine mathematische Begründung dafür, warum in der Realität
so viele Verteilungen zu finden sind, die sehr gut durch eine
Normalverteilung beschrieben werden können. Zum zweiten ist er der
Ausgangspunkt für die Herleitung von Konfidenzintervallen (Kapitel XXX) und
Hypothesentests (Kapitel XXX).

Für den zentralen Grenzwertsatz betrachtet man nicht die Folge
von Zufallsvariablen 
$$
\bar X_n=\frac{1}{n}\sum_{i=1}^n X_i
$$ 
(d.h. den Durchschnitt), sondern die
Folge der standardisierten $\bar X_n$. Dazu subtrahiert man den
Erwartungswert und dividiert durch die Standardabweichung. Die
Folge der standardisierten Durchschnitte bezeichnen wir mit
$$
U_n=\frac{\bar X_n-\mu}{\sqrt{\sigma^2/n}}.
$$
Die Wurzel von $n$ wird im allgemeinen vor den Bruch gezogen,
$$
U_n=\sqrt{n}\frac{\bar X_n-\mu}{\sigma}.
$$

::: callout-note
Der **zentrale Grenzwertsatz** besagt, dass für alle $u\in\mathbb{R}$ gilt
$$
\lim_{n\to\infty} P(U_n\le u)=\Phi(u),
$$
wobei $\Phi$ die Verteilungsfunktion der Standardnormalverteilung 
$N(0,1)$ ist.
:::

Etwas weniger präzise - aber dafür leichter verständlich - sagt der
zentrale Grenzwertsatz, dass der standardisierte Durchschnitt von 
vielen Zufallsvariablen im Limes einer Standardnormalverteilung folgt.
Natürlich ist $n$ niemals wirklich unendlich groß, aber selbst
wenn $n$ nur "ausreichend groß" ist, gilt 
$$
U_n\stackrel{appr}{\sim}N(0,1).
$$
Der standardisierte Durchschnitt von "vielen" Zufallsvariablen 
ist annähernd normalverteilt. Diese Aussage ist deswegen 
bemerkenswert, weil es gleichgültig ist, wie die zugrundeliegenden
$X_i$ verteilt sind! Sie müssen nicht einmal stetig verteilt
sein.

Wenn der standardisierte Durchschnitt approximativ 
standardnormalverteilt ist, dann ist der Durchschnitt selber
approximativ normalverteilt. Für ausreichend große $n$ gilt
also 
$$
\bar X_n \stackrel{appr}{\sim}N\left(\mu,\frac{\sigma^2}{n}\right).
$$
Und wenn beide Seiten mit $n$ multipliziert werden, dann
gilt für die Summe
$$
\sum_{i=1}^n X_i \stackrel{appr}{\sim}N\left(n\mu,n\sigma^2\right),
$$
wobei zu beachten ist, dass bei der Multiplikation mit $n$ sich die
Varianz um den Faktor $n^2$ erhöht.

## Monte-Carlo-Simulationen (II)

