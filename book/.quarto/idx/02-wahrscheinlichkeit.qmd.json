{"title":"Wahrscheinlichkeit","markdown":{"headingText":"Wahrscheinlichkeit","headingAttr":{"id":"wahrscheinlichkeit","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Definition {#def:wahrscheinlichkeit}\n\nManche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung (Funktion) angegeben.\n\n::: {.callout-note}\n## Definition\n\nEine Abbildung $P: A \\mapsto P(A)$ heißt **Wahrscheinlichkeitsmaß**, wenn gilt\n\n-   Nichtnegativität: $P(A)\\ge 0$ für alle Ereignisse $A$\n\n-   Normierung: $P(\\Omega)=1$\n\n-   Additivität: Für disjunkte Ereignisse $A$ und $B$ (d.h. $A\\cap B=\\emptyset$) ist \n$$\nP(A\\cup B)=P(A)+P(B).\n$$\n:::\n\nDiese Eigenschaften, die eine Wahrscheinlichkeitsabbildung erfüllen muss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten hergeleitet werden, daher spricht man auch von **Axiomen**. Etwas allgemeiner und präziser wurden diese Axiome 1933 von Andrey Kolmogorov für eine saubere mathematische Fundierung der Wahrscheinlichkeitstheorie eingeführt.\n\nAus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum Beispiel\n\n-   Monotonität: Wenn $A\\subseteq B$, dann ist $P(A)\\le P(B)$\n\n-   Komplementärereignis: $P(\\bar A)=1-P(A)$\n\n-   Additionssatz: $P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$.\n\n## Venn-Diagramme {#venn}\n\nEreignisse lassen sich - wie andere Mengen in der Mengenlehre auch - in Form von Venn-Diagrammen darstellen. Wenn die Ergebnismenge $\\Omega$ durch ein Rechteck repräsentiert wird, dann kann man Ereignisse als Teilmengen des Rechtecks darstellen. In dem folgenden Bild ist zum Beispiel das Ereignis $A$ der kleine Kreis in der Mitte.\n\n```{r}\n#| out.width: \"95%\"\n#| echo: FALSE\n  \nlibrary(plotrix)\npar(mar=c(0.1,0.1,0.1,0.1))\nplot(c(0,1),c(0,1),t=\"n\", axes=F, xlab=\"\",ylab=\"\")\ndraw.circle(0.5,0.5,radius=0.1,col=NA)\ndraw.circle(0.65,0.65,radius=0.2,col=NA)\ndraw.circle(0.2,0.3,radius=0.2,col=NA)\nbox()\ntext(0.1,0.8,expression(Omega),cex=2)\ntext(0.5,0.5,\"A\",cex=2)\ntext(0.65,0.65,\"B\",cex=2)\ntext(0.2,0.3,\"C\",cex=2)\n```\n\nIn diesem Venn-Diagramm wird deutlich, dass die Ereignisse $A$ und $C$ nicht gleichzeitig eintreten können, $A$ und $B$ hingegen schon, denn ihre Schnittmenge ist nicht leer. Außerdem erkennt man, dass $A$ eine kleinere Wahrscheinlichkeit hat als $B$ und $C$. Der Additionssatz $P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$ wird in dieser grafischen Sichtweise sehr einfach klar: Die Wahrscheinlichkeit von $A\\cup B$ (also $A$ oder $B$) ergibt sich als die Summe aus den beiden Kreisflächen $A$ und $B$ abzüglich der Fläche der Schnittmenge, die sonst doppelt gezählt werden würde.\n\n## Laplace-Experimente {#laplace}\n\nEine besonders einfache Art von Zufallsvorgängen sind die sogenannten Laplace-Experimente.\n\n::: {.callout-note}\n## Definition\n\nEin Zufallsvorgang heißt **Laplace-Experiment**, wenn es nur endlich viele Ergebnisse gibt (d.h. wenn $|\\Omega|=n$) und wenn alle Elementarereignisse als gleich wahrscheinlich angenommen werden können.\n:::\n\nBeachten Sie, dass es sich dabei um eine Aussage handelt, die aus unserem Alltagswissen herrührt, nicht aus mathematischen Überlegungen oder Herleitungen. <img src=\"images/kronkorken.jpg\" align=\"right\" width=\"20%\"/> Ein typisches, einfaches Beispiel für ein Laplace-Experiment sind Würfelwürfe. Es ist aus unserem Alltagswissen heraus plausibel, davon auszugehen, dass alle Augenzahlen eines Würfels gleich wahrscheinlich sind. Auch bei einer Münze ist es naheliegend, dass die Wahrscheinlichkeit für Kopf und die Wahrscheinlichkeit für Zahl gleich sind. Hingegen würde man beim Werfen eines Kronkorkens nicht unbedingt vermuten, dass beide Seiten mit der gleichen Wahrscheinlichkeit oben liegen. Hier liegt also kein Laplace-Experiment vor.\n\nBei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein Ergebnis $A$ eintritt leicht zu ermitteln. Sie beträgt \n$$\nP(A)=\\frac{|A|}{|\\Omega|},\n$$ \nalso die Anzahl der Ergebnisse in $A$ dividiert durch die Anzahl aller Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss man also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist manchmal sehr einfach, kann aber bei großen Mengen kompliziert sein. In solchen Fällen hilft der Teilbereich der Mathematik weiter, den man \"Kombinatorik\" nennt. Wir gehen in diesem Kurs jedoch nicht näher auf kombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei denen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.\n\n::: {.callout-tip}\n## Beispiele\n\n- Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment. Die Anzahl der Ergebnisse in $\\Omega$ beträgt 6. Sei $A$ das Ereignis \"Eine gerade Zahl wird geworfen\", also $A=\\{2,4,6\\}$. Dann ist \n$$\n\\begin{align*}\nP(A)&=\\frac{|A|}{|\\Omega|}\\\\\n&=\\frac{3}{6}\\\\\n&=0.5.\n\\end{align*}\n$$\n\n- Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine Rolle, wie man die Ergebnismenge festlegt. Wir wählen \n$$\n\\begin{align*}\n\\Omega=\\{&11,12,13,14,15,16,\\\\\n&21,22,23,24,25,26,\\\\\n&31,32,33,34,35,36,\\\\\n&41,42,43,44,45,46,\\\\\n&51,52,53,54,55,56,\\\\\n&61,62,63,64,65,66\\}\n\\end{align*}\n$$ \nweil es sich dann um ein Laplace-Experiment handelt. Die Ergebnisse \"24\" und \"42\" sind zwar nicht unterscheidbar, wenn die Würfel gleich aussehen, aber wir können den zuerst geworfenen Würfel (oder den weiter links liegenden) als ersten Würfel bezeichnen. Dass alle 36 Ergebnisse gleich wahrscheinlich sind, können wir (nur) mit unserem Alltagswissen begründen, nicht aus der Mathematik heraus.\nSei $A$ das Ereignis \"Die Augenzahlen der beiden Würfel unterscheiden sich um 2\", d.h.\n$$\nA=\\{13,24,31,35,42,46,53,64\\}.\n$$\nDann gilt \n$$\n\\begin{align*}\nP(A)&=\\frac{|A|}{|\\Omega|}\\\\\n&=\\frac{8}{36}\\\\\n&=\\frac{2}{9}\\\\\n&\\approx 0.2222.\n\\end{align*}\n$$ \nAls Ergebnismenge wäre auch \n$$\n\\begin{align*}\n\\Omega=\\{\n&11,12,13,14,15,16,\\\\\n&22,23,24,25,26,\\\\\n&33,34,35,36,\\\\\n&44,45,46,\\\\\n&55,56,\\\\\n&66\\}\n\\end{align*}\n$$ \nmöglich gewesen, wenn die beiden Würfel nicht unterscheidbar sind. Dann wäre aber die Annahme eines Laplace-Experiments nicht mehr korrekt Die Elementarereignisse \"11\" und \"12\" sind beispielsweise nicht gleich wahrscheinlich (wir wissen aus unserer Erfahrung, dass ein Pasch seltener auftritt).\n:::\n\nDie Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere Vorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge so gewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte man das tun.\n\n## Bedingte Wahrscheinlichkeit {#bedingt}\n\nManchmal gibt es begrenzte Informationen über einen Zufallsvorgang. Dann kennt man nicht das realisierte Ergebnis, kann aber die Menge der möglichen Ergebnisse eingrenzen. Dadurch ändern sich die Wahrscheinlichkeiten für Ereignisse.\n\n::: {.callout-note}\n## Definition\n\nWir betrachten zwei Ereignsse $A$ und $B$ mit $P(B)>0$. Dann heißt \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \ndie **bedingte Wahrscheinlichkeit** von $A$ gegeben $B$\n:::\n\nDie Notation $A|B$ steht nicht für ein bestimmtes Ereignis, sondern zeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten, nämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über Wahrscheinlichkeiten ist es nicht immer einfach (aber sehr wichtig!), zwischen der Wahrscheinlichkeit $P(A\\cap B)$ und der bedingten Wahrscheinlichkeit $P(A|B)$ zu unterscheiden. Wenn man $P(A\\cap B)$ meint, spricht man von der Wahrscheinlichkeit, dass $A$ und $B$ eintreten. Wenn man $P(A|B)$ meint, sagt man $A$ gegeben $B$, oder: $A$ wenn $B$, oder: $A$ unter der Bedingung $B$. Wenn man ausdrücklich angeben möchte, dass eine Wahrscheinlichkeit *keine* bedingte Wahrscheinlichkeit ist, nennt man sie auch eine unbedingte Wahrscheinlichkeit.\n\n::: {.callout-tip}\n## Beispiele\n\n- Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse $A=\\{2,4,6\\}$ (\"gerade Zahl\"), und $B=\\{4,5,6\\}$ (\"eine Zahl größer als 3\"). Die bedingte Wahrscheinlichkeit von $A$ gegeben $B$ beträgt \n$$\n\\begin{align*}\nP(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{4,5,6\\})}\\\\\n&=\\frac{2}{3}\n\\end{align*}\n$$ \nund die bedingte Wahrscheinlichkeit von $B$ gegeben $A$ lautet \n$$\n\\begin{align*}\nP(B|A)&=\\frac{P(B\\cap A)}{P(A)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{2,4,6\\})}\\\\\n&=\\frac{2}{3}.\n\\end{align*}\n$$\n\n- Zwei Würfel werden geworfen. Sei $A$ das Ereignis \"die Augensumme ist 10\". Sei $B$ das Ereignis \"einer der beiden Würfel (oder beide) zeigt eine 2\". Dann ist \n$$\n\\begin{align*}\nP(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n&=0.\n\\end{align*}\n$$ \nDie bedingte Wahrscheinlichkeit für das Ereignis $C$: \"Augensumme ist 5\" unter der Bedingung $B$ ist \n$$\n\\begin{align*}\nP(C|B)&=\\frac{P(C\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{23,32\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n&=\\frac{2}{11}.\n\\end{align*}\n$$\n\n- Bei einer Umfrage werden Personen zufällig ausgewählt und befragt. Sei $A$ das Ereignis \"die befragte Person ist weiblich\". Sei $B$ das Ereignis \"die befragte Person arbeitet in Teilzeit\". Zwischen der (unbedingten) Wahrscheinlichkeit $P(B)$ und der bedingten Wahrscheinlichkeit $P(B|A)$ besteht ein Unterschied. Die Wahrscheinlichkeit $P(B)$ steht dafür, dass eine zufällig ausgewählte Person in Teilzeit arbeitet, und diese Person kann männlich oder weiblich sein. Dagegen ist $P(B|A)$ die Wahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit arbeitet.\n:::\n\nIn den Medien werden häufig bedingte Wahrscheinlichkeiten berichtet, ohne dass das explizit erwähnt wird. Oftmals sind die berichteten bedingten Wahrscheinlichkeiten gar nicht die, für die man sich eigentlich interessiert, weil die Bedingung und das Bedingte quasi falsch herum angeordnet sind. Man interessiert sich für $P(A|B)$, aber berichtet wird $P(B|A)$.\n\n\n::: {.callout-tip}\n## Beispiele\n\n- Es wird berichtet, dass ein Corona-Schnelltest mit einer Wahrscheinlichkeit von xxx Prozent ein falsch positives Ergebnis liefert. Falsch positiv bedeutet, die Wahrscheinlichkeit wird unter der Bedingung angegeben, dass die getestete Person negativ ist. Wenn man Test an sich durchführt, interessiert man sich dagegen für die bedingte Wahrscheinlichkeit, infiziert zu sein, obwohl der Test negativ ausfällt, und natürlich auch für die bedingte Wahrscheinlichkeit, tatsächlich infiziert zu sein, wenn der Test positiv ausfällt.\n\n- In den Medien erfährt man, dass XX Prozent der Studierenden ein \"akademisches Elternhaus\" haben. Es handelt sich um die bedingte Wahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person studiert. Für die Bewertung der Chancengleichheit ist hingegen die bedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert, wenn es aus einem akademischen Elternhaus stammt - und zwar im Vergleich zu der bedingten Wahrscheinlichkeit, dass ein Kind studiert, wenn es aus einem nicht-akademischen Elternhaus stammt.\n:::\n\nWie kann man eine bedingte Wahrscheinlichkeit \"umdrehen\"? Wie verhält sich $P(B|A)$ zu $P(A|B)$? Das sehen wir im folgenden Abschnitt.\n\nBedingte Wahrscheinlichkeiten sind auch nützlich, um die Wahrscheinlichkeit zu berechnen, dass mehrere Ereignisse gemeinsam passieren. Dazu formen wir die Definition der bedingten Wahrscheinlichkeit einfach um. Aus \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \nwird \n$$\nP(A\\cap B)={P(B)P(A|B)}.\n$$\n\n::: {.callout-tip}\n## Beispiele\n\n-   Eins\n\n-   Zwei\n:::\n\nDas lässt sich auf mehr als zwei Ereignisse erweitern: \n$$\n\\begin{align*}\nP(A\\cap B \\cap C)&=P(C)P(B|C)P(A|B\\cap C)\\\\\nP(A\\cap B\\cap C\\cap D)&=P(D)P(C|D)P(B|C\\cap D)P(A|B\\cap C\\cap D)\n\\end{align*}\n$$ \nDiese Formeln sind bei näherer Betrachtung intuitiv: Die Wahrscheinlichkeit, dass $A$, $B$ und $C$ gemeinsam eintreten, ergibt sich, indem man zuerst eine der unbedingten Wahrscheinlichkeiten nimmt (z.B. $P(C)$). Nun ist $C$ quasi eingetreten und wir arbeiten unter der Bedingung $C$ weiter. Die Wahrscheinlichkeit $P(C)$ wird jetzt mit der bedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert, also $P(B|C)$. Nun sind $B$ und $C$ eingetreten, und auf der nächsten Stufe multiplizieren wir deshalb mit $P(A|B\\cap C)$. Eine andere Reihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B. \n$$\nP(A\\cap B\\cap C)=P(A)P(B|A)P(C|A\\cap B).\n$$\n\n## Totale Wahrscheinlichkeit {#total}\n\nAus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte Wahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum $\\Omega$ in eine Partition. Unter einer Partition versteht man eine Zerlegung in disjunkte Mengen $A_1, A_2, \\ldots, A_n$, so dass die Vereinigungsmenge der $A_1,\\ldots, A_n$ wieder $\\Omega$ ergibt. Die \"Partitionierung eines Rinds\" könnte etwa so aussehen:\n\n```{r echo=FALSE,out.width=\"100%\"}\nknitr::include_graphics(\"images/partitionrind.jpg\")\n```\n\nDas Rind wird also vollständig Teilmengen zerlegt, und alle Teilmengen zusammen ergeben wieder das gesamte Rind.\n\nNun nehmen wir an, dass für jedes Ereignis $A_i$, $i=1,\\ldots,n$, die bedingte Wahrscheinlichkeit $P(B|A_i$ gegeben ist, wobei $B$ irgendein Ereignis ist. Dann gilt für die unbedingte Wahrscheinlichkeit von $B$: \n$$\nP(B)=\\sum_{i=1}^n P(B|A_i)P(A_i).\n$$ \nDie unbedingte Wahrscheinlichkeit ergibt sich also als gewichtete Summe aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch die Wahrscheinlichkeiten der bedingenden Ereignisse.\n\n::: {.callout-tip}\n## Beispiele\n\n-   Eins\n\n-   Zwei\n:::\n\n## Satz von Bayes {#bayes}\n\nDer Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten $P(A|B)$ und $P(B|A)$ zueinander in Beziehung. Es gilt \n$$\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\n$$ \nUm die Bedingung und das Bedingte zu vertauschen, braucht man also beide unbedingte Wahrscheinlichkeiten. Die Herleitung des Satzes von Bayes ergibt sich aus der Definition der bedingten Wahrscheinlichkeit. Wegen \n$$\n\\begin{align*}\nP(A|B) &= \\frac{P(A\\cap B)}{P(B)}\\\\\nP(B|A) &= \\frac{P(A\\cap B)}{P(A)}\n\\end{align*}\n$$ \ngilt \n$$\nP(A|B)P(B)=P(B|A)P(A).\n$$ \nDaraus folgt durch Umstellen unmittelbar der Satz von Bayes.\n\n::: {.callout-tip}\n## Beispiele\n\n-   Eins\n\n-   Zwei\n:::\n\n## Unabhängigkeit\n\nZwei Ereignisse $A$ und $B$ heißen unabhängig (oder auch stochastisch unabhängig), wenn \n$$\nP(A\\cap B)=P(A)\\cdot P(B).\n$$ \nEine alternative (fast äquivalente) Definition der Unabhängigkeit lautet so: Die Ereignisse $A$ und $B$ heißen unabhängig, wenn gilt \n$$\nP(A|B)=P(A)\n$$ \nbzw. wenn \n$$\nP(B|A)=P(B).\n$$ \nDie Definition über die bedingten Wahrscheinlichkeiten ist etwas intuitiver. Wenn das Wissen darüber, ob $A$ eingetreten ist oder nicht, für die Wahrscheinlichkeit von $B$ keine Rolle spielt (und umgekehrt), dann sind $A$ und $B$ unabhängig.\n\nAchten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln. Zur Erinnerung: Die Ereignisse $A$ und $B$ sind disjunkt, wenn $A\\cap B=\\emptyset$, so dass $P(A\\cap B)=0$. Disjunkte Ereignisse können also nicht beide zusammen eintreten. Wenn $A$ eintritt, kann man sicher sein, dass $B$ nicht eintritt. Und wenn $B$ eintritt, kann man sicher sein, dass $A$ nicht eintritt. Unabhängige Ereignisse können hingegen sehr wohl zusammen eintreten.\n\nWie lässt sich die Definition von Unabhängigkeit auf mehr als zwei Ereignisse verallgemeinern? Das ist etwas komplizierter, als man es vielleicht zunächst vermutet. Intuitiv gesprochen liegt bei nur zwei Ereignissen ($A$ und $B$) Unabhängigkeit dann vor, wenn Informationen über die Ausprägung eines Ereignisses die Wahrscheinlichkeit für das andere Ereignis nicht verändern. Bei drei Ereignissen ($A$, $B$, $C$) kann es passieren, dass die Information über $A$ weder die Wahrscheinlichkeit von $B$ noch die von $C$ verändert, dass aber die Information über $A$ *und* $B$ dennoch die Wahrscheinlichkeit von $C$ verändert. Sehen wir uns dazu ein einfaches Beispiel an:\n\n::: {.callout-tip}\n## Beispiel\n\nZwei faire Würfel werden geworfen (mit der üblichen Ergebnismenge). Wir definieren die folgenden drei Ereignisse:\n\n$A$: Die Augenzahl des ersten Würfels ist ungerade\n\n$B$: Die Augenzahl des zweiten Würfels ist ungerade\n\n$C$: Die Summe der Augenzahlen ist ungerade\n\nWenn man zwei beliebige dieser Ereignisse auswählt, dann sind sie unabhängig voneinander. Das dritte Ereignis ist aber von den beiden anderen abhängig, weil die Summe von zwei ungeraden Zahlen niemals ungerade ist. \n:::\n\nWeil die Situation bei mehr als zwei Ereignissen also komplexer ist, wird Unabhängigkeit für mehrere Ereignisse so definiert:\n\n::: {.callout-note}\n## Definition\n\nSeien $A_1,\\ldots,A_n$ Ereignisse. Sie heißen *global unabhängig*, wenn für jedes $m\\in \\{2,\\ldots,n\\}$ und alle möglichen Indizes $1\\le i_1<\\ldots<i_m\\le n$ gilt \n$$\nP(A_{i_1}\\cap\\ldots\\cap A_{i_m})=P(A_{i_1})\\cdot\\ldots\\cdot P(A_{i_m}).\n$$\n:::\n\nDie formale Definition ist nicht ganz leicht zu durchschauen. Letztlich sagt sie aber einfach aus: Bei globaler Unabhängigkeit ist die Wahrscheinlichkeit, dass beliebig viele beliebig ausgewählte Ereignisse gemeinsam eintreten, gerade dem Produkt der einzelnen Wahrscheinlichkeiten entspricht.\n\n## Vier-Felder-Tafeln\n\nDie Vier-Felder-Tafel ist ein besonders einfaches, aber nützliches Werkzeug für Fragen zu Wahrscheinlichkeiten, wenn zwei genau Ereignisse (und ihre Gegenereignisse) eine Rolle spielen, z.B. $A$ und $B$ (und $\\bar A$ und $\\bar B$). Jedes der beiden Ereignisse kann eintrefen oder nicht, so dass es insgesamt vier Kombinationsmöglichkeiten gibt. Die Wahrscheinlichkeiten dieser vier Möglichkeiten werden wie folgt angeordnet: \n$$\n\\begin{array}{c|cc|} \n& A & \\bar A& \\\\ \\hline\nB & P(A\\cap B) & P(\\bar A\\cap B)\\\\\n\\bar B & P(A\\cap \\bar B) & P(\\bar A\\cap \\bar B)\\\\ \\hline\n\\end{array}\n$$ \nDa $P((A\\cap B)\\cup (A\\cap \\bar B))=P(A)$ ist (und entsprechend für die anderen Zeilen und Spalten), kann man die Ränder ergänzen: \n$$\n\\begin{array}{c|cc|c} \n& A & \\bar A& \\\\ \\hline\nB & P(A\\cap B) & P(\\bar A\\cap B) & P(B)\\\\\n\\bar B & P(A\\cap \\bar B) & P(\\bar A\\cap \\bar B) & P(\\bar B)\\\\ \\hline\n& P(A) & P(\\bar A) & 1\n\\end{array}\n$$ \nWeil die Zeilen und Spalten sich immer zu den Randwerten addieren müssen, kann man Felder leicht ergänzen, wenn in einer Zeile oder Spalte ein Wert fehlt. Eine häufiger Fehler bei der Konstruktion von Vier-Felder-Tafeln besteht darin, dass man bedingte Wahrscheinlichkeiten in die Felder einträgt, also beispielsweise $P(A|B)$ anstelle von $P(A\\cap B)$. Wenn die bedingte Wahrscheinlichkeit $P(A|B)$ bekannt ist, hilft das bei der Erstellung der Vier-Felder-Tafel aber durchaus weiter, weil $P(A\\cap B)=P(A|B)P(B)$ ist.\n\nWill man eine Vier-Felder-Tafel ausfüllen, dann ist es wichtig darauf zu achten, ob es sich bei den vorgegebenen Wahrscheinlichkeiten um gemeinsame Wahrscheinlichkeiten (z.B. $A$ und $B$ bzw. $P(A\\cap B)$) handelt, oder um bedingte Wahrscheinlichkeiten (also z.B. $P(A|B)$). Letztere können leicht umgeformt werden, wenn man den Zusammenhang \\[ P(A\\cap B)=P(A\\|B)P(B) \\] beachtet (oder die analogen Zusammenhänge für andere bedingte Wahrscheinlichkeiten).\n\n::: {.callout-tip}\n## Beispiele\n\nXXX\n:::\n\n## Wahrscheinlichkeitsbäume\n\nVier-Felder-Tafeln sind ein nützliches Werkzeug, um einfaches Wahrscheinlichkeitsprobleme zu lösen, in denen nur zwei Ereignisse und ihre Gegenwahrscheinlichkeiten vorkommen. Ein Werkzeug, mit dem man auch allgemeine und etwas komplexere Probleme übersichtlich bearbeiten kann, sind Wahrscheinlichkeitsbäume. Die Ereignisse - es können hier durch aus mehr als zwei sein - bilden die Äste des Baums.\n\n::: {.callout-tip}\n## Beispiel\n\nDefinition der Ereignisse: \n$$\n\\begin{eqnarray*}\nA &\\text{:}&\\text{ Die Firma hat Erfolg} \\\\\nB &\\text{:}&\\text{ Der Manager strengt sich an}\n\\end{eqnarray*}\n$$ \nGesucht sei die bedingte Wahrscheinlichkeit $P(B|A)=P(A\\cap B)/P(A)$. Für die Auszahlung von Boni an einen Manager ist es wichtig zu wissen, ob er eine gute Leistung erbracht hat. Wir wollen bestimmen, wie hoch die Wahrscheinlichkeit ist, dass der Manager sich tatsächlich für den Erfolg angestrengt hat, wenn wir beobachten, dass die Firma erfolgreich ist. Oder anders gesagt, ist der Erfolg der Firma wirklich ein Indikator für eine gute Leistung?\n\nWir nehmen an (auch wenn es in der Praxis unrealistisch ist, diese Werte zu kennen): \n$$\n\\begin{eqnarray*}\nP\\left( A|B\\right) &=&0.8 \\\\\nP(A|\\bar{B}) &=&0.5 \\\\\nP\\left( B\\right) &=&0.6\n\\end{eqnarray*}\n$$ \nHier handelt es sich um eine Fragestellung, die auch mit einer Vier-Felder-Tafel beantwortet werden könnte. Trotzdem nutzen wir nun einen Wahrscheinlichkeitsbaum. Auf der ersten Ebene verzweigt der Baum in die beiden Ereignisse $B$ und $\\bar B$, denn wir kennen bedingte Wahrscheinlichkeiten, die diese beiden Ereignisse als Bedingung haben. An die beiden Zweige schreiben wir die Ereignisse und ihre Wahrscheinlichkeiten. Die Wahrscheinlichkeit von 0.4 für das Ereignis $\\bar B$ ergibt sich aus der Gegenwahrscheinlichkeit von $B$.\n\n```{r echo=FALSE}\nplot(c(0,1),c(0,1),t=\"n\",xlab=\"\",ylab=\"\",axes=F)\npoints(c(0,0.4,0.4),c(0.5,0.2,0.8))\nlines(c(0.4,0,0.4),c(0.2,0.5,0.8))\ntext(0.2,0.7,expression(B))\ntext(0.35,0.83,0.6)\ntext(0.35,0.17,0.4)\ntext(0.2,0.29,expression(bar(B)))\n```\n\nAuf der zweiten Stufe werden nun die bedingten Ereignisse und Wahrscheinlichkeiten ergänzt, einmal unter der Bedingung, dass $B$ eingetreten ist, einmal unter der Bedingung, dass $\\bar B$ eingetreten ist. An die neuen Zweige werden die zugehörigen bedingten Wahrscheinlichkeiten geschrieben. Dabei nutzen wir wiederum die Gegenwahrscheinlichkeiten der bedingten Ereignisse.\n\n```{r echo=FALSE}\nplot(c(0,1),c(0,1),t=\"n\",xlab=\"\",ylab=\"\",axes=F)\npoints(c(0,0.4,0.4,0.8,0.8,0.8,0.8),c(0.5,0.2,0.8,0,0.4,0.6,1))\nlines(c(0.4,0,0.4),c(0.2,0.5,0.8))\nlines(c(0.8,0.4,0.8),c(0,0.2,0.4))\nlines(c(0.8,0.4,0.8),c(0.6,0.8,1))\ntext(0.2,0.7,expression(B))\ntext(0.2,0.29,expression(bar(B)))\ntext(0.6,0.04,expression(bar(A)))\ntext(0.6,0.35,expression(A))\ntext(0.6,0.65,expression(bar(A)))\ntext(0.6,0.95,expression(A))\ntext(c(0.35,0.35,0.75,0.75,0.75,0.75),\n     c(0.3,0.7,0.93,0.67,0.33,0.07),\n     c(0.4,0.6,0.8,0.2,0.5,0.5))\n```\n\nDie Wahrscheinlichkeiten für $A\\cap B$, $A\\cap \\bar B$, $\\bar A\\cap B$ und $\\bar A\\cap\\bar b$ ergeben sich nun, indem man entlang der Äste die Wahrscheinlichkeiten multipliziert. Man erhält \n$$\n\\begin{align*}\nP(A\\cap B)&=0.6\\cdot 0.8 = 0.48\\\\\nP(\\bar A\\cap B)&=0.6\\cdot 0.2 = 0.12\\\\\nP(A\\cap \\bar B)&=0.4\\cdot 0.5 = 0.2\\\\\nP(\\bar A\\cap \\bar B)&=0.4\\cdot 0.5 = 0.2\n\\end{align*}\n$$ \nDie (unbedingten) Wahrscheinlichkeiten $P(A)$ und $P(\\bar A)$ ergeben sich, indem man die Wahrscheinlichkeiten aller Pfade aufaddiert, die einen $A$-Ast (bzw. einen $\\bar A$-Ast) haben. Es ergibt sich $P(A)=0.68$ und $P(\\bar A)=0.32$.\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"02-wahrscheinlichkeit.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","bibliography":["references.bib"],"editor":"source","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"02-wahrscheinlichkeit.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["references.bib"],"editor":"source","documentclass":"scrreprt"},"extensions":{"book":{}}}}}