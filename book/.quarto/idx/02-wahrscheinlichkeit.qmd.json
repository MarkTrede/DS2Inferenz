{"title":"Wahrscheinlichkeit","markdown":{"headingText":"Wahrscheinlichkeit","headingAttr":{"id":"sec-wahrscheinlichkeit","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Definition {#sec-definition}\n\nManche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die\nWahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung\n(Funktion) angegeben.\n\n:::: callout-note\n## Definition: Wahrscheinlichkeit\nEine Abbildung $P: A \\mapsto P(A)$ heißt **Wahrscheinlichkeit**\n(engl. probability), wenn gilt\n\n- Nichtnegativität: $P(A)\\ge 0$ für alle Ereignisse $A$\n\n- Normierung: $P(\\Omega)=1$\n\n- Additivität: Für disjunkte Ereignisse $A$ und $B$ (d.h.\n$A\\cap B=\\emptyset$) ist \n$$\nP(A\\cup B)=P(A)+P(B).\n$$\n:::\n\nDiese Eigenschaften, die eine Wahrscheinlichkeit erfüllen\nmuss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten\nhergeleitet werden, daher spricht man auch von **Axiomen**. Etwas\nallgemeiner und präziser wurden diese Axiome 1933 \nvon Andrey Kolmogorov (1903-1987) für eine saubere mathematische \nFundierung der Wahrscheinlichkeitstheorie eingeführt.\n\nAus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum\nBeispiel:\n\n-   Komplementärereignis: $P(\\bar A)=1-P(A)$\n\n-   Additionssatz: $P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$\n\n-   Monotonität: Wenn $A\\subseteq B$, dann ist $P(A)\\le P(B)$.\n\n## Laplace-Experimente {#sec-laplace}\n\nEine besonders einfache Art von Zufallsvorgängen sind die sogenannten\nLaplace-Experimente.\n\n::: callout-note\n## Definition: Laplace-Experiment\nEin Zufallsvorgang heißt **Laplace-Experiment** (engl. Laplace experiment), \nwenn es nur endlich viele Ergebnisse gibt (d.h. wenn $|\\Omega|=n$) und wenn \nalle Elementarereignisse als gleich wahrscheinlich angenommen werden können.\n:::\n\n<img src=\"images/AdobeStock_332174058b_Kronkorken.jpeg\" align=\"right\" width=\"35%\"/>\nBeachten Sie, dass es sich dabei um eine Aussage handelt, die aus\nunserem Alltagswissen herrührt, nicht aus mathematischen Überlegungen\noder Herleitungen! Ein\ntypisches, einfaches Beispiel für ein Laplace-Experiment sind\nWürfelwürfe. Es ist aus unserem Alltagswissen heraus plausibel, davon\nauszugehen, dass alle Augenzahlen eines normalen Würfels gleich wahrscheinlich\nsind. Auch bei einer Münze ist es naheliegend, dass die\nWahrscheinlichkeit für Kopf und die Wahrscheinlichkeit für Zahl gleich\nsind. Hingegen würde man beim Werfen eines Kronkorkens nicht unbedingt\nvermuten, dass beide Seiten mit der gleichen Wahrscheinlichkeit oben\nliegen. Hier liegt also kein Laplace-Experiment vor.\n\nBei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein\nErgebnis $A$ eintritt, leicht zu ermitteln. Sie beträgt \n$$\nP(A)=\\frac{|A|}{|\\Omega|},\n$$ \nalso die Anzahl der Ergebnisse in $A$ dividiert durch die Anzahl\naller Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss\nman also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist\nmanchmal sehr einfach, kann aber bei großen Mengen kompliziert sein. In\nsolchen Fällen hilft der Teilbereich der Mathematik weiter, den man\n\"Kombinatorik\" nennt. Wir gehen in diesem Kurs jedoch nicht näher auf\nkombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei\ndenen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Ein Würfel als Laplace-Experiment\nEin Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.\nDie Anzahl der Ergebnisse in $\\Omega$ beträgt 6. Sei $A$ das\nEreignis \"Eine gerade Zahl wird geworfen\", also $A=\\{2,4,6\\}$. Dann\nist \n$$\n\\begin{align*}\nP(A)&=\\frac{|A|}{|\\Omega|}\\\\\n&=\\frac{3}{6}\\\\\n&=0.5.\n\\end{align*}\n$$\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Zwei Würfel als Laplace-Experiment\nZwei gleich aussehende Würfel werden geworfen. Nun spielt es eine\nRolle, wie man die Ergebnismenge festlegt. Wir wählen \n$$\n\\begin{align*}\n\\Omega=\\{&11,12,13,14,15,16,\\\\\n&21,22,23,24,25,26,\\\\\n&31,32,33,34,35,36,\\\\\n&41,42,43,44,45,46,\\\\\n&51,52,53,54,55,56,\\\\\n&61,62,63,64,65,66\\}\n\\end{align*}\n$$ \nweil es sich dann um ein Laplace-Experiment handelt. Die\nErgebnisse \"24\" und \"42\" sind zwar nicht unterscheidbar, wenn die\nWürfel gleich aussehen, aber wir können den zuerst geworfenen Würfel\n(oder z.B. den weiter links liegenden) als ersten Würfel bezeichnen. Dass\nalle 36 Ergebnisse gleich wahrscheinlich sind, können wir (nur) mit\nunserem Alltagswissen begründen, nicht aus der Mathematik heraus.\n\nSei $A$ das Ereignis \"Die Augenzahlen der beiden Würfel\nunterscheiden sich um 2\", d.h. \n$$\nA=\\{13,24,31,35,42,46,53,64\\}.\n$$ \nDann gilt \n$$\n\\begin{align*}\nP(A)&=\\frac{|A|}{|\\Omega|}\\\\\n&=\\frac{8}{36}\\\\\n&=\\frac{2}{9}\\\\\n&\\approx 0.2222.\n\\end{align*}\n$$ \nDa die beiden Würfel nicht unterscheidbar sind, wäre als Ergebnismenge auch \n$$\n\\begin{align*}\n\\Omega=\\{\n&11,12,13,14,15,16,\\\\\n&22,23,24,25,26,\\\\\n&33,34,35,36,\\\\\n&44,45,46,\\\\\n&55,56,\\\\\n&66\\}\n\\end{align*}\n$$ \nmöglich gewesen. Die Ergebnismenge hätte dann 21 Elemente. Die Annahme \neines Laplace-Experiments wäre jedoch in diesem Fall nicht\nmehr korrekt, denn die Elementarereignisse \"11\" und \"12\" sind\nbeispielsweise nicht gleich wahrscheinlich. Wir wissen aus unserer\nErfahrung, dass ein Pasch seltener auftritt. \n:::\n\nDie Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere\nVorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge so\ngewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte man\ndas tun.\n\n## Bedingte Wahrscheinlichkeit {#sec-bedingt}\n\nManchmal gibt es begrenzte Informationen über einen Zufallsvorgang. Dann\nkennt man zwar nicht das realisierte Ergebnis, kann aber die Menge der\nmöglichen Ergebnisse eingrenzen. Dadurch ändern sich die\nWahrscheinlichkeiten für Ereignisse.\n\n::: callout-note\n## Definition: Bedingte Wahrscheinlichkeit\nWir betrachten zwei Ereignsse $A$ und $B$ mit $P(B)>0$. Dann heißt \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \ndie **bedingte Wahrscheinlichkeit** (engl. conditional probability) von $A$ gegeben $B$.\n:::\n\nDie Notation $A|B$ steht nicht für ein bestimmtes Ereignis, sondern\nzeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten,\nnämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über\nWahrscheinlichkeiten ist es nicht immer einfach (aber sehr wichtig!),\nzwischen der Wahrscheinlichkeit $P(A\\cap B)$ und der bedingten\nWahrscheinlichkeit $P(A|B)$ zu unterscheiden. Wenn man $P(A\\cap B)$\nmeint, spricht man von der Wahrscheinlichkeit, dass $A$ und $B$\neintreten. Wenn man $P(A|B)$ meint, sagt man $A$ gegeben $B$, oder: $A$\nwenn $B$, oder: $A$ unter der Bedingung $B$. Wenn man ausdrücklich\nangeben möchte, dass eine Wahrscheinlichkeit *keine* bedingte\nWahrscheinlichkeit ist, nennt man sie auch eine unbedingte\nWahrscheinlichkeit (engl. unconditional probability).\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Bedingte Wahrscheinlichkeit für einen Würfel\nEin Würfel wird geworfen. Wir betrachten die beiden Ereignisse\n$A=\\{2,4,6\\}$ (\"gerade Zahl\"), und $B=\\{3,4,5,6\\}$ (\"eine Zahl größer\nals 2\"). Die bedingte Wahrscheinlichkeit von $A$ gegeben $B$ beträgt\n$$\n\\begin{align*}\nP(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{3,4,5,6\\})}\\\\\n&=\\frac{1}{2}\n\\end{align*}\n$$ \nund die bedingte Wahrscheinlichkeit von $B$ gegeben $A$ lautet \n$$\n\\begin{align*}\nP(B|A)&=\\frac{P(B\\cap A)}{P(A)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{2,4,6\\})}\\\\\n&=\\frac{2}{3}.\n\\end{align*}\n$$\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Bedingte Wahrscheinlichkeit bei zwei Würfeln\nZwei Würfel werden geworfen, ohne dass Sie es sehen können.\nSie fragen, ob eine Sechs geworfen wurde. Die Frage wird\nwahrheitsgemäß bejaht. Wie groß ist die Wahrscheinlichkeit,\ndass ein 6er-Pasch geworfen wurde? \n\nSei $A$ das Ereignis \"6er-Pasch\" und $B$ das Ereignis\n\"mindest eine Sechs wurde geworfen\", d.h.\n$$\n\\begin{align*}\nA &= \\{66\\}\\\\\nB &= \\{16,26,36,46,56,61,62,63,64,65,66\\}.\n\\end{align*}\n$$\nDaher gilt\n$$\n\\begin{align*}\nP(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{66\\})}{P(\\{16,26,36,46,56,61,62,63,64,65,66\\})}\\\\\n&=1/11.\n\\end{align*}\n$$ \n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Erwerbsstatus und Geschlecht\n<img src=\"images/AdobeStock_172088861b_Umfrage.jpeg\" align=\"right\" width=50%\" style=\"padding-left:15px;padding-top:5px;padding-bottom:5px;\"/>\nBei einer Umfrage werden Personen zufällig ausgewählt und befragt.\nSei $A$ das Ereignis \"die befragte Person ist weiblich\". Sei $B$ das\nEreignis \"die befragte Person arbeitet in Teilzeit\". Zwischen der\n(unbedingten) Wahrscheinlichkeit $P(B)$ und der bedingten\nWahrscheinlichkeit $P(B|A)$ besteht ein Unterschied. Die\nWahrscheinlichkeit $P(B)$ steht dafür, dass eine zufällig\nausgewählte Person in Teilzeit arbeitet, und diese Person kann\nmännlich oder weiblich sein. Dagegen ist $P(B|A)$ die\nWahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit\narbeitet.\n:::\n\nIn den Medien werden manchmal bedingte Wahrscheinlichkeiten berichtet,\nohne dass das explizit erwähnt wird. Oftmals sind die berichteten\nbedingten Wahrscheinlichkeiten gar nicht die, für die man sich\neigentlich interessiert, weil die Bedingung und das Bedingte quasi\nfalsch herum angeordnet sind. Man interessiert sich für $P(A|B)$, \nberichtet wird aber $P(B|A)$.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Medizinischer Schnelltest\n<img src=\"images/AdobeStock_338942366b_Medizintest.jpeg\" align=\"right\" width=50%\" style=\"padding-left:15px;padding-top:5px;\"/>Es wird berichtet, dass \nein medizinischer Schnelltest mit einer\nWahrscheinlichkeit von 99 Prozent eine infizierte Person korrekt als\ninfiziert (positiv) erkennt und ebenfalls mit einer Wahrscheinlichkeit von 99\nProzent eine nicht infizierte Person als nicht infiziert erkennt\n(negativ). Sei $A$ das Ereignis \"infiziert\" und $B$ das Ereignis \"Test positiv\". \nDie angegebenen Wahrscheinlichkeiten sind bedingte Wahrscheinlichkeiten,\nund zwar $P(B|A)=0.99$ und $P(\\bar B|\\bar A)=0.99$. Als Testanwender \ninteressiert man sich jedoch eher für die\nbedingte Wahrscheinlichkeit, tatsächlich infiziert zu sein, \nwenn der Test positiv ausfällt, also für $P(A|B)$. Oder auch für die \nbedingte Wahrscheinlichkeit infiziert zu sein, obwohl der Test\nnegativ ausfällt, also $P(A|\\bar B)$.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Studierneigung und Elternhaus\nEs wird berichtet, dass 50 Prozent der Studierenden ein\n\"akademisches Elternhaus\" haben (die Zahlen in diesem\nBeispiel sind fiktiv). Es handelt sich um die bedingte\nWahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person\nstudiert. Hier sind die Ereignisse $A$: \"Kind studiert\", und\n$B$: \"akademisches Elternhaus\". Gegeben ist $P(B|A)=0.5$.\nFür die Bewertung der Chancengleichheit ist jedoch die\nbedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert,\nwenn es aus einem akademischen Elternhaus stammt, also\n$P(A|B)$ - und zwar im Vergleich zu der bedingten Wahrscheinlichkeit, \ndass ein Kind studiert, das nicht aus einem akademischen Elternhaus stammt,\nalso $P(A|\\bar B)$.\n:::\n\nWie kann man eine bedingte Wahrscheinlichkeit \"umdrehen\"? Wie verhält\nsich $P(B|A)$ zu $P(A|B)?$ Das sehen wir in Kürze im @sec-bayes.\n\nBedingte Wahrscheinlichkeiten sind auch nützlich, um die\nWahrscheinlichkeit zu berechnen, dass mehrere Ereignisse gemeinsam\npassieren. Dazu formen wir die Definition der bedingten\nWahrscheinlichkeit einfach um. Aus \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \nwird \n$$\nP(A\\cap B)={P(B)P(A|B)}.\n$$\nDas lässt sich auf mehr als zwei Ereignisse erweitern: \n$$\n\\begin{align*}\nP(A\\cap B \\cap C)&=P(C)P(B|C)P(A|B\\cap C)\\\\\nP(A\\cap B\\cap C\\cap D)&=P(D)P(C|D)P(B|C\\cap D)P(A|B\\cap C\\cap D)\n\\end{align*}\n$$ \nDiese Formeln sind bei näherer Betrachtung intuitiv: Die\nWahrscheinlichkeit, dass $A$, $B$ und $C$ gemeinsam eintreten, ergibt\nsich, indem man zuerst eine der unbedingten Wahrscheinlichkeiten nimmt\n(z.B. $P(C)$). Nun ist $C$ quasi eingetreten und wir arbeiten unter der\nBedingung $C$ weiter. Die Wahrscheinlichkeit $P(C)$ wird jetzt mit der\nbedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,\nalso $P(B|C)$. Nun sind $B$ und $C$ eingetreten, und auf der nächsten\nStufe multiplizieren wir deshalb mit $P(A|B\\cap C)$. Eine andere\nReihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B. \n$$\nP(A\\cap B\\cap C)=P(A)P(B|A)P(C|A\\cap B).\n$$\nFür jede Reihenfolge gelangt man zum gleichen Ergebnis.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Kündigungswahrscheinlichkeiten\n<img src=\"images/AdobeStock_19427911b_Sparbuch.jpeg\" align=\"right\" width=50%\" style=\"padding-left:15px;padding-top:5px;\"/>\nEine Bank möchte die Dauer ihrer Kundenbeziehungen modellieren. \nDazu definiert sie die Ereignisse $A_d$: \"die Kundenbeziehung besteht seit $d$ Jahren\"\nfür $d=0,1,\\ldots,D$. Die Bank kennt (aus Erfahrung) die\nWahrscheinlichkeit für eine Kündigung in Abhängigkeit von der Kundendauer\n(und geht davon aus, dass sich diese Wahrscheinlichkeiten im\nLaufe der Zeit nicht verändern).\nMit $k_{d,d+1}$ wird die Wahrscheinlichkeit bezeichnet, dass ein\nKunde, der schon seit $d$ Jahren Kunde ist, innerhalb des nächsten\nJahres kündigt. Es handelt sich also um die bedingten Wahrscheinlichkeiten\n$$\nk_{d,d+1}=1-P(A_{d+1}|A_d).\n$$\nDie Wahrscheinlichkeit, dass ein Neukunde ($d=0$) nach 5 Jahren immer\nnoch Kunde ist, beträgt\n$$\n\\begin{align*}\nP(A_5)&=P(A_5|A_4)P(A_4|A_3)P(A_3|A_2)P(A_2|A_1)P(A_1|A_0)\\\\\n&=(1-k_{4,5})(1-k_{3,4})(1-k_{2,3})(1-k_{1,2})(1-k_{0,1}).\n\\end{align*}\n$$\n:::\n\n## Totale Wahrscheinlichkeit {#sec-total}\n\nAus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte\nWahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum\n$\\Omega$ in eine Partition. Unter einer **Partition** versteht man eine\nZerlegung in disjunkte Mengen $A_1, A_2, \\ldots, A_n$, so dass die\nVereinigungsmenge der $A_1,\\ldots, A_n$ wieder $\\Omega$ ergibt. Die\n\"Partitionierung eines Rinds\" könnte etwa so aussehen:\n\n```{r echo=FALSE,out.width=\"60%\",fig.align='center'}\nknitr::include_graphics(\"images/AdobeStock_101998556b_ButcherGuide.jpeg\")\n```\n\nDas Rind wird also vollständig in Teilmengen aufgeteilt, und alle Teilmengen\nzusammen ergeben wieder das gesamte Rind.\n\n::: callout-important\n## Satz von der totalen Wahrscheinlichkeit\n\nSei $A_1,\\ldots,A_n$ eine Partition des Ergebnisraums $\\Omega$.\nFür jedes Ereignis $A_i$, $i=1,\\ldots,n$, sei\ndie bedingte Wahrscheinlichkeit $P(B|A_i)$ gegeben, \nwobei $B$ irgendein Ereignis ist. \nDann gilt für die **unbedingte (totale) Wahrscheinlichkeit** von $B$:\n$$\nP(B)=\\sum_{i=1}^n P(B|A_i)P(A_i).\n$$ \n:::\n\nZur Begründung: Da $A_1, A_2, \\ldots, A_n$ eine Partition ist, gilt\n$$\n\\begin{align*}\nP(B) &= P((B\\cap A_1)\\cup (B\\cap A_2)\\cup\\ldots\\cup(B\\cap A_n))\\\\\n&= P(B\\cap A_1)+P(B\\cap A_2)+\\ldots+P(B\\cap A_n)\\\\\n&= P(B|A_1)P(A_1)+P(B|A_2)P(A_2)+\\ldots+P(B|A_n)P(A_n).\n\\end{align*}\n$$\nEine besonders simple Partition ist die Unterteilung in\n$A$ und $\\bar A$. Dann lautet die Formel\n$$\nP(B)=P(B|A)P(A)+P(B|\\bar A)P(\\bar A).\n$$ \n\nDie unbedingte Wahrscheinlichkeit ergibt sich als gewichtete\nSumme aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch\ndie Wahrscheinlichkeiten der bedingenden Ereignisse. Dieser Zusammenhang \nist dann besonders nützlich, wenn die bedingten Wahrscheinlichkeiten\nschon bekannt oder einfach zu ermitteln sind, die unbedingte \nWahrscheinlichkeit jedoch schwierig zu finden ist.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Kreditausfall und Konjunktur\nEine Bank vergibt einen Kredit an ein Unternehmen. Das Unternehmen\nzahlt den Kredit mit einer Wahrscheinlichkeit von 0.95 zurück,\nwenn die konjunkturelle Lage positiv ist. Bei einer schwachen\nKonjunktur wird der Kredit jedoch nur mit einer Wahrscheinlichkeit\nvon 0.8 zurückgezahlt. Die Wahrscheinlichkeit einer guten\nKonjunktur sei 0.75 (und die Konjunktur kann nur gut oder\nschlecht sein).\n\nAls Ereignisse definiert man $A$: \"gute Konjunktur\" und\n$B$: \"Kredit wird zurückgezahlt\". Gegeben sind die bedingten Wahrscheinlichkeiten\n$P(B|A)=0.95$ und $P(B|\\bar A)=0.8$. Außerdem ist $P(A)=0.75$\nbekannt. Damit ergibt sich die Wahrscheinlichkeit einer\nRückzahlung als\n$$\n\\begin{align*}\nP(B)&=P(B|A)P(A)+P(B|\\bar A)P(\\bar A)\\\\\n&=0.95\\cdot 0.75+0.8\\cdot 0.25\\\\\n&=0.9125\n\\end{align*}\n$$\n:::\n\n## Satz von Bayes {#sec-bayes}\n\nDer Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten\n$P(A|B)$ und $P(B|A)$ zueinander in Beziehung. \n\n::: callout-important\n## Satz von Bayes\n\nFür zwei Ereignisse $A$ und $B$ mit $P(B)>0$ gilt der **Satz von Bayes**\n(engl. Bayes theorem)\n$$\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\n$$ \n:::\n\nUm die Bedingung und das Bedingte zu vertauschen, braucht man also\ndie beiden unbedingten Wahrscheinlichkeiten. Die Herleitung des Satzes von\nBayes ergibt sich aus der Definition der bedingten Wahrscheinlichkeit.\nWegen \n$$\n\\begin{align*}\nP(A|B) &= \\frac{P(A\\cap B)}{P(B)}\\\\\nP(B|A) &= \\frac{P(A\\cap B)}{P(A)}\n\\end{align*}\n$$ \ngilt \n$$\nP(A|B)P(B)=P(B|A)P(A).\n$$ \nDaraus folgt durch Umstellen unmittelbar der Satz von Bayes.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Medizinischer Schnelltest\nWir betrachten wieder den medizinischen Schnelltest und die beiden\nEreignisse $A$: \"Person ist infiziert\", und $B$: \"Test positiv\". Neben\nden beiden bedingten Wahrscheinlichkeiten $P(B|A)=0.99$ und \n$P(\\bar B|\\bar A)=0.99$ sei bekannt, dass $P(A)=0.001$, d.h. nur mit\neiner Wahrscheinlichkeit von 0.1 Prozent ist eine zufällig aus der\nPopulation ausgewählte Person infiziert. \n\nWie groß ist Wahrscheinlichkeit, dass eine Person infiziert ist, bei\nder der Schnelltest ein positives Ergebnis zeigt? Gesucht ist also\n$P(A|B)$. Nach dem Satz von Bayes gilt\n$$\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\n$$\nNach dem Satz der totalen Wahrscheinlichkeit berechnet man\n$$\n\\begin{align*}\nP(B) &=P(B|A)P(A)+P(B|\\bar A)P(\\bar A)\\\\\n&= P(B|A)P(A)+[1-P(\\bar B|\\bar A)][1-P(A)]\\\\\n&= 0.99\\cdot 0.001+[1-0.99]\\cdot [1-0.001]\\\\\n&= 0.01098.\n\\end{align*}\n$$\nDie Wahrscheinlichkeit, dass eine Person mit einem positiven Testergebnis\ntatsächlich infiziert ist, ist folglich immer noch sehr klein, obwohl der\nTest scheinbar sehr genau arbeitet. \n:::\n\n## Unabhängigkeit {#sec-unabhaengig1}\n\n::: callout-note\n## Definition: Unabhängigkeit\n\nZwei Ereignisse $A$ und $B$ heißen **unabhängig** \n(oder auch **stochastisch unabhängig**, \nengl. independent bzw. stochastically independent), wenn \n$$\nP(A\\cap B)=P(A)\\cdot P(B).\n$$ \n:::\n\nEine alternative (fast äquivalente) Definition der Unabhängigkeit\nlautet so: Die Ereignisse $A$ und $B$ heißen unabhängig, wenn gilt \n$$\nP(A|B)=P(A)\n$$ \nbzw. wenn \n$$\nP(B|A)=P(B).\n$$ \nDie Definition über die bedingten Wahrscheinlichkeiten ist etwas\nintuitiver. Wenn das Wissen darüber, ob $A$ eingetreten ist oder nicht,\nfür die Wahrscheinlichkeit von $B$ keine Rolle spielt (und umgekehrt),\ndann sind $A$ und $B$ unabhängig.\n\nAchten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln.\nZur Erinnerung: Die Ereignisse $A$ und $B$ sind disjunkt, wenn\n$A\\cap B=\\emptyset$, so dass $P(A\\cap B)=0$. Disjunkte Ereignisse können\nalso nicht beide zusammen eintreten. Wenn $A$ eintritt, kann man sicher\nsein, dass $B$ nicht eintritt. Und wenn $B$ eintritt, kann man sicher\nsein, dass $A$ nicht eintritt. Unabhängige Ereignisse können hingegen\nsehr wohl zusammen eintreten.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Medizinischer Schnelltest\nWir betrachten wieder den medizinischen Schnelltest und die beiden\nEreignisse $A$: \"Person ist infiziert\", und $B$: \"Test positiv\".\nZur Erinnerung: Gegeben sind die beiden bedingten Wahrscheinlichkeiten\n$P(B|A)=0.99$ und $P(\\bar B|\\bar A)=0.99$ sowie die\nunbedingte Wahrscheinlichkeit $P(A)=0.001$. Außerdem haben wir\nim letzten Abschnitt $P(B)=0.01098$ berechnet. \n\nSind die Ereignisse $A$ und $B$ unabhängig? Die Antwort lautet \"nein\",\ndenn es gilt\n$$\n0.99=P(B|A)\\neq P(B)=0.01098.\n$$\nUnabhängigkeit ist in diesem Fall natürlich auch nicht zu erwarten.\nEin Schnelltest, dessen Ergebnis keinen Einfluss auf die \nEinschätzung der Wahrscheinlichkeit einer Infektion hätte, wäre\nvollkommen nutzlos.\n:::\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"output-file":"02-wahrscheinlichkeit.html"},"language":{"toc-title-document":"Inhaltsverzeichnis","toc-title-website":"Auf dieser Seite","related-formats-title":"Andere Formate","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Quelle","section-title-abstract":"Zusammenfassung","section-title-appendices":"Anhang","section-title-footnotes":"Fußnoten","section-title-references":"Literatur","section-title-reuse":"Wiederverwendung","section-title-copyright":"Urheberrechte","section-title-citation":"Zitat","appendix-attribution-cite-as":"Bitte zitieren Sie diese Arbeit als:","appendix-attribution-bibtex":"Mit BibTeX zitieren:","title-block-author-single":"Autor","title-block-author-plural":"Authors","title-block-affiliation-single":"Zugehörigkeit","title-block-affiliation-plural":"Zugehörigkeiten","title-block-published":"Veröffentlicht am","title-block-modified":"Geändert","callout-tip-title":"Beispiel","callout-note-title":"Definition","callout-warning-title":"Warnung","callout-important-title":"Wichtig","callout-caution-title":"Vorsicht","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Gesamten Code zeigen","code-tools-hide-all-code":"Gesamten Code verbergen","code-tools-view-source":"Quellcode anzeigen","code-tools-source-code":"Quellcode","code-line":"Zeile","code-lines":"Zeilen","copy-button-tooltip":"In die Zwischenablage kopieren","copy-button-tooltip-success":"Kopiert","repo-action-links-edit":"Seite editieren","repo-action-links-source":"Quellcode anzeigen","repo-action-links-issue":"Problem melden","back-to-top":"Zurück nach oben","search-no-results-text":"Keine Treffer","search-matching-documents-text":"Treffer","search-copy-link-title":"Link in die Suche kopieren","search-hide-matches-text":"Zusätzliche Treffer verbergen","search-more-match-text":"weitere Treffer in diesem Dokument","search-more-matches-text":"weitere Treffer in diesem Dokument","search-clear-button-title":"Zurücksetzen","search-detached-cancel-button-title":"Abbrechen","search-submit-button-title":"Abschicken","search":"Search","toggle-section":"Abschnitt umschalten","toggle-sidebar":"Seitenleiste umschalten","toggle-dark-mode":"Dunkelmodus umschalten","toggle-reader-mode":"Lesemodus umschalten","toggle-navigation":"Navigation umschalten","crossref-fig-title":"Abbildung","crossref-tbl-title":"Tabelle","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Korollar","crossref-prp-title":"Aussage","crossref-cnj-title":"Annahme","crossref-def-title":"Definition","crossref-exm-title":"Beispiel","crossref-exr-title":"Übungsaufgabe","crossref-ch-prefix":"Kapitel","crossref-apx-prefix":"Anhang","crossref-sec-prefix":"Kapitel","crossref-eq-prefix":"Gleichung","crossref-lof-title":"Abbildungsverzeichnis","crossref-lot-title":"Tabellenverzeichnis","crossref-lol-title":"Listingverzeichnis","environment-proof-title":"Beweis","environment-remark-title":"Anmerkung","environment-solution-title":"Lösung","listing-page-order-by":"Sortieren nach","listing-page-order-by-default":"Voreinstellung","listing-page-order-by-date-asc":"Datum (aufsteigend)","listing-page-order-by-date-desc":"Neueste","listing-page-order-by-number-desc":"Absteigend","listing-page-order-by-number-asc":"Aufsteigend","listing-page-field-date":"Datum","listing-page-field-title":"Titel","listing-page-field-description":"Beschreibung","listing-page-field-author":"Autor:in","listing-page-field-filename":"Dateiname","listing-page-field-filemodified":"Geändert","listing-page-field-subtitle":"Untertitel","listing-page-field-readingtime":"Lesezeit","listing-page-field-categories":"Kategorien","listing-page-minutes-compact":"{0} min","listing-page-category-all":"alle","listing-page-no-matches":"Keine Treffer"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"source","callout-icon":false,"other-links-title":"Weitere Links","code-links-title":"Code-Links","launch-dev-container-title":"Dev Container starten","launch-binder-title":"Binder starten","title-block-keywords":"Schlüsselwörter","search-label":"Suchen","listing-page-field-wordcount":"Wortanzahl","listing-page-words":"{0} Wörter","theme":"sandstone"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}