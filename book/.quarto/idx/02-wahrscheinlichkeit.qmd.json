<<<<<<< Updated upstream
{"title":"Wahrscheinlichkeit","markdown":{"headingText":"Wahrscheinlichkeit","headingAttr":{"id":"sec-wahrscheinlichkeit","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Definition {#sec-definition}\n\nManche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die\nWahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung\n(Funktion) angegeben.\n\n:::: callout-note\n## Definition: Wahrscheinlichkeit\nEine Abbildung $P: A \\mapsto P(A)$ heißt **Wahrscheinlichkeit**\n(engl. probability), wenn gilt\n\n- Nichtnegativität: $P(A)\\ge 0$ für alle Ereignisse $A$\n\n- Normierung: $P(\\Omega)=1$\n\n- Additivität: Für disjunkte Ereignisse $A$ und $B$ (d.h.\n$A\\cap B=\\emptyset$) ist \n$$\nP(A\\cup B)=P(A)+P(B).\n$$\n:::\n\nDiese Eigenschaften, die eine Wahrscheinlichkeit erfüllen\nmuss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten\nhergeleitet werden, daher spricht man auch von **Axiomen**. Etwas\nallgemeiner und präziser wurden diese Axiome 1933 \nvon Andrey Kolmogorov (1903-1987) für eine saubere mathematische \nFundierung der Wahrscheinlichkeitstheorie eingeführt.\n\nAus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum\nBeispiel:\n\n-   Komplementärereignis: $P(\\bar A)=1-P(A)$\n\n-   Additionssatz: $P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$\n\n-   Monotonität: Wenn $A\\subseteq B$, dann ist $P(A)\\le P(B)$.\n\n## Laplace-Experimente {#sec-laplace}\n\nEine besonders einfache Art von Zufallsvorgängen sind die sogenannten\nLaplace-Experimente.\n\n::: callout-note\n## Definition: Laplace-Experiment\nEin Zufallsvorgang heißt **Laplace-Experiment** (engl. Laplace experiment), \nwenn es nur endlich viele Ergebnisse gibt (d.h. wenn $|\\Omega|=n$) und wenn \nalle Elementarereignisse als gleich wahrscheinlich angenommen werden können.\n:::\n\n<img src=\"images/AdobeStock_332174058b_Kronkorken.jpeg\" align=\"right\" width=\"35%\"/>\nBeachten Sie, dass es sich dabei um eine Aussage handelt, die aus\nunserem Alltagswissen herrührt, nicht aus mathematischen Überlegungen\noder Herleitungen! Ein\ntypisches, einfaches Beispiel für ein Laplace-Experiment sind\nWürfelwürfe. Es ist aus unserem Alltagswissen heraus plausibel, davon\nauszugehen, dass alle Augenzahlen eines normalen Würfels gleich wahrscheinlich\nsind. Auch bei einer Münze ist es naheliegend, dass die\nWahrscheinlichkeit für Kopf und die Wahrscheinlichkeit für Zahl gleich\nsind. Hingegen würde man beim Werfen eines Kronkorkens nicht unbedingt\nvermuten, dass beide Seiten mit der gleichen Wahrscheinlichkeit oben\nliegen. Hier liegt also kein Laplace-Experiment vor.\n\nBei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein\nErgebnis $A$ eintritt, leicht zu ermitteln. Sie beträgt \n$$\nP(A)=\\frac{|A|}{|\\Omega|},\n$$ \nalso die Anzahl der Ergebnisse in $A$ dividiert durch die Anzahl\naller Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss\nman also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist\nmanchmal sehr einfach, kann aber bei großen Mengen kompliziert sein. In\nsolchen Fällen hilft der Teilbereich der Mathematik weiter, den man\n\"Kombinatorik\" nennt. Wir gehen in diesem Kurs jedoch nicht näher auf\nkombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei\ndenen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Ein Würfel als Laplace-Experiment\nEin Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.\nDie Anzahl der Ergebnisse in $\\Omega$ beträgt 6. Sei $A$ das\nEreignis \"Eine gerade Zahl wird geworfen\", also $A=\\{2,4,6\\}$. Dann\nist \n$$\n\\begin{align*}\nP(A)&=\\frac{|A|}{|\\Omega|}\\\\\n&=\\frac{3}{6}\\\\\n&=0.5.\n\\end{align*}\n$$\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Zwei Würfel als Laplace-Experiment\nZwei gleich aussehende Würfel werden geworfen. Nun spielt es eine\nRolle, wie man die Ergebnismenge festlegt. Wir wählen \n$$\n\\begin{align*}\n\\Omega=\\{&11,12,13,14,15,16,\\\\\n&21,22,23,24,25,26,\\\\\n&31,32,33,34,35,36,\\\\\n&41,42,43,44,45,46,\\\\\n&51,52,53,54,55,56,\\\\\n&61,62,63,64,65,66\\}\n\\end{align*}\n$$ \nweil es sich dann um ein Laplace-Experiment handelt. Die\nErgebnisse \"24\" und \"42\" sind zwar nicht unterscheidbar, wenn die\nWürfel gleich aussehen, aber wir können den zuerst geworfenen Würfel\n(oder z.B. den weiter links liegenden) als ersten Würfel bezeichnen. Dass\nalle 36 Ergebnisse gleich wahrscheinlich sind, können wir (nur) mit\nunserem Alltagswissen begründen, nicht aus der Mathematik heraus.\n\nSei $A$ das Ereignis \"Die Augenzahlen der beiden Würfel\nunterscheiden sich um 2\", d.h. \n$$\nA=\\{13,24,31,35,42,46,53,64\\}.\n$$ \nDann gilt \n$$\n\\begin{align*}\nP(A)&=\\frac{|A|}{|\\Omega|}\\\\\n&=\\frac{8}{36}\\\\\n&=\\frac{2}{9}\\\\\n&\\approx 0.2222.\n\\end{align*}\n$$ \nDa die beiden Würfel nicht unterscheidbar sind, wäre als Ergebnismenge auch \n$$\n\\begin{align*}\n\\Omega=\\{\n&11,12,13,14,15,16,\\\\\n&22,23,24,25,26,\\\\\n&33,34,35,36,\\\\\n&44,45,46,\\\\\n&55,56,\\\\\n&66\\}\n\\end{align*}\n$$ \nmöglich gewesen. Die Ergebnismenge hätte dann 21 Elemente. Die Annahme \neines Laplace-Experiments wäre jedoch in diesem Fall nicht\nmehr korrekt, denn die Elementarereignisse \"11\" und \"12\" sind\nbeispielsweise nicht gleich wahrscheinlich. Wir wissen aus unserer\nErfahrung, dass ein Pasch seltener auftritt. \n:::\n\nDie Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere\nVorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge so\ngewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte man\ndas tun.\n\n## Bedingte Wahrscheinlichkeit {#sec-bedingt}\n\nManchmal gibt es begrenzte Informationen über einen Zufallsvorgang. Dann\nkennt man zwar nicht das realisierte Ergebnis, kann aber die Menge der\nmöglichen Ergebnisse eingrenzen. Dadurch ändern sich die\nWahrscheinlichkeiten für Ereignisse.\n\n::: callout-note\n## Definition: Bedingte Wahrscheinlichkeit\nWir betrachten zwei Ereignsse $A$ und $B$ mit $P(B)>0$. Dann heißt \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \ndie **bedingte Wahrscheinlichkeit** (engl. conditional probability) von $A$ gegeben $B$.\n:::\n\nDie Notation $A|B$ steht nicht für ein bestimmtes Ereignis, sondern\nzeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten,\nnämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über\nWahrscheinlichkeiten ist es nicht immer einfach (aber sehr wichtig!),\nzwischen der Wahrscheinlichkeit $P(A\\cap B)$ und der bedingten\nWahrscheinlichkeit $P(A|B)$ zu unterscheiden. Wenn man $P(A\\cap B)$\nmeint, spricht man von der Wahrscheinlichkeit, dass $A$ und $B$\neintreten. Wenn man $P(A|B)$ meint, sagt man $A$ gegeben $B$, oder: $A$\nwenn $B$, oder: $A$ unter der Bedingung $B$. Wenn man ausdrücklich\nangeben möchte, dass eine Wahrscheinlichkeit *keine* bedingte\nWahrscheinlichkeit ist, nennt man sie auch eine unbedingte\nWahrscheinlichkeit (engl. unconditional probability).\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Bedingte Wahrscheinlichkeit für einen Würfel\nEin Würfel wird geworfen. Wir betrachten die beiden Ereignisse\n$A=\\{2,4,6\\}$ (\"gerade Zahl\"), und $B=\\{3,4,5,6\\}$ (\"eine Zahl größer\nals 2\"). Die bedingte Wahrscheinlichkeit von $A$ gegeben $B$ beträgt\n$$\n\\begin{align*}\nP(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{3,4,5,6\\})}\\\\\n&=\\frac{1}{2}\n\\end{align*}\n$$ \nund die bedingte Wahrscheinlichkeit von $B$ gegeben $A$ lautet \n$$\n\\begin{align*}\nP(B|A)&=\\frac{P(B\\cap A)}{P(A)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{2,4,6\\})}\\\\\n&=\\frac{2}{3}.\n\\end{align*}\n$$\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Bedingte Wahrscheinlichkeit bei zwei Würfeln\nZwei Würfel werden geworfen, ohne dass Sie es sehen können.\nSie fragen, ob eine Sechs geworfen wurde. Die Frage wird\nwahrheitsgemäß bejaht. Wie groß ist die Wahrscheinlichkeit,\ndass ein 6er-Pasch geworfen wurde? \n\nSei $A$ das Ereignis \"6er-Pasch\" und $B$ das Ereignis\n\"mindest eine Sechs wurde geworfen\", d.h.\n$$\n\\begin{align*}\nA &= \\{66\\}\\\\\nB &= \\{16,26,36,46,56,61,62,63,64,65,66\\}.\n\\end{align*}\n$$\nDaher gilt\n$$\n\\begin{align*}\nP(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{66\\})}{P(\\{16,26,36,46,56,61,62,63,64,65,66\\})}\\\\\n&=1/11.\n\\end{align*}\n$$ \n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Erwerbsstatus und Geschlecht\n<img src=\"images/AdobeStock_172088861b_Umfrage.jpeg\" align=\"right\" width=50%\" style=\"padding-left:15px;padding-top:5px;padding-bottom:5px;\"/>\nBei einer Umfrage werden Personen zufällig ausgewählt und befragt.\nSei $A$ das Ereignis \"die befragte Person ist weiblich\". Sei $B$ das\nEreignis \"die befragte Person arbeitet in Teilzeit\". Zwischen der\n(unbedingten) Wahrscheinlichkeit $P(B)$ und der bedingten\nWahrscheinlichkeit $P(B|A)$ besteht ein Unterschied. Die\nWahrscheinlichkeit $P(B)$ steht dafür, dass eine zufällig\nausgewählte Person in Teilzeit arbeitet, und diese Person kann\nmännlich oder weiblich sein. Dagegen ist $P(B|A)$ die\nWahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit\narbeitet.\n:::\n\nIn den Medien werden manchmal bedingte Wahrscheinlichkeiten berichtet,\nohne dass das explizit erwähnt wird. Oftmals sind die berichteten\nbedingten Wahrscheinlichkeiten gar nicht die, für die man sich\neigentlich interessiert, weil die Bedingung und das Bedingte quasi\nfalsch herum angeordnet sind. Man interessiert sich für $P(A|B)$, \nberichtet wird aber $P(B|A)$.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Medizinischer Schnelltest\n<img src=\"images/AdobeStock_338942366b_Medizintest.jpeg\" align=\"right\" width=50%\" style=\"padding-left:15px;padding-top:5px;\"/>Es wird berichtet, dass \nein medizinischer Schnelltest mit einer\nWahrscheinlichkeit von 99 Prozent eine infizierte Person korrekt als\ninfiziert (positiv) erkennt und ebenfalls mit einer Wahrscheinlichkeit von 99\nProzent eine nicht infizierte Person als nicht infiziert erkennt\n(negativ). Sei $A$ das Ereignis \"infiziert\" und $B$ das Ereignis \"Test positiv\". \nDie angegebenen Wahrscheinlichkeiten sind bedingte Wahrscheinlichkeiten,\nund zwar $P(B|A)=0.99$ und $P(\\bar B|\\bar A)=0.99$. Als Testanwender \ninteressiert man sich jedoch eher für die\nbedingte Wahrscheinlichkeit, tatsächlich infiziert zu sein, \nwenn der Test positiv ausfällt, also für $P(A|B)$. Oder auch für die \nbedingte Wahrscheinlichkeit infiziert zu sein, obwohl der Test\nnegativ ausfällt, also $P(A|\\bar B)$.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Studierneigung und Elternhaus\nEs wird berichtet, dass 50 Prozent der Studierenden ein\n\"akademisches Elternhaus\" haben (die Zahlen in diesem\nBeispiel sind fiktiv). Es handelt sich um die bedingte\nWahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person\nstudiert. Hier sind die Ereignisse $A$: \"Kind studiert\", und\n$B$: \"akademisches Elternhaus\". Gegeben ist $P(B|A)=0.5$.\nFür die Bewertung der Chancengleichheit ist jedoch die\nbedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert,\nwenn es aus einem akademischen Elternhaus stammt, also\n$P(A|B)$ - und zwar im Vergleich zu der bedingten Wahrscheinlichkeit, \ndass ein Kind studiert, das nicht aus einem akademischen Elternhaus stammt,\nalso $P(A|\\bar B)$.\n:::\n\nWie kann man eine bedingte Wahrscheinlichkeit \"umdrehen\"? Wie verhält\nsich $P(B|A)$ zu $P(A|B)?$ Das sehen wir in Kürze im @sec-bayes.\n\nBedingte Wahrscheinlichkeiten sind auch nützlich, um die\nWahrscheinlichkeit zu berechnen, dass mehrere Ereignisse gemeinsam\npassieren. Dazu formen wir die Definition der bedingten\nWahrscheinlichkeit einfach um. Aus \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \nwird \n$$\nP(A\\cap B)={P(B)P(A|B)}.\n$$\nDas lässt sich auf mehr als zwei Ereignisse erweitern: \n$$\n\\begin{align*}\nP(A\\cap B \\cap C)&=P(C)P(B|C)P(A|B\\cap C)\\\\\nP(A\\cap B\\cap C\\cap D)&=P(D)P(C|D)P(B|C\\cap D)P(A|B\\cap C\\cap D)\n\\end{align*}\n$$ \nDiese Formeln sind bei näherer Betrachtung intuitiv: Die\nWahrscheinlichkeit, dass $A$, $B$ und $C$ gemeinsam eintreten, ergibt\nsich, indem man zuerst eine der unbedingten Wahrscheinlichkeiten nimmt\n(z.B. $P(C)$). Nun ist $C$ quasi eingetreten und wir arbeiten unter der\nBedingung $C$ weiter. Die Wahrscheinlichkeit $P(C)$ wird jetzt mit der\nbedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,\nalso $P(B|C)$. Nun sind $B$ und $C$ eingetreten, und auf der nächsten\nStufe multiplizieren wir deshalb mit $P(A|B\\cap C)$. Eine andere\nReihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B. \n$$\nP(A\\cap B\\cap C)=P(A)P(B|A)P(C|A\\cap B).\n$$\nFür jede Reihenfolge gelangt man zum gleichen Ergebnis.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Kündigungswahrscheinlichkeiten\n<img src=\"images/AdobeStock_19427911b_Sparbuch.jpeg\" align=\"right\" width=50%\" style=\"padding-left:15px;padding-top:5px;\"/>\nEine Bank möchte die Dauer ihrer Kundenbeziehungen modellieren. \nDazu definiert sie die Ereignisse $A_d$: \"die Kundenbeziehung besteht seit $d$ Jahren\"\nfür $d=0,1,\\ldots,D$. Die Bank kennt (aus Erfahrung) die\nWahrscheinlichkeit für eine Kündigung in Abhängigkeit von der Kundendauer\n(und geht davon aus, dass sich diese Wahrscheinlichkeiten im\nLaufe der Zeit nicht verändern).\nMit $k_{d,d+1}$ wird die Wahrscheinlichkeit bezeichnet, dass ein\nKunde, der schon seit $d$ Jahren Kunde ist, innerhalb des nächsten\nJahres kündigt. Es handelt sich also um die bedingten Wahrscheinlichkeiten\n$$\nk_{d,d+1}=1-P(A_{d+1}|A_d).\n$$\nDie Wahrscheinlichkeit, dass ein Neukunde ($d=0$) nach 5 Jahren immer\nnoch Kunde ist, beträgt\n$$\n\\begin{align*}\nP(A_5)&=P(A_5|A_4)P(A_4|A_3)P(A_3|A_2)P(A_2|A_1)P(A_1|A_0)\\\\\n&=(1-k_{4,5})(1-k_{3,4})(1-k_{2,3})(1-k_{1,2})(1-k_{0,1}).\n\\end{align*}\n$$\n:::\n\n## Totale Wahrscheinlichkeit {#sec-total}\n\nAus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte\nWahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum\n$\\Omega$ in eine Partition. Unter einer **Partition** versteht man eine\nZerlegung in disjunkte Mengen $A_1, A_2, \\ldots, A_n$, so dass die\nVereinigungsmenge der $A_1,\\ldots, A_n$ wieder $\\Omega$ ergibt. Die\n\"Partitionierung eines Rinds\" könnte etwa so aussehen:\n\n```{r echo=FALSE,out.width=\"60%\",fig.align='center'}\nknitr::include_graphics(\"images/AdobeStock_101998556b_ButcherGuide.jpeg\")\n```\n\nDas Rind wird also vollständig in Teilmengen aufgeteilt, und alle Teilmengen\nzusammen ergeben wieder das gesamte Rind.\n\n::: callout-important\n## Satz von der totalen Wahrscheinlichkeit\n\nSei $A_1,\\ldots,A_n$ eine Partition des Ergebnisraums $\\Omega$.\nFür jedes Ereignis $A_i$, $i=1,\\ldots,n$, sei\ndie bedingte Wahrscheinlichkeit $P(B|A_i)$ gegeben, \nwobei $B$ irgendein Ereignis ist. \nDann gilt für die **unbedingte (totale) Wahrscheinlichkeit** von $B$:\n$$\nP(B)=\\sum_{i=1}^n P(B|A_i)P(A_i).\n$$ \n:::\n\nZur Begründung: Da $A_1, A_2, \\ldots, A_n$ eine Partition ist, gilt\n$$\n\\begin{align*}\nP(B) &= P((B\\cap A_1)\\cup (B\\cap A_2)\\cup\\ldots\\cup(B\\cap A_n))\\\\\n&= P(B\\cap A_1)+P(B\\cap A_2)+\\ldots+P(B\\cap A_n)\\\\\n&= P(B|A_1)P(A_1)+P(B|A_2)P(A_2)+\\ldots+P(B|A_n)P(A_n).\n\\end{align*}\n$$\nEine besonders simple Partition ist die Unterteilung in\n$A$ und $\\bar A$. Dann lautet die Formel\n$$\nP(B)=P(B|A)P(A)+P(B|\\bar A)P(\\bar A).\n$$ \n\nDie unbedingte Wahrscheinlichkeit ergibt sich als gewichtete\nSumme aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch\ndie Wahrscheinlichkeiten der bedingenden Ereignisse. Dieser Zusammenhang \nist dann besonders nützlich, wenn die bedingten Wahrscheinlichkeiten\nschon bekannt oder einfach zu ermitteln sind, die unbedingte \nWahrscheinlichkeit jedoch schwierig zu finden ist.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Kreditausfall und Konjunktur\nEine Bank vergibt einen Kredit an ein Unternehmen. Das Unternehmen\nzahlt den Kredit mit einer Wahrscheinlichkeit von 0.95 zurück,\nwenn die konjunkturelle Lage positiv ist. Bei einer schwachen\nKonjunktur wird der Kredit jedoch nur mit einer Wahrscheinlichkeit\nvon 0.8 zurückgezahlt. Die Wahrscheinlichkeit einer guten\nKonjunktur sei 0.75 (und die Konjunktur kann nur gut oder\nschlecht sein).\n\nAls Ereignisse definiert man $A$: \"gute Konjunktur\" und\n$B$: \"Kredit wird zurückgezahlt\". Gegeben sind die bedingten Wahrscheinlichkeiten\n$P(B|A)=0.95$ und $P(B|\\bar A)=0.8$. Außerdem ist $P(A)=0.75$\nbekannt. Damit ergibt sich die Wahrscheinlichkeit einer\nRückzahlung als\n$$\n\\begin{align*}\nP(B)&=P(B|A)P(A)+P(B|\\bar A)P(\\bar A)\\\\\n&=0.95\\cdot 0.75+0.8\\cdot 0.25\\\\\n&=0.9125\n\\end{align*}\n$$\n:::\n\n## Satz von Bayes {#sec-bayes}\n\nDer Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten\n$P(A|B)$ und $P(B|A)$ zueinander in Beziehung. \n\n::: callout-important\n## Satz von Bayes\n\nFür zwei Ereignisse $A$ und $B$ mit $P(B)>0$ gilt der **Satz von Bayes**\n(engl. Bayes theorem)\n$$\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\n$$ \n:::\n\nUm die Bedingung und das Bedingte zu vertauschen, braucht man also\ndie beiden unbedingten Wahrscheinlichkeiten. Die Herleitung des Satzes von\nBayes ergibt sich aus der Definition der bedingten Wahrscheinlichkeit.\nWegen \n$$\n\\begin{align*}\nP(A|B) &= \\frac{P(A\\cap B)}{P(B)}\\\\\nP(B|A) &= \\frac{P(A\\cap B)}{P(A)}\n\\end{align*}\n$$ \ngilt \n$$\nP(A|B)P(B)=P(B|A)P(A).\n$$ \nDaraus folgt durch Umstellen unmittelbar der Satz von Bayes.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Medizinischer Schnelltest\nWir betrachten wieder den medizinischen Schnelltest und die beiden\nEreignisse $A$: \"Person ist infiziert\", und $B$: \"Test positiv\". Neben\nden beiden bedingten Wahrscheinlichkeiten $P(B|A)=0.99$ und \n$P(\\bar B|\\bar A)=0.99$ sei bekannt, dass $P(A)=0.001$, d.h. nur mit\neiner Wahrscheinlichkeit von 0.1 Prozent ist eine zufällig aus der\nPopulation ausgewählte Person infiziert. \n\nWie groß ist Wahrscheinlichkeit, dass eine Person infiziert ist, bei\nder der Schnelltest ein positives Ergebnis zeigt? Gesucht ist also\n$P(A|B)$. Nach dem Satz von Bayes gilt\n$$\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\n$$\nNach dem Satz der totalen Wahrscheinlichkeit berechnet man\n$$\n\\begin{align*}\nP(B) &=P(B|A)P(A)+P(B|\\bar A)P(\\bar A)\\\\\n&= P(B|A)P(A)+[1-P(\\bar B|\\bar A)][1-P(A)]\\\\\n&= 0.99\\cdot 0.001+[1-0.99]\\cdot [1-0.001]\\\\\n&= 0.01098.\n\\end{align*}\n$$\nDie Wahrscheinlichkeit, dass eine Person mit einem positiven Testergebnis\ntatsächlich infiziert ist, ist folglich immer noch sehr klein, obwohl der\nTest scheinbar sehr genau arbeitet. \n:::\n\n## Unabhängigkeit {#sec-unabhaengig1}\n\n::: callout-note\n## Definition: Unabhängigkeit\n\nZwei Ereignisse $A$ und $B$ heißen **unabhängig** \n(oder auch **stochastisch unabhängig**, \nengl. independent bzw. stochastically independent), wenn \n$$\nP(A\\cap B)=P(A)\\cdot P(B).\n$$ \n:::\n\nEine alternative (fast äquivalente) Definition der Unabhängigkeit\nlautet so: Die Ereignisse $A$ und $B$ heißen unabhängig, wenn gilt \n$$\nP(A|B)=P(A)\n$$ \nbzw. wenn \n$$\nP(B|A)=P(B).\n$$ \nDie Definition über die bedingten Wahrscheinlichkeiten ist etwas\nintuitiver. Wenn das Wissen darüber, ob $A$ eingetreten ist oder nicht,\nfür die Wahrscheinlichkeit von $B$ keine Rolle spielt (und umgekehrt),\ndann sind $A$ und $B$ unabhängig.\n\nAchten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln.\nZur Erinnerung: Die Ereignisse $A$ und $B$ sind disjunkt, wenn\n$A\\cap B=\\emptyset$, so dass $P(A\\cap B)=0$. Disjunkte Ereignisse können\nalso nicht beide zusammen eintreten. Wenn $A$ eintritt, kann man sicher\nsein, dass $B$ nicht eintritt. Und wenn $B$ eintritt, kann man sicher\nsein, dass $A$ nicht eintritt. Unabhängige Ereignisse können hingegen\nsehr wohl zusammen eintreten.\n\n::: {.callout-tip collapse=\"true\"}\n## Beispiel: Medizinischer Schnelltest\nWir betrachten wieder den medizinischen Schnelltest und die beiden\nEreignisse $A$: \"Person ist infiziert\", und $B$: \"Test positiv\".\nZur Erinnerung: Gegeben sind die beiden bedingten Wahrscheinlichkeiten\n$P(B|A)=0.99$ und $P(\\bar B|\\bar A)=0.99$ sowie die\nunbedingte Wahrscheinlichkeit $P(A)=0.001$. Außerdem haben wir\nim letzten Abschnitt $P(B)=0.01098$ berechnet. \n\nSind die Ereignisse $A$ und $B$ unabhängig? Die Antwort lautet \"nein\",\ndenn es gilt\n$$\n0.99=P(B|A)\\neq P(B)=0.01098.\n$$\nUnabhängigkeit ist in diesem Fall natürlich auch nicht zu erwarten.\nEin Schnelltest, dessen Ergebnis keinen Einfluss auf die \nEinschätzung der Wahrscheinlichkeit einer Infektion hätte, wäre\nvollkommen nutzlos.\n:::\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"output-file":"02-wahrscheinlichkeit.html"},"language":{"toc-title-document":"Inhaltsverzeichnis","toc-title-website":"Auf dieser Seite","related-formats-title":"Andere Formate","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Quelle","section-title-abstract":"Zusammenfassung","section-title-appendices":"Anhang","section-title-footnotes":"Fußnoten","section-title-references":"Literatur","section-title-reuse":"Wiederverwendung","section-title-copyright":"Urheberrechte","section-title-citation":"Zitat","appendix-attribution-cite-as":"Bitte zitieren Sie diese Arbeit als:","appendix-attribution-bibtex":"Mit BibTeX zitieren:","title-block-author-single":"Autor","title-block-author-plural":"Authors","title-block-affiliation-single":"Zugehörigkeit","title-block-affiliation-plural":"Zugehörigkeiten","title-block-published":"Veröffentlicht am","title-block-modified":"Geändert","callout-tip-title":"Beispiel","callout-note-title":"Definition","callout-warning-title":"Warnung","callout-important-title":"Wichtig","callout-caution-title":"Vorsicht","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Gesamten Code zeigen","code-tools-hide-all-code":"Gesamten Code verbergen","code-tools-view-source":"Quellcode anzeigen","code-tools-source-code":"Quellcode","code-line":"Zeile","code-lines":"Zeilen","copy-button-tooltip":"In die Zwischenablage kopieren","copy-button-tooltip-success":"Kopiert","repo-action-links-edit":"Seite editieren","repo-action-links-source":"Quellcode anzeigen","repo-action-links-issue":"Problem melden","back-to-top":"Zurück nach oben","search-no-results-text":"Keine Treffer","search-matching-documents-text":"Treffer","search-copy-link-title":"Link in die Suche kopieren","search-hide-matches-text":"Zusätzliche Treffer verbergen","search-more-match-text":"weitere Treffer in diesem Dokument","search-more-matches-text":"weitere Treffer in diesem Dokument","search-clear-button-title":"Zurücksetzen","search-detached-cancel-button-title":"Abbrechen","search-submit-button-title":"Abschicken","search":"Search","toggle-section":"Abschnitt umschalten","toggle-sidebar":"Seitenleiste umschalten","toggle-dark-mode":"Dunkelmodus umschalten","toggle-reader-mode":"Lesemodus umschalten","toggle-navigation":"Navigation umschalten","crossref-fig-title":"Abbildung","crossref-tbl-title":"Tabelle","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Korollar","crossref-prp-title":"Aussage","crossref-cnj-title":"Annahme","crossref-def-title":"Definition","crossref-exm-title":"Beispiel","crossref-exr-title":"Übungsaufgabe","crossref-ch-prefix":"Kapitel","crossref-apx-prefix":"Anhang","crossref-sec-prefix":"Kapitel","crossref-eq-prefix":"Gleichung","crossref-lof-title":"Abbildungsverzeichnis","crossref-lot-title":"Tabellenverzeichnis","crossref-lol-title":"Listingverzeichnis","environment-proof-title":"Beweis","environment-remark-title":"Anmerkung","environment-solution-title":"Lösung","listing-page-order-by":"Sortieren nach","listing-page-order-by-default":"Voreinstellung","listing-page-order-by-date-asc":"Datum (aufsteigend)","listing-page-order-by-date-desc":"Neueste","listing-page-order-by-number-desc":"Absteigend","listing-page-order-by-number-asc":"Aufsteigend","listing-page-field-date":"Datum","listing-page-field-title":"Titel","listing-page-field-description":"Beschreibung","listing-page-field-author":"Autor:in","listing-page-field-filename":"Dateiname","listing-page-field-filemodified":"Geändert","listing-page-field-subtitle":"Untertitel","listing-page-field-readingtime":"Lesezeit","listing-page-field-categories":"Kategorien","listing-page-minutes-compact":"{0} min","listing-page-category-all":"alle","listing-page-no-matches":"Keine Treffer"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"source","callout-icon":false,"other-links-title":"Weitere Links","code-links-title":"Code-Links","launch-dev-container-title":"Dev Container starten","launch-binder-title":"Binder starten","title-block-keywords":"Schlüsselwörter","search-label":"Suchen","listing-page-field-wordcount":"Wortanzahl","listing-page-words":"{0} Wörter","theme":"sandstone"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
=======
{"title":"Wahrscheinlichkeit","markdown":{"headingText":"Wahrscheinlichkeit","headingAttr":{"id":"wahrscheinlichkeit","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Definition {#def:wahrscheinlichkeit}\n\nManche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die\nWahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung\n(Funktion) angegeben.\n\n::: callout-note\nEine Abbildung $P: A \\mapsto P(A)$ heißt **Wahrscheinlichkeitsmaß**\n(engl. probability mass) oder kurz Wahrscheinlichkeit, wenn gilt\n\n-   Nichtnegativität: $P(A)\\ge 0$ für alle Ereignisse $A$\n\n-   Normierung: $P(\\Omega)=1$\n\n-   Additivität: Für disjunkte Ereignisse $A$ und $B$ (d.h.\n    $A\\cap B=\\emptyset$) ist \n    $$\n    P(A\\cup B)=P(A)+P(B).\n    $$\n:::\n\nDiese Eigenschaften, die eine Wahrscheinlichkeitsabbildung erfüllen\nmuss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten\nhergeleitet werden, daher spricht man auch von **Axiomen**. Etwas\nallgemeiner und präziser wurden diese Axiome 1933 von Andrey Kolmogorov (1903-1987)\nfür eine saubere mathematische Fundierung der Wahrscheinlichkeitstheorie\neingeführt.\n\nAus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum\nBeispiel\n\n-   Monotonität: Wenn $A\\subseteq B$, dann ist $P(A)\\le P(B)$\n\n-   Komplementärereignis: $P(\\bar A)=1-P(A)$\n\n-   Additionssatz: $P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$.\n\n## Venn-Diagramme {#venn}\n\nEreignisse lassen sich - wie andere Mengen in der Mengenlehre auch - in\nForm von Venn-Diagrammen darstellen. Wenn die Ergebnismenge $\\Omega$\ndurch ein Rechteck repräsentiert wird, dann kann man Ereignisse als\nTeilmengen des Rechtecks darstellen. In dem folgenden Bild ist zum\nBeispiel das Ereignis $A$ der kleine Kreis in der Mitte.\n\n```{r}\n#| out.width: \"95%\"\n#| echo: FALSE\n  \nlibrary(plotrix)\npar(mar=c(0.1,0.1,0.1,0.1))\nplot(c(0,1),c(0,1),t=\"n\", axes=F, xlab=\"\",ylab=\"\")\ndraw.circle(0.5,0.5,radius=0.1,col=NA)\ndraw.circle(0.65,0.65,radius=0.2,col=NA)\ndraw.circle(0.2,0.3,radius=0.2,col=NA)\nbox()\ntext(0.1,0.8,expression(Omega),cex=1.2)\ntext(0.5,0.5,\"A\",cex=1.2)\ntext(0.65,0.65,\"B\",cex=1.2)\ntext(0.2,0.3,\"C\",cex=1.2)\n```\n\nIn diesem Venn-Diagramm wird deutlich, dass die Ereignisse $A$ und $C$\nnicht gleichzeitig eintreten können, $A$ und $B$ hingegen schon, denn\nihre Schnittmenge ist nicht leer. Außerdem erkennt man, dass $A$ eine\nkleinere Wahrscheinlichkeit hat als $B$ und $C$. Der Additionssatz\n$P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$ wird in dieser grafischen Sichtweise\nsehr einfach klar: Die Wahrscheinlichkeit von $A\\cup B$ (also $A$ oder\n$B$) ergibt sich als die Summe aus den beiden Kreisflächen $A$ und $B$\nabzüglich der Fläche der Schnittmenge, die sonst doppelt gezählt werden\nwürde.\n\n## Laplace-Experimente {#laplace}\n\nEine besonders einfache Art von Zufallsvorgängen sind die sogenannten\nLaplace-Experimente.\n\n::: callout-note\nEin Zufallsvorgang heißt **Laplace-Experiment**, wenn es nur endlich\nviele Ergebnisse gibt (d.h. wenn $|\\Omega|=n$) und wenn alle\nElementarereignisse als gleich wahrscheinlich angenommen werden können.\n:::\n\n<img src=\"images/AdobeStock_332174058b.jpeg\" align=\"right\" width=\"35%\"/>\nBeachten Sie, dass es sich dabei um eine Aussage handelt, die aus\nunserem Alltagswissen herrührt, nicht aus mathematischen Überlegungen\noder Herleitungen. Ein\ntypisches, einfaches Beispiel für ein Laplace-Experiment sind\nWürfelwürfe. Es ist aus unserem Alltagswissen heraus plausibel, davon\nauszugehen, dass alle Augenzahlen eines Würfels gleich wahrscheinlich\nsind. Auch bei einer Münze ist es naheliegend, dass die\nWahrscheinlichkeit für Kopf und die Wahrscheinlichkeit für Zahl gleich\nsind. Hingegen würde man beim Werfen eines Kronkorkens nicht unbedingt\nvermuten, dass beide Seiten mit der gleichen Wahrscheinlichkeit oben\nliegen. Hier liegt also kein Laplace-Experiment vor.\n\nBei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein\nErgebnis $A$ eintritt leicht zu ermitteln. Sie beträgt \n$$\nP(A)=\\frac{|A|}{|\\Omega|},\n$$ \nalso die Anzahl der Ergebnisse in $A$ dividiert durch die Anzahl\naller Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss\nman also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist\nmanchmal sehr einfach, kann aber bei großen Mengen kompliziert sein. In\nsolchen Fällen hilft der Teilbereich der Mathematik weiter, den man\n\"Kombinatorik\" nennt. Wir gehen in diesem Kurs jedoch nicht näher auf\nkombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei\ndenen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.\n\n::: callout-tip\n-   Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.\n    Die Anzahl der Ergebnisse in $\\Omega$ beträgt 6. Sei $A$ das\n    Ereignis \"Eine gerade Zahl wird geworfen\", also $A=\\{2,4,6\\}$. Dann\n    ist \n    $$\n    \\begin{align*}\n    P(A)&=\\frac{|A|}{|\\Omega|}\\\\\n    &=\\frac{3}{6}\\\\\n    &=0.5.\n    \\end{align*}\n    $$\n\n-   Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine\n    Rolle, wie man die Ergebnismenge festlegt. Wir wählen \n    $$\n    \\begin{align*}\n    \\Omega=\\{&11,12,13,14,15,16,\\\\\n    &21,22,23,24,25,26,\\\\\n    &31,32,33,34,35,36,\\\\\n    &41,42,43,44,45,46,\\\\\n    &51,52,53,54,55,56,\\\\\n    &61,62,63,64,65,66\\}\n    \\end{align*}\n    $$ \n    weil es sich dann um ein Laplace-Experiment handelt. Die\n    Ergebnisse \"24\" und \"42\" sind zwar nicht unterscheidbar, wenn die\n    Würfel gleich aussehen, aber wir können den zuerst geworfenen Würfel\n    (oder z.B. den weiter links liegenden) als ersten Würfel bezeichnen. Dass\n    alle 36 Ergebnisse gleich wahrscheinlich sind, können wir (nur) mit\n    unserem Alltagswissen begründen, nicht aus der Mathematik heraus.\n    Sei $A$ das Ereignis \"Die Augenzahlen der beiden Würfel\n    unterscheiden sich um 2\", d.h. \n    $$\n    A=\\{13,24,31,35,42,46,53,64\\}.\n    $$ \n    Dann gilt \n    $$\n    \\begin{align*}\n    P(A)&=\\frac{|A|}{|\\Omega|}\\\\\n    &=\\frac{8}{36}\\\\\n    &=\\frac{2}{9}\\\\\n    &\\approx 0.2222.\n    \\end{align*}\n    $$ \n    Da die beiden Würfel nicht unterscheidbar sind, wäre als Ergebnismenge auch \n    $$\n    \\begin{align*}\n    \\Omega=\\{\n    &11,12,13,14,15,16,\\\\\n    &22,23,24,25,26,\\\\\n    &33,34,35,36,\\\\\n    &44,45,46,\\\\\n    &55,56,\\\\\n    &66\\}\n    \\end{align*}\n    $$ \n    möglich gewesen. Dann wäre aber die Annahme eines Laplace-Experiments nicht\n    mehr korrekt, denn die Elementarereignisse \"11\" und \"12\" sind\n    beispielsweise nicht gleich wahrscheinlich (wir wissen aus unserer\n    Erfahrung, dass ein Pasch seltener auftritt).\n:::\n\nDie Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere\nVorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge so\ngewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte man\ndas tun.\n\n## Bedingte Wahrscheinlichkeit {#bedingt}\n\nManchmal gibt es begrenzte Informationen über einen Zufallsvorgang. Dann\nkennt man zwar nicht das realisierte Ergebnis, kann aber die Menge der\nmöglichen Ergebnisse eingrenzen. Dadurch ändern sich die\nWahrscheinlichkeiten für Ereignisse.\n\n::: callout-note\nWir betrachten zwei Ereignsse $A$ und $B$ mit $P(B)>0$. Dann heißt \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \ndie **bedingte Wahrscheinlichkeit** (engl. conditional probability) von $A$ gegeben $B$.\n:::\n\nDie Notation $A|B$ steht nicht für ein bestimmtes Ereignis, sondern\nzeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten,\nnämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über\nWahrscheinlichkeiten ist es nicht immer einfach (aber sehr wichtig!),\nzwischen der Wahrscheinlichkeit $P(A\\cap B)$ und der bedingten\nWahrscheinlichkeit $P(A|B)$ zu unterscheiden. Wenn man $P(A\\cap B)$\nmeint, spricht man von der Wahrscheinlichkeit, dass $A$ und $B$\neintreten. Wenn man $P(A|B)$ meint, sagt man $A$ gegeben $B$, oder: $A$\nwenn $B$, oder: $A$ unter der Bedingung $B$. Wenn man ausdrücklich\nangeben möchte, dass eine Wahrscheinlichkeit *keine* bedingte\nWahrscheinlichkeit ist, nennt man sie auch eine unbedingte\nWahrscheinlichkeit (engl. unconditional probability).\n\n::: callout-tip\n-   Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse\n    $A=\\{2,4,6\\}$ (\"gerade Zahl\"), und $B=\\{4,5,6\\}$ (\"eine Zahl größer\n    als 3\"). Die bedingte Wahrscheinlichkeit von $A$ gegeben $B$ beträgt\n    $$\n    \\begin{align*}\n    P(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n    &=\\frac{P(\\{4,6\\})}{P(\\{4,5,6\\})}\\\\\n    &=\\frac{2}{3}\n    \\end{align*}\n    $$ \n    und die bedingte Wahrscheinlichkeit von $B$ gegeben $A$ lautet \n    $$\n    \\begin{align*}\n    P(B|A)&=\\frac{P(B\\cap A)}{P(A)}\\\\\n    &=\\frac{P(\\{4,6\\})}{P(\\{2,4,6\\})}\\\\\n    &=\\frac{2}{3}.\n    \\end{align*}\n    $$\n\n-   Zwei Würfel werden geworfen. Sei $A$ das Ereignis \"die Augensumme\n    ist 10\". Sei $B$ das Ereignis \"einer der beiden Würfel (oder beide)\n    zeigt eine 2\". Dann ist \n    $$\n    \\begin{align*}\n    P(A|B)&=\\frac{P(A\\cap B)}{P(B)}\\\\\n    &=\\frac{P(\\{\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n    &=0.\n    \\end{align*}\n    $$ \n    Die bedingte Wahrscheinlichkeit für das Ereignis $C$: \"Augensumme\n    ist 5\" unter der Bedingung $B$ ist \n    $$\n    \\begin{align*}\n    P(C|B)&=\\frac{P(C\\cap B)}{P(B)}\\\\\n    &=\\frac{P(\\{23,32\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n    &=\\frac{2}{11}.\n    \\end{align*}\n    $$\n\n-   Bei einer Umfrage werden Personen zufällig ausgewählt und befragt.\n    Sei $A$ das Ereignis \"die befragte Person ist weiblich\". Sei $B$ das\n    Ereignis \"die befragte Person arbeitet in Teilzeit\". Zwischen der\n    (unbedingten) Wahrscheinlichkeit $P(B)$ und der bedingten\n    Wahrscheinlichkeit $P(B|A)$ besteht ein Unterschied. Die\n    Wahrscheinlichkeit $P(B)$ steht dafür, dass eine zufällig\n    ausgewählte Person in Teilzeit arbeitet, und diese Person kann\n    männlich oder weiblich sein. Dagegen ist $P(B|A)$ die\n    Wahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit\n    arbeitet.\n:::\n\nIn den Medien werden häufig bedingte Wahrscheinlichkeiten berichtet,\nohne dass das explizit erwähnt wird. Oftmals sind die berichteten\nbedingten Wahrscheinlichkeiten gar nicht die, für die man sich\neigentlich interessiert, weil die Bedingung und das Bedingte quasi\nfalsch herum angeordnet sind. Man interessiert sich für $P(A|B)$, \nberichtet wird aber $P(B|A)$.\n\n::: callout-tip\n-   Es wird berichtet, dass ein medizinischer Schnelltest mit einer\n    Wahrscheinlichkeit von 99 Prozent eine infizierte Person korrekt als\n    infiziert (positiv) erkennt und ebenfalls mit einer Wahrscheinlichkeit von 99\n    Prozent eine nicht infizierte Person als nicht infiziert erkennt\n    (negativ). Sei $A$ das Ereignis \"infiziert\" und $B$ das Ereignis \"Test positiv\". \n    Die angegebenen Wahrscheinlichkeiten sind bedingte Wahrscheinlichkeiten,\n    und zwar $P(B|A)=0.99$ und $P(\\bar B|\\bar A)=0.99$. Als Testanwender \n    interessiert man sich jedoch eher für die\n    bedingte Wahrscheinlichkeit, tatsächlich infiziert zu sein, \n    wenn der Test positiv ausfällt, also für $P(A|B)$. Oder auch für die \n    bedingte Wahrscheinlichkeit infiziert zu sein, obwohl der Test\n    negativ ausfällt, also $P(A|\\bar B)$.\n\n-   Es wird berichtet, dass 75 Prozent der Studierenden ein\n    \"akademisches Elternhaus\" haben. Es handelt sich um die bedingte\n    Wahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person\n    studiert. Hier sind die Ereignisse $A$: \"Kind studiert\", und\n    $B$: \"akademisches Elternhaus\". Gegeben ist $P(B|A)=0.75$.\n    Für die Bewertung der Chancengleichheit ist jedoch die\n    bedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert,\n    wenn es aus einem akademischen Elternhaus stammt, also\n    $P(A|B)$ - und zwar im Vergleich zu der bedingten Wahrscheinlichkeit, \n    dass ein Kind studiert, das nicht aus einem akademischen Elternhaus stammt,\n    also $P(A|\\bar B)$.\n:::\n\nWie kann man eine bedingte Wahrscheinlichkeit \"umdrehen\"? Wie verhält\nsich $P(B|A)$ zu $P(A|B)$? Das sehen wir in Kürze im Abschnitt \\@ref(bayes).\n\nBedingte Wahrscheinlichkeiten sind auch nützlich, um die\nWahrscheinlichkeit zu berechnen, dass mehrere Ereignisse gemeinsam\npassieren. Dazu formen wir die Definition der bedingten\nWahrscheinlichkeit einfach um. Aus \n$$\nP(A|B)=\\frac{P(A\\cap B)}{P(B)}\n$$ \nwird \n$$\nP(A\\cap B)={P(B)P(A|B)}.\n$$\n\n::: callout-tip\n-   Eins\n-   Zwei\n:::\n\nDas lässt sich auf mehr als zwei Ereignisse erweitern: \n$$\n\\begin{align*}\nP(A\\cap B \\cap C)&=P(C)P(B|C)P(A|B\\cap C)\\\\\nP(A\\cap B\\cap C\\cap D)&=P(D)P(C|D)P(B|C\\cap D)P(A|B\\cap C\\cap D)\n\\end{align*}\n$$ \nDiese Formeln sind bei näherer Betrachtung intuitiv: Die\nWahrscheinlichkeit, dass $A$, $B$ und $C$ gemeinsam eintreten, ergibt\nsich, indem man zuerst eine der unbedingten Wahrscheinlichkeiten nimmt\n(z.B. $P(C)$). Nun ist $C$ quasi eingetreten und wir arbeiten unter der\nBedingung $C$ weiter. Die Wahrscheinlichkeit $P(C)$ wird jetzt mit der\nbedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,\nalso $P(B|C)$. Nun sind $B$ und $C$ eingetreten, und auf der nächsten\nStufe multiplizieren wir deshalb mit $P(A|B\\cap C)$. Eine andere\nReihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B. \n$$\nP(A\\cap B\\cap C)=P(A)P(B|A)P(C|A\\cap B).\n$$\nFür jede Reihenfolge gelangt man zum gleichen Ergebnis.\n\n## Totale Wahrscheinlichkeit {#total}\n\nAus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte\nWahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum\n$\\Omega$ in eine Partition. Unter einer **Partition** versteht man eine\nZerlegung in disjunkte Mengen $A_1, A_2, \\ldots, A_n$, so dass die\nVereinigungsmenge der $A_1,\\ldots, A_n$ wieder $\\Omega$ ergibt. Die\n\"Partitionierung eines Rinds\" könnte etwa so aussehen:\n\n```{r echo=FALSE,out.width=\"100%\"}\nknitr::include_graphics(\"images/AdobeStock_101998556b.jpeg\")\n```\n\nDas Rind wird also vollständig Teilmengen zerlegt, und alle Teilmengen\nzusammen ergeben wieder das gesamte Rind.\n\nNun nehmen wir an, dass für jedes Ereignis $A_i$, $i=1,\\ldots,n$, die\nbedingte Wahrscheinlichkeit $P(B|A_i$ gegeben ist, wobei $B$ irgendein\nEreignis ist. Dann gilt für die unbedingte Wahrscheinlichkeit von $B$:\n$$\nP(B)=\\sum_{i=1}^n P(B|A_i)P(A_i).\n$$ \nDie unbedingte Wahrscheinlichkeit ergibt sich also als gewichtete\nSumme aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch\ndie Wahrscheinlichkeiten der bedingenden Ereignisse.\n\n::: callout-tip\n-   Eins\n-   Zwei\n:::\n\n## Satz von Bayes {#bayes}\n\nDer Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten\n$P(A|B)$ und $P(B|A)$ zueinander in Beziehung. Es gilt \n$$\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\n$$ \nUm die Bedingung und das Bedingte zu vertauschen, braucht man also\nbeide unbedingte Wahrscheinlichkeiten. Die Herleitung des Satzes von\nBayes ergibt sich aus der Definition der bedingten Wahrscheinlichkeit.\nWegen \n$$\n\\begin{align*}\nP(A|B) &= \\frac{P(A\\cap B)}{P(B)}\\\\\nP(B|A) &= \\frac{P(A\\cap B)}{P(A)}\n\\end{align*}\n$$ \ngilt \n$$\nP(A|B)P(B)=P(B|A)P(A).\n$$ \nDaraus folgt durch Umstellen unmittelbar der Satz von Bayes.\n\n::: callout-tip\n-   Eins\n-   Zwei\n:::\n\n## Unabhängigkeit\n\nZwei Ereignisse $A$ und $B$ heißen unabhängig (oder auch stochastisch\nunabhängig, engl. independent bzw. stochastically independent), wenn \n$$\nP(A\\cap B)=P(A)\\cdot P(B).\n$$ \nEine alternative (fast äquivalente) Definition der Unabhängigkeit\nlautet so: Die Ereignisse $A$ und $B$ heißen unabhängig, wenn gilt \n$$\nP(A|B)=P(A)\n$$ \nbzw. wenn \n$$\nP(B|A)=P(B).\n$$ \nDie Definition über die bedingten Wahrscheinlichkeiten ist etwas\nintuitiver. Wenn das Wissen darüber, ob $A$ eingetreten ist oder nicht,\nfür die Wahrscheinlichkeit von $B$ keine Rolle spielt (und umgekehrt),\ndann sind $A$ und $B$ unabhängig.\n\nAchten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln.\nZur Erinnerung: Die Ereignisse $A$ und $B$ sind disjunkt, wenn\n$A\\cap B=\\emptyset$, so dass $P(A\\cap B)=0$. Disjunkte Ereignisse können\nalso nicht beide zusammen eintreten. Wenn $A$ eintritt, kann man sicher\nsein, dass $B$ nicht eintritt. Und wenn $B$ eintritt, kann man sicher\nsein, dass $A$ nicht eintritt. Unabhängige Ereignisse können hingegen\nsehr wohl zusammen eintreten.\n\nWie lässt sich die Definition von Unabhängigkeit auf mehr als zwei\nEreignisse verallgemeinern? Das ist etwas komplizierter, als man es\nvielleicht zunächst vermutet. Intuitiv gesprochen liegt bei nur zwei\nEreignissen ($A$ und $B$) Unabhängigkeit dann vor, wenn Informationen\nüber die Ausprägung eines Ereignisses die Wahrscheinlichkeit für das\nandere Ereignis nicht verändern. Bei drei Ereignissen ($A$, $B$, $C$)\nkann es passieren, dass die Information über $A$ weder die\nWahrscheinlichkeit von $B$ noch die von $C$ verändert, dass aber die\nInformation über $A$ *und* $B$ dennoch die Wahrscheinlichkeit von $C$\nverändert. Sehen wir uns dazu ein einfaches Beispiel an:\n\n::: callout-tip\nZwei faire Würfel werden geworfen (mit der üblichen Ergebnismenge). Wir\ndefinieren die folgenden drei Ereignisse:\n\n$A$: Die Augenzahl des ersten Würfels ist ungerade\n\n$B$: Die Augenzahl des zweiten Würfels ist ungerade\n\n$C$: Die Summe der Augenzahlen ist ungerade\n\nWenn man zwei beliebige dieser Ereignisse auswählt, dann sind sie\nunabhängig voneinander. Das dritte Ereignis ist aber von den beiden\nanderen abhängig, weil die Summe von zwei ungeraden Zahlen niemals\nungerade ist.\n:::\n\nWeil die Situation bei mehr als zwei Ereignissen also komplexer ist,\nwird Unabhängigkeit für mehrere Ereignisse so definiert:\n\n::: callout-note\nSeien $A_1,\\ldots,A_n$ Ereignisse. Sie heißen *global unabhängig*, wenn\nfür jedes $m\\in \\{2,\\ldots,n\\}$ und alle möglichen Indizes\n$1\\le i_1<\\ldots<i_m\\le n$ gilt \n$$\nP(A_{i_1}\\cap\\ldots\\cap A_{i_m})=P(A_{i_1})\\cdot\\ldots\\cdot P(A_{i_m}).\n$$\n:::\n\nDie formale Definition ist nicht ganz leicht zu durchschauen. Letztlich\nsagt sie aber einfach aus: Bei globaler Unabhängigkeit ist die\nWahrscheinlichkeit, dass beliebig viele beliebig ausgewählte Ereignisse\ngemeinsam eintreten, gerade dem Produkt der einzelnen\nWahrscheinlichkeiten entspricht.\n\n## Vier-Felder-Tafeln\n\nDie Vier-Felder-Tafel ist ein besonders einfaches, aber nützliches\nWerkzeug für Fragen zu Wahrscheinlichkeiten, wenn zwei genau Ereignisse\n(und ihre Gegenereignisse) eine Rolle spielen, z.B. $A$ und $B$ (und\n$\\bar A$ und $\\bar B$). Jedes der beiden Ereignisse kann eintrefen oder\nnicht, so dass es insgesamt vier Kombinationsmöglichkeiten gibt. Die\nWahrscheinlichkeiten dieser vier Möglichkeiten werden wie folgt\nangeordnet: \n$$\n\\begin{array}{c|cc|} \n& A & \\bar A& \\\\ \\hline\nB & P(A\\cap B) & P(\\bar A\\cap B)\\\\\n\\bar B & P(A\\cap \\bar B) & P(\\bar A\\cap \\bar B)\\\\ \\hline\n\\end{array}\n$$ \nDa $P((A\\cap B)\\cup (A\\cap \\bar B))=P(A)$ ist (und entsprechend für\ndie anderen Zeilen und Spalten), kann man die Ränder ergänzen: \n$$\n\\begin{array}{c|cc|c} \n& A & \\bar A& \\\\ \\hline\nB & P(A\\cap B) & P(\\bar A\\cap B) & P(B)\\\\\n\\bar B & P(A\\cap \\bar B) & P(\\bar A\\cap \\bar B) & P(\\bar B)\\\\ \\hline\n& P(A) & P(\\bar A) & 1\n\\end{array}\n$$ \nWeil die Zeilen und Spalten sich immer zu den Randwerten addieren\nmüssen, kann man Felder leicht ergänzen, wenn in einer Zeile oder Spalte\nein Wert fehlt. Eine häufiger Fehler bei der Konstruktion von\nVier-Felder-Tafeln besteht darin, dass man bedingte Wahrscheinlichkeiten\nin die Felder einträgt, also beispielsweise $P(A|B)$ anstelle von\n$P(A\\cap B)$. Wenn die bedingte Wahrscheinlichkeit $P(A|B)$ bekannt ist,\nhilft das bei der Erstellung der Vier-Felder-Tafel aber durchaus weiter,\nweil $P(A\\cap B)=P(A|B)P(B)$ ist.\n\nWill man eine Vier-Felder-Tafel ausfüllen, dann ist es wichtig darauf zu\nachten, ob es sich bei den vorgegebenen Wahrscheinlichkeiten um\ngemeinsame Wahrscheinlichkeiten (z.B. $A$ und $B$ bzw. $P(A\\cap B)$)\nhandelt, oder um bedingte Wahrscheinlichkeiten (also z.B. $P(A|B)$).\nLetztere können leicht umgeformt werden, wenn man den Zusammenhang \\[\nP(A\\cap B)=P(A\\|B)P(B) \\] beachtet (oder die analogen Zusammenhänge für\nandere bedingte Wahrscheinlichkeiten).\n\n::: callout-tip\nXXX\n:::\n\n## Wahrscheinlichkeitsbäume\n\nVier-Felder-Tafeln sind ein nützliches Werkzeug, um einfaches\nWahrscheinlichkeitsprobleme zu lösen, in denen nur zwei Ereignisse und\nihre Gegenwahrscheinlichkeiten vorkommen. Ein Werkzeug, mit dem man auch\nallgemeine und etwas komplexere Probleme übersichtlich bearbeiten kann,\nsind Wahrscheinlichkeitsbäume. Die Ereignisse - es können hier durch aus\nmehr als zwei sein - bilden die Äste des Baums.\n\n::: callout-tip\nDefinition der Ereignisse: \n$$\n\\begin{eqnarray*}\nA &\\text{:}&\\text{ Die Firma hat Erfolg} \\\\\nB &\\text{:}&\\text{ Der Manager strengt sich an}\n\\end{eqnarray*}\n$$ \nGesucht sei die bedingte Wahrscheinlichkeit $P(B|A)=P(A\\cap B)/P(A)$.\nFür die Auszahlung von Boni an einen Manager ist es wichtig zu wissen,\nob er eine gute Leistung erbracht hat. Wir wollen bestimmen, wie hoch\ndie Wahrscheinlichkeit ist, dass der Manager sich tatsächlich für den\nErfolg angestrengt hat, wenn wir beobachten, dass die Firma erfolgreich\nist. Oder anders gesagt, ist der Erfolg der Firma wirklich ein Indikator\nfür eine gute Leistung?\n\nWir nehmen an (auch wenn es in der Praxis unrealistisch ist, diese Werte\nzu kennen): \n$$\n\\begin{eqnarray*}\nP\\left( A|B\\right) &=&0.8 \\\\\nP(A|\\bar{B}) &=&0.5 \\\\\nP\\left( B\\right) &=&0.6\n\\end{eqnarray*}\n$$ \nHier handelt es sich um eine Fragestellung, die auch mit einer\nVier-Felder-Tafel beantwortet werden könnte. Trotzdem nutzen wir nun\neinen Wahrscheinlichkeitsbaum. Auf der ersten Ebene verzweigt der Baum\nin die beiden Ereignisse $B$ und $\\bar B$, denn wir kennen bedingte\nWahrscheinlichkeiten, die diese beiden Ereignisse als Bedingung haben.\nAn die beiden Zweige schreiben wir die Ereignisse und ihre\nWahrscheinlichkeiten. Die Wahrscheinlichkeit von 0.4 für das Ereignis\n$\\bar B$ ergibt sich aus der Gegenwahrscheinlichkeit von $B$.\n\n```{r echo=FALSE}\nplot(c(0,1),c(0,1),t=\"n\",xlab=\"\",ylab=\"\",axes=F)\npoints(c(0,0.4,0.4),c(0.5,0.2,0.8))\nlines(c(0.4,0,0.4),c(0.2,0.5,0.8))\ntext(0.2,0.7,expression(B))\ntext(0.35,0.83,0.6)\ntext(0.35,0.17,0.4)\ntext(0.2,0.29,expression(bar(B)))\n```\n\nAuf der zweiten Stufe werden nun die bedingten Ereignisse und\nWahrscheinlichkeiten ergänzt, einmal unter der Bedingung, dass $B$\neingetreten ist, einmal unter der Bedingung, dass $\\bar B$ eingetreten\nist. An die neuen Zweige werden die zugehörigen bedingten\nWahrscheinlichkeiten geschrieben. Dabei nutzen wir wiederum die\nGegenwahrscheinlichkeiten der bedingten Ereignisse.\n\n```{r echo=FALSE}\nplot(c(0,1),c(0,1),t=\"n\",xlab=\"\",ylab=\"\",axes=F)\npoints(c(0,0.4,0.4,0.8,0.8,0.8,0.8),c(0.5,0.2,0.8,0,0.4,0.6,1))\nlines(c(0.4,0,0.4),c(0.2,0.5,0.8))\nlines(c(0.8,0.4,0.8),c(0,0.2,0.4))\nlines(c(0.8,0.4,0.8),c(0.6,0.8,1))\ntext(0.2,0.7,expression(B))\ntext(0.2,0.29,expression(bar(B)))\ntext(0.6,0.04,expression(bar(A)))\ntext(0.6,0.35,expression(A))\ntext(0.6,0.65,expression(bar(A)))\ntext(0.6,0.95,expression(A))\ntext(c(0.35,0.35,0.75,0.75,0.75,0.75),\n     c(0.3,0.7,0.93,0.67,0.33,0.07),\n     c(0.4,0.6,0.8,0.2,0.5,0.5))\n```\n\nDie Wahrscheinlichkeiten für $A\\cap B$, $A\\cap \\bar B$, $\\bar A\\cap B$\nund $\\bar A\\cap\\bar b$ ergeben sich nun, indem man entlang der Äste die\nWahrscheinlichkeiten multipliziert. Man erhält \n$$\n\\begin{align*}\nP(A\\cap B)&=0.6\\cdot 0.8 = 0.48\\\\\nP(\\bar A\\cap B)&=0.6\\cdot 0.2 = 0.12\\\\\nP(A\\cap \\bar B)&=0.4\\cdot 0.5 = 0.2\\\\\nP(\\bar A\\cap \\bar B)&=0.4\\cdot 0.5 = 0.2\n\\end{align*}\n$$ \nDie (unbedingten) Wahrscheinlichkeiten $P(A)$ und $P(\\bar A)$ ergeben\nsich, indem man die Wahrscheinlichkeiten aller Pfade aufaddiert, die\neinen $A$-Ast (bzw. einen $\\bar A$-Ast) haben. Es ergibt sich\n$P(A)=0.68$ und $P(\\bar A)=0.32$.\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"output-file":"02-wahrscheinlichkeit.html"},"language":{"toc-title-document":"Inhalt","toc-title-website":"In diesem Kapitel"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","bibliography":["references.bib"],"editor":"source","callout-icon":false,"callout-tip-title":"Beispiele","callout-note-title":"Definition","theme":"sandstone"},"extensions":{"book":{"multiFile":true}}}}}
>>>>>>> Stashed changes
