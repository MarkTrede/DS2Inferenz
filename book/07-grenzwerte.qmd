# Grenzwertsätze {#sec-grenzwerte}

In diesem Kapitel gehen wir davon aus, dass eine Zufallsvariable 
immer wieder neu gezogen wird. Als Ausgangspunkt der Überlegungen
dient eine Folge von unabhängigen, identisch verteilten (engl. 
independent and identically distributed, i.i.d.) Zufallsvariablen 
$$
X_1,X_2,X_3,\ldots
$$
Man kann sich diese Folge als unabhängige Wiederholungen einer
Zufallsvariable $X$ vorstellen, z.B. als immer wieder neu
geworfene Würfel. Im folgenden bezeichnen wir den Erwartungswert
der Folgenelemente mit $E(X)=\mu$ und die Varianz mit
$Var(X)=\sigma^2$.

## Gesetz der großen Zahl {#sec-lln}

Aus den Elementen der Folge von Zufallsvariablen berechnen wir
für jedes $n$ das arithmetische Mittel (den Durchschnitt)
$$
\bar X_n=\frac{1}{n}\sum_{i=1}^n X_i.
$$
*Achtung*: Es handelt sich bei $\bar X_n$ um den Durchschnitt
von Zufallsvariablen, also quasi von Zahlen, deren Wert man noch
nicht kennt. Darum kennt man natürlich auch den Wert des Durchschnitts
noch nicht. Mit anderen Worten: Der Durchschnitt $\bar X_n$ ist
ebenfalls eine Zufallsvariable.

Da der Durchschnitt $\bar X_n$ eine Zufallsvariable ist, kann man
den Erwartungswert und die Varianz ausrechnen. Sie sind mit den
Rechenregeln aus @sec-erwartungswert und @sec-varianz leicht zu 
finden. Es gilt
$$
\begin{align*}
E(\bar X_n) &= E\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \frac{1}{n}\sum_{i=1}^n E(X_i) \\
&= \frac{1}{n}\sum_{i=1}^n \mu \\
&= \mu
\end{align*}
$$
und
$$
\begin{align*}
Var(\bar X_n) &= Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i) \\
&= \frac{1}{n^2}\sum_{i=1}^n \sigma^2 \\
&= \frac{\sigma^2}{n}.
\end{align*}
$$
Mit anderen Worten: Wenn man den Durchschnitt aus immer mehr
Folgeelementen bildet, dann ist der Erwartungswert immer $\mu$,
aber die Varianz wird mit steigendem $n$ immer kleiner. Der
Durchschnitt $\bar X_n$ liegt also mit steigendem $n$
"immer näher" am Erwartungswert $E(X)=\mu$ der Zufallsvariable $X$.

Diese Einsicht lässt sich mathematisch präzise formulieren.

::: callout-important
## Gesetz der großen Zahl

Das **Gesetz der großen Zahl** besagt, dass für
jedes (noch so kleine) $\epsilon >0$ gilt
$$
\lim_{n\to\infty} P(|\bar X_n-\mu|\ge \epsilon)=0.
$$
:::

Als alternative Notation findet man oft
$$
\text{plim}_{n\to\infty}\bar X_n=\mu,
$$
wobei man "plim" als "Wahrscheinlichkeits-Limes" oder engl.
"probability limit" spricht.

Es gibt neben diesem Gesetz der großen Zahl, das auch als
"schwaches Gesetz der großen Zahl" bezeichnet wird, noch weitere
Varianten, die aber im Kern ebenfalls aussagen, dass der Durchschnitt
in einem gewissen Sinn gegen den Erwartungswert konvergiert.

Das Gesetz der großen Zahl gilt nicht nur für den Erwartungswert,
sondern auch für andere wichtige Größen der Zufallsvariablen.
So konvergiert die Varianz der Folgeelemente gegen die Varianz
der Zufallsvariable; die empirische Verteilungsfunktion der Folgeelemente
konvergiert gegen die Verteilungsfunktion der Zufallsvariable;
die Quantile der Folgeelemente konvergieren gegen die Quantile
der Zufallsvariable; und das Histogramm der Folgeelmente konvergiert
gegen die Dichte der Zufallsvariable. 

## Monte-Carlo (I) {#sec-mc1}

Das Gesetz der großen Zahl hat eine weitreichende 
Konsequenz: Um die Verteilung einer Zufallsvariable zu kennen, 
muss man nicht unbedingt die Verteilungsfunktion,
Dichte oder Wahrscheinlichkeitsfunktion kennen, sondern es
reicht aus, wenn man einen **Algorithmus** zur Verfügung hat, der viele 
unabhängige Ziehungen aus der Zufallsvariable liefert. Solche
Algorithmen gibt es - und viele davon sind in R (oder anderen
Programmiersprachen) implementiert. Die Ziehungen, die vom
Computer generiert werden, nennt man **Zufallszahlen** (engl.
random numbers). Verfahren, die auf solchen Algorithmen
beruhen, werden **Monte-Carlo-Simulationen** genannt, weil
das Casino von Monte Carlo früher als Inbegriff von 
Zufälligkeit galt.

<img src="images/AdobeStock_266493629b.jpeg" width="90%"/>

Die R-Funktionen zum Erzeugen von Zufallszahlen haben alle 
die Form

`rVERTEILUNG(n, PARAMETER)`

Dabei gibt `n` an, wie viele Folgeelemente gezogen werden sollen.
Für die Standardverteilungen, die in @sec-verteilungen behandelt wurden,
lauten die Funktionsnamen:

- `rbinom(n, size, prob)`
- `rpois(n, lambda)`
- `rnorm(n, mean, sd)`
- `rexp(n, rate)`

Für die Paretoverteilung gibt es keinen vorgefertigten Algorithmus.
Das Paket `distributionsrd` stellt jedoch die Funktion 

- `rpareto(n, k, xmin)`

bereit, mit der Zufallszahlen aus einer Paretoverteilung gezogen
werden können.

Die Vorgehensweise einer Monte-Carlo-Simulation versteht
man am einfachsten anhand eines Beispiels. Das folgende 
Beispiel zeigt, wie durch eine Monte-Carlo-Simulation
Kennzahlen einer Normalverteilung mit Erwartungswert 
$\mu=10$ und Standardabweichung $\sigma=3$ bestimmt werden 
können. *Vorsicht*: Die R-Funktionen zur Normalverteilung
erwarten als Parameter für die Streuung nicht die
Varianz, sondern die Standardabweichung.

Zuerst werden $n=100\,000$ Zufallszahlen gezogen
(Sie können die Zahl der Ziehungen durchaus noch
höher setzen) und in einen Vektor `x` geschrieben.
```{r}
n <- 100000
x <- rnorm(n, mean=10, sd=3)
```
Der Erwartungswert wird approximiert durch den Durchschnitt.
```{r}
mean(x)
```
Dieser Wert liegt sehr nahe an dem theoretischen Erwartungswert
$E(X)=\mu=10$.

Die Varianz der Zufallszahlen ist
```{r}
var(x)
```
Der Wert weicht nur wenig von der theoretischen Varianz
$Var(X)=\sigma^2=9$ ab. 

Als 0.75-Quantil erhält man
```{r}
quantile(x, 0.75)
```
Das theoretische 0.75-Quantil der $N(10,3^2)$ ist
```{r}
qnorm(0.75, mean=10, sd=3)
```
Auch hier ist die Approximation durch die Monte-Carlo-Simulation
also sehr präzise.

Der folgende Plot zeigt das Histogramm der Zufallszahlen
zusammen mit der theoretischen Dichtefunktion der Normalverteilung,
die als rote Linie zu sehen ist.
```{r}
hist(x, breaks=100, probability=TRUE)
curve(dnorm(x, mean=10, sd=3), add=TRUE, col="red")
```
Zur Erklärung: Der R-Befehl `hist` erzeugt ein Histogramm. Die Option
`breaks` gibt an, wie viele Intervalle gebildet werden sollen. 
Die Option `probability=TRUE` dient dazu, die Fläche unter der
Dichte auf 1 zu normieren, so dass das Histogramm quasi eine Dichte wird.
Die eingefügte rote Linie zeigt die Dichte der Standardnormalverteilung.
Sie wird durch die Funktion `curve` erzeugt. Das erste Argument
von `curve` ist eine Funktion (von `x`), die geplottet werden soll.
Die Option `add=TRUE` legt fest, dass die Funktion in einen
bereits bestehenden Plot eingefügt wird. Und die Option 
`col` bestimmt die Farbe der Linie.

Im folgenden Plot sieht man die empirische Verteilungsfunktion 
der Ziehungen $X_1,\ldots,X_n$. Es handelt sich zwar um eine
Treppenfunktion, aber die Stufen sind so klein (nämlich jeweils
$1/n=0.00001$), dass sie nicht zu erkennen sind. Mit Hilfe
der Funktion `curve` ergänzen wir die theoretische
Verteilungsfunktion der Normalverteilung als rote Linie.
```{r}
plot(ecdf(x),
     main="Verteilungsfunktion")
curve(pnorm(x, mean=10, sd=3),
      add=TRUE,
      col="red")
```
Die beiden Linien stimmen praktisch perfekt überein. Man kann also
die theoretische Verteilungsfunktion beliebig genau bestimmen,
indem man eine große Zahl von Zufallszahlen generiert und 
ihre empirische Verteilungsfunktion bestimmt.

Offensichtlich ist es nicht sinnvoll, eine Monte-Carlo-Simulation 
durchzuführen, um Erwartungswert, Varianz, Quantile und Histogramm einer 
Verteilung zu finden, für die man diese Größen auch einfach
theoretisch herleiten kann. Monte-Carlo-Simulationen sind 
immer dann extrem hilfreich, 

- wenn es zwar nicht leicht ist, die theoretischen 
Größen zu bestimmen, 
- wenn man aber sehr leicht und schnell Zufallszahlen 
erzeugen kann aus der Verteilung, die einen interessiert. 

Das passiert beispielsweise, wenn wir uns
für eine Zufallsvariable interessieren, die zwar nicht unmittelbar
einer Standardverteilung folgt, die sich aber aus einer anderen
Zufallsvariable ergibt, welche wiederum einer Standardverteilung 
folgt. Das nächste Beispiel zeigt das.

Die Zufallsvariable $X$ folge einer Normalverteilung mit
Erwartungswert $\mu=8$ und Standardabweichung $\sigma=0.5$.
Die Zufallsvariable $Y=e^X$ soll den Bruttomonatslohn von
Angestellten einer bestimmten Branche modellieren. Wir interessieren
uns nicht für den Brutto-, sondern den Nettolohn. Letzterer
ergibt sich, indem man die Abgaben und Steuern vom Bruttolohn 
subtrahiert. Die Höhe der Abgaben und Steuern werde durch 
folgende Funktion beschrieben:
$$
S(y)=\left\{ 
\begin{array}{ll}
0 & \text{ für }y\le 2000\\
0.35(y-2000) & \text{ für }y> 2000\\
\end{array}
\right.
$$
wobei $y$ der Bruttolohn ist. Es handelt sich also um eine "flat tax"
von 35 Prozent mit einem Freibetrag von 2000. 
Der Nettolohn wird modelliert durch die Zufallsvariable
$$
Z=Y-S(Y)
$$
Wie hoch ist der Erwartungswert des Nettolohns? Das lässt
sich recht einfach durch eine Monte-Carlo-Simulation
ermitteln. Die Anzahl der Zufallszahlen sei $n=1\,000\,000$.
```{r}
n <- 1000000
x <- rnorm(n, mean=8, sd=0.5)
y <- exp(x)
s <- ifelse(y <= 2000, 0, 0.35*(y-2000))
z <- y-s
mean(z)
```
Die Dichte des Nettolohns kann durch das Histogramm
approximiert werden. Diese Form der Dichte entspricht keiner
bekannten Standardverteilung.
```{r}
hist(z, probability=TRUE, breaks=150, xlim=c(0,10000))
```

## Zentraler Grenzwertsatz {#sec-clt}

Der **zentrale Grenzwertsatz** (engl. central limit theorem) 
ist aus zwei Gründen relevant. Zum einen 
liefert er eine mathematische Begründung dafür, warum in der Realität
so viele Verteilungen zu finden sind, die sehr gut durch eine
Normalverteilung beschrieben werden können. Zum zweiten ist er der
Ausgangspunkt für die Herleitung von Konfidenzintervallen
(@sec-intervallschaetzung) und Hypothesentests 
(@sec-hypothesentests).

Für den zentralen Grenzwertsatz betrachtet man *nicht* die Folge
von Zufallsvariablen 
$$
\bar X_n=\frac{1}{n}\sum_{i=1}^n X_i
$$ 
(d.h. den Durchschnitt), sondern die
Folge der standardisierten $\bar X_n$. Dazu subtrahiert man den
Erwartungswert und dividiert durch die Standardabweichung. Zur 
Erinnerung: Der Erwartungswert von $\bar X_n$ ist $E(\bar X_n)=\mu$,
und die Standardabweichung ist $\sqrt{Var(\bar X_n)}=\sqrt{\sigma^2/n}$.
Die Folge der standardisierten Durchschnitte bezeichnen wir mit
$$
U_n=\frac{\bar X_n-\mu}{\sqrt{\sigma^2/n}}.
$$
Die Wurzel von $n$ wird im allgemeinen aus dem Nenner heraus
vor den Bruch gezogen,
$$
U_n=\sqrt{n}\frac{\bar X_n-\mu}{\sigma}.
$$

::: callout-important
## Zentraler Grenzwertsatz

Der **zentrale Grenzwertsatz** besagt, dass für alle $u\in\mathbb{R}$ gilt
$$
\lim_{n\to\infty} P(U_n\le u)=\Phi(u),
$$
wobei $\Phi$ die Verteilungsfunktion der Standardnormalverteilung 
$N(0,1)$ ist.
:::

Etwas weniger präzise - aber dafür leichter verständlich - sagt der
zentrale Grenzwertsatz, dass der standardisierte Durchschnitt von 
vielen Zufallsvariablen im Limes einer Standardnormalverteilung folgt.
Natürlich ist $n$ niemals wirklich unendlich groß, aber selbst
wenn $n$ nur "ausreichend groß" ist, gilt 
$$
U_n\stackrel{appr}{\sim}N(0,1).
$$
Der standardisierte Durchschnitt von "vielen" Zufallsvariablen 
ist annähernd normalverteilt. 
**Diese Aussage ist deswegen bemerkenswert, weil es gleichgültig ist, wie die zugrundeliegenden $X_i$ verteilt sind!**
Sie müssen nicht einmal stetig verteilt sein.

Wenn der standardisierte Durchschnitt approximativ 
standardnormalverteilt ist, dann ist der Durchschnitt selber
approximativ normalverteilt. Für ausreichend große $n$ gilt
also 
$$
\bar X_n \stackrel{appr}{\sim}N\left(\mu,\frac{\sigma^2}{n}\right).
$$
Und wenn beide Seiten mit $n$ multipliziert werden, dann
gilt für die Summe
$$
\sum_{i=1}^n X_i \stackrel{appr}{\sim}N\left(n\mu,n\sigma^2\right),
$$
wobei zu beachten ist, dass bei der Multiplikation mit $n$ sich die
Varianz um den Faktor $n^2$ erhöht. 

Die **intuitive Bedeutung** des zentralen Grenzwertsatzes lautet also:
Wenn man viele unabhängige Zufallsvariablen addiert oder ihren
Durchschnitt berechnet, dann ist die Summe bzw. der Durchschnitt
ebenfalls eine Zufallsvariable, und zwar mit einer Normalverteilung.
Das Besondere daran ist, dass die Verteilung der Summanden nicht
normal sein muss. Die Normalverteilung ergibt sich quasi "aus dem Nichts"!

Auf den Beweis des zentralen Grenzwertsatzes gehen wir in diesem
Kurs nicht ein, er wird in dem Bachelor-Modul **Advanced Statistics**
vorgestellt. Der zentrale Grenzwertsatz kann erheblich verallgemeinert 
werden. Er gilt z.B. auch dann, wenn die Zufallsvariablen nicht unabhängig
sind, sondern eine (nicht zu starke) Abhängigkeit aufweisen. Die
Summanden müssen auch nicht unbedingt identisch verteilt sein.
Die Verallgemeinerungen des zentralen Grenzwertsatzes sind aber
fortgeschrittene Materie und werden in diesem Kurs nicht behandelt.

## Monte-Carlo (II) {#sec-mc2}

Wie lässt sich der zentrale Grenzwertsatz durch eine Monte-Carlo-Simulation
illustrieren? Um das zu verstehen, muss man sich zunächst klar machen,
dass die Folge $U_1,U_2,\ldots$ nicht gegen einen bestimmten *Wert*, sondern 
gegen eine *Verteilung* konvergiert. 

Um die Verteilung von $U_n$ zu simulieren, müssen wir 
*für gegebenes* $n$ viele Ziehungen aus $U_n$ generieren, die 
wir beispielsweise
$$
U_{n,1},U_{n,2},\ldots,U_{n,R}
$$
nennen können, wobei $R$ die vorgegebene (große) Zahl von Ziehungen ist,
z.B. $R=10000$. Für jede dieser Ziehungen müssen die Zufallsvariablen
$X_1,\ldots,X_n$ neu gezogen werden. Das lässt sich am elegantesten
durch eine Schleife programmieren. Genauere Erklärungen zum Programmieren
mit Schleifen finden Sie im @sec-programmieren.

Bevor man die Schleife durchlaufen lässt, initialisiert man einen
Vektor, der die Ergebnisse der Schleifendurchläufe aufnimmt. Welche
Werte in diesem Vektor stehen, ist gleichgültig, da sie in der Schleife 
ohnehin überschrieben werden. Es geht also nur darum, Platz für die
Ergebnisse vorzubereiten.
```{r}
R <- 10000
u <- rep(0, R)
```
Innerhalb der Schleife werden nun in jedem Durchlauf die
Zufallszahlen $X_1,\ldots,X_n$ gezogen. Konkret nehmen wir
an dieser Stelle an, dass die Folge $n=50$ Elemente umfasst
und dass sie unabhängig exponentialverteilt mit Parameter $\lambda=2$
sind, 
$$
X\sim Exp(2).
$$
Erwartungswert und Standardabweichung sind
$$
\begin{align*}
E(X) &= \frac{1}{2}\\
Var(X)& =\frac{1}{4}.
\end{align*}
$$
Der R-Code sieht folgendermaßen aus:
```{r}
mu <- 1/2
sigma <- sqrt(1/4)
n <- 50
for(r in 1:R){
    x <- rexp(n, rate=2)
    xquer <- mean(x)
    u[r] <- sqrt(n)*(xquer - mu)/sigma
}
```
Zur Erklärung: Der Laufindex der Schleife `r` durchläuft
alle Werte von 1 bis `R`. Bei jedem Durchlauf wird zuerst
die Folge `x` vom Umfang `n` aus der Rechteckverteilung
gezogen. Anschließend wird der Durchschnitt der 
Elemente der Folge gebildet und unter dem Namen `xquer`
abgespeichert. In der letzten Zeile der Schleife 
wird der Durchschnitt standardisiert und in die
`r`-te Stelle des Vektors `u` geschrieben.

Das Histogramm der $R=10000$ Zufallszahlen 
$U_{n,1},\ldots,U_{n,R}$ kann trotz des noch kleinen
Umfang $n=50$ schon recht gut durch eine Standardnormalverteilung
approximiert werden.
```{r}
hist(u, prob=TRUE, breaks=50)
curve(dnorm(x), add=TRUE, col="red")
```
Reduziert man die Länge der Folge auf $n=5$ Elemente, greift
der zentrale Grenzwertsatz noch nicht so gut, wie das folgende Bild
zeigt:
```{r}
n <- 5
for(r in 1:R){
    x <- rexp(n, rate=2)
    xquer <- mean(x)
    u[r] <- sqrt(n)*(xquer - 1/2)/sqrt(1/4)
}
hist(u, prob=TRUE, breaks=50)
curve(dnorm(x), add=TRUE, col="red")
```
Wie viele Elemente die Folge $X_1,\ldots,X_n$ haben muss,
damit der zentrale Grenzwertsatz greift, hängt von ihrer
Verteilung ab. Es kann sein, dass schon für $n=10$ die
Approximation sehr gut ist, es kann jedoch auch sein,
dass selbst ein Umfang von $n=1000$ oder sogar $n=10000$
noch zu klein ist.
Erfahrungsgemäß ist in den meisten Fällen ein Umfang
von $n=40$ schon ausreichend groß, um eine einigermaßen gute
Annäherung an die Standardnormalverteilung zu erreichen.
Das ist jedoch nur eine Faustregel, sie kann im 
konkreten Einzelfall vollkommen falsch sein.
