<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Kapitel 2 Wahrscheinlichkeit | Data Science 2: Statistische Inferenz</title>
<meta name="author" content="Mark Trede">
<meta name="description" content="2.1 Definition Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung (Funktion) angegeben. Eine Abbildung \(P:...">
<meta name="generator" content="bookdown 0.33 with bs4_book()">
<meta property="og:title" content="Kapitel 2 Wahrscheinlichkeit | Data Science 2: Statistische Inferenz">
<meta property="og:type" content="book">
<meta property="og:description" content="2.1 Definition Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung (Funktion) angegeben. Eine Abbildung \(P:...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kapitel 2 Wahrscheinlichkeit | Data Science 2: Statistische Inferenz">
<meta name="twitter:description" content="2.1 Definition Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine Abbildung (Funktion) angegeben. Eine Abbildung \(P:...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Data Science 2: Statistische Inferenz</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Suche" aria-label="Search">
</form>

      <nav aria-label="Inhalt"><h2>Inhaltsverzeichnis</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Einleitung</a></li>
<li><a class="" href="zufall.html"><span class="header-section-number">1</span> Zufallsvorgänge</a></li>
<li><a class="active" href="wahrscheinlichkeit.html"><span class="header-section-number">2</span> Wahrscheinlichkeit</a></li>
<li><a class="" href="zufallsvariablen.html"><span class="header-section-number">3</span> Zufallsvariablen</a></li>
<li><a class="" href="zufallsvektoren.html"><span class="header-section-number">4</span> Zufallsvektoren</a></li>
<li><a class="" href="stichproben.html"><span class="header-section-number">5</span> Stichproben</a></li>
<li><a class="" href="punktschaetzung.html"><span class="header-section-number">6</span> Punktschätzung</a></li>
<li><a class="" href="intervallschaetzung.html"><span class="header-section-number">7</span> Intervallschätzung</a></li>
<li><a class="" href="hypothesentests.html"><span class="header-section-number">8</span> Hypothesentests</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="wahrscheinlichkeit" class="section level1" number="2">
<h1>
<span class="header-section-number">Kapitel 2</span> Wahrscheinlichkeit<a class="anchor" aria-label="anchor" href="#wahrscheinlichkeit"><i class="fas fa-link"></i></a>
</h1>
<div id="def:wahrscheinlichkeit" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Definition<a class="anchor" aria-label="anchor" href="#def:wahrscheinlichkeit"><i class="fas fa-link"></i></a>
</h2>
<p>Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch
die Wahrscheinlichkeit eines Ereignisses ist, wird durch eine
Abbildung (Funktion) angegeben.</p>
<blockquote>
<p>Eine Abbildung <span class="math inline">\(P: A \mapsto P(A)\)</span>
heißt <strong>Wahrscheinlichkeitsmaß</strong>, wenn gilt</p>
<ul>
<li><p>Nichtnegativität: <span class="math inline">\(P(A)\ge 0\)</span> für alle Ereignisse <span class="math inline">\(A\)</span></p></li>
<li><p>Normierung: <span class="math inline">\(P(\Omega)=1\)</span></p></li>
<li><p>Additivität: Für disjunkte Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span>
(d.h. <span class="math inline">\(A\cap B=\emptyset\)</span>) ist <span class="math inline">\(P(A\cup B)=P(A)+P(B)\)</span></p></li>
</ul>
</blockquote>
<p>Diese Eigenschaften, die eine Wahrscheinlichkeitsabbildung erfüllen
muss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten
hergeleitet werden, daher spricht man auch von Axiomen. Etwas allgemeiner
und präziser wurden diese Axiome 1933 von Andrey Kolmogorov
für eine saubere mathematische Fundierung der Wahrscheinlichkeitstheorie
eingeführt.</p>
<p>Aus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum Beispiel</p>
<ul>
<li><p>Monotonität: Wenn <span class="math inline">\(A\subseteq B\)</span>, dann ist <span class="math inline">\(P(A)\le P(B)\)</span></p></li>
<li><p>Komplementärereignis: <span class="math inline">\(P(\bar A)=1-P(A)\)</span></p></li>
<li><p>Additionssatz: <span class="math inline">\(P(A\cup B)=P(A)+P(B)-P(A\cap B)\)</span></p></li>
</ul>
</div>
<div id="venn" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Venn-Diagramme<a class="anchor" aria-label="anchor" href="#venn"><i class="fas fa-link"></i></a>
</h2>
<p>Ereignisse lassen sich - wie andere Mengen in der Mengenlehre auch - in
Form von Venn-Diagrammen darstellen. Wenn die Ergebnismenge <span class="math inline">\(\Omega\)</span>
durch ein Rechteck repräsentiert wird, dann kann man Ereignisse
als Teilmengen des Rechtecks darstellen. In dem folgenden Bild ist zum
Beispiel das Ereignis <span class="math inline">\(A\)</span> der kleine Kreis in der Mitte.
<img src="02-wahrscheinlichkeit_files/figure-html/unnamed-chunk-1-1.png" width="95%"></p>
<p>In diesem Venn-Diagramm wird deutlich, dass die Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(C\)</span>
nicht gleichzeitig eintreten können, <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> hingegen schon, denn
ihre Schnittmenge ist nicht leer. Außerdem
erkennt man, dass <span class="math inline">\(A\)</span> eine kleinere Wahrscheinlichkeit hat als <span class="math inline">\(B\)</span> und <span class="math inline">\(C\)</span>.
Der Additionssatz <span class="math inline">\(P(A\cup B)=P(A)+P(B)-P(A\cap B)\)</span>
wird in dieser grafischen Sichtweise sehr einfach klar:
Die Wahrscheinlichkeit von <span class="math inline">\(A\cup B\)</span> (also <span class="math inline">\(A\)</span> oder <span class="math inline">\(B\)</span>) ergibt sich als
die Summe aus den beiden Kreisflächen <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> abzüglich der Fläche
der Schnittmenge, die sonst doppelt gezählt werden würde.</p>
</div>
<div id="laplace" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Laplace-Experimente<a class="anchor" aria-label="anchor" href="#laplace"><i class="fas fa-link"></i></a>
</h2>
<p>Eine besonders einfache Art von Zufallsvorgängen sind die sogenannten
Laplace-Experimente.</p>
<blockquote>
<p>Ein Zufallsvorgang heißt <strong>Laplace-Experiment</strong>, wenn es nur endlich viele
Ergebnisse gibt (d.h. wenn <span class="math inline">\(|\Omega|=n\)</span>) und wenn alle
Elementarereignisse als gleich wahrscheinlich angenommen werden können.</p>
</blockquote>
<div class="inline-figure">Beachten Sie, dass es sich
dabei um eine Aussage handelt, die aus unserem Alltagswissen herrührt,
nicht aus mathematischen Überlegungen oder Herleitungen.
<img src="images/kronkorken.jpg" align="right" width="20%">
Ein typisches, einfaches Beispiel für ein Laplace-Experiment sind Würfelwürfe.
Es ist aus unserem Alltagswissen heraus plausibel, davon auszugehen,
dass alle Augenzahlen eines Würfels gleich wahrscheinlich sind. Auch
bei einer Münze ist es naheliegend, dass die Wahrscheinlichkeit für
Kopf und die Wahrscheinlichkeit für Zahl gleich sind. Hingegen würde
man beim Werfen eines Kronkorkens nicht unbedingt vermuten, dass beide Seiten
mit der gleichen Wahrscheinlichkeit oben liegen. Hier liegt also kein
Laplace-Experiment vor.</div>
<p>Bei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein
Ergebnis <span class="math inline">\(A\)</span> eintritt leicht zu ermitteln. Sie beträgt
<span class="math display">\[
P(A)=\frac{|A|}{|\Omega|},
\]</span>
also die Anzahl der Ergebnisse in <span class="math inline">\(A\)</span> dividiert durch die Anzahl aller
Ergebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss man
also abzählen, wie viele Ergebnisse in den Mengen sind. Das ist manchmal
sehr einfach, kann aber bei großen Mengen kompliziert sein. In solchen
Fällen hilft der Teilbereich der Mathematik weiter, den man “Kombinatorik”
nennt. Wir gehen in diesem Kurs jedoch nicht näher auf
kombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei
denen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.
Die Anzahl der Ergebnisse in <span class="math inline">\(\Omega\)</span> beträgt 6. Sei <span class="math inline">\(A\)</span> das
Ereignis “Eine gerade Zahl wird geworfen”, also <span class="math inline">\(A=\{2,4,6\}\)</span>.
Dann ist
<span class="math display">\[
\begin{align*}
P(A)&amp;=\frac{|A|}{|\Omega|}\\
&amp;=\frac{3}{6}\\
&amp;=0.5.
\end{align*}
\]</span></p></li>
<li>
<p>Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine Rolle,
wie man die Ergebnismenge festlegt. Wir wählen
<span class="math display">\[
\begin{align*}
\Omega=\{&amp;11,12,13,14,15,16,\\
&amp;21,22,23,24,25,26,\\
&amp;31,32,33,34,35,36,\\
&amp;41,42,43,44,45,46,\\
&amp;51,52,53,54,55,56,\\
&amp;61,62,63,64,65,66\}
\end{align*}
\]</span>
weil es sich dann um ein Laplace-Experiment handelt. Die Ergebnisse
“24” und “42” sind zwar nicht unterscheidbar, wenn die Würfel gleich
aussehen, aber wir können den zuerst geworfenen Würfel (oder den
weiter links liegenden) als ersten Würfel bezeichnen. Dass alle 36
Ergebnisse gleich wahrscheinlich sind, können wir mit unserem
Alltagswissen begründen, nicht aus der Mathematik heraus.</p>
<p>Sei <span class="math inline">\(A\)</span> das Ereignis “Die Augenzahlen der beiden Würfel unterscheiden
sich um 2”, d.h. <span class="math inline">\(A=\{13,24,31,35,42,46,53,64\}\)</span>. Dann gilt
<span class="math display">\[
\begin{align*}
P(A)&amp;=\frac{|A|}{|\Omega|}\\
&amp;=\frac{8}{36}\\
&amp;=\frac{2}{9}\\
&amp;\approx 0.2222.
\end{align*}
\]</span>
Als Ergebnismenge wäre auch
<span class="math display">\[
\begin{align*}
\Omega=\{
&amp;11,12,13,14,15,16,\\
&amp;22,23,24,25,26,\\
&amp;33,34,35,36,\\
&amp;44,45,46,\\
&amp;55,56,\\
&amp;66\}
\end{align*}
\]</span>
möglich gewesen, aber dann wäre die Annahme eines Laplace-Experiments
nicht plausibel gewesen. Die Elementarereignisse “11” und “12” wären
beispielsweise nicht gleich wahrscheinlich (ein Pasch tritt
erfahrungsgemäß seltener auf).</p>
<p>Die Wahl der Ergebnismenge sollte immer so erfolgen, dass das weitere
Vorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge
so gewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte
man das tun.</p>
</li>
</ul>
</div>
<div id="bedingt" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Bedingte Wahrscheinlichkeit<a class="anchor" aria-label="anchor" href="#bedingt"><i class="fas fa-link"></i></a>
</h2>
<p>Manchmal gibt es begrenzte Informationen über einen Zufallsvorgang.
Dann kennt man nicht das realisierte Ergebnis, kann aber die Menge der
möglichen Ergebnisse eingrenzen. Dadurch ändern sich die
Wahrscheinlichkeiten für Ereignisse.</p>
<blockquote>
<p>Wir betrachten zwei Ereignsse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> mit <span class="math inline">\(P(B)&gt;0\)</span>.
Dann heißt
<span class="math display">\[
P(A|B)=\frac{P(A\cap B)}{P(B)}
\]</span>
die <strong>bedingte Wahrscheinlichkeit</strong> von <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span></p>
</blockquote>
<p>Die Notation <span class="math inline">\(A|B\)</span> steht nicht für ein bestimmtes Ereignis, sondern
zeigt an, dass wir eine neue Art von Wahrscheinlichkeit betrachten,
nämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über
Wahrscheinlichkeiten ist es nicht immer einfach (aber wichtig!),
zwischen der Wahrscheinlichkeit <span class="math inline">\(P(A\cap B)\)</span> und der bedingten
Wahrscheinlichkeit <span class="math inline">\(P(A|B)\)</span> zu unterscheiden. Wenn man <span class="math inline">\(P(A\cap B)\)</span>
meint, spricht man von der Wahrscheinlichkeit für <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span>.
Wenn man <span class="math inline">\(P(A|B)\)</span> meint, sagt man <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span>, oder: <span class="math inline">\(A\)</span> wenn <span class="math inline">\(B\)</span>,
oder: <span class="math inline">\(A\)</span> unter der Bedingung <span class="math inline">\(B\)</span>. Wenn man ausdrücklich angeben
möchte, dass eine Wahrscheinlichkeit <em>keine</em> bedingte Wahrscheinlichkeit
ist, nennt man sie auch eine unbedingte Wahrscheinlichkeit.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse
<span class="math inline">\(A=\{2,4,6\}\)</span> (“gerade Zahl”), und <span class="math inline">\(B=\{4,5,6\}\)</span> (“eine Zahl größer als 3”).
Die bedingte Wahrscheinlichkeit von <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span> beträgt
<span class="math display">\[
\begin{align*}
P(A|B)&amp;=\frac{P(A\cap B)}{P(B)}\\
&amp;=\frac{P(\{4,6\})}{P(\{4,5,6\})}\\
&amp;=\frac{2}{3}
\end{align*}
\]</span>
und die bedingte Wahrscheinlichkeit von <span class="math inline">\(B\)</span> gegeben <span class="math inline">\(A\)</span> lautet
<span class="math display">\[
\begin{align*}
P(B|A)&amp;=\frac{P(B\cap A)}{P(A)}\\
&amp;=\frac{P(\{4,6\})}{P(\{2,4,6\})}\\
&amp;=\frac{2}{3}.
\end{align*}
\]</span></p></li>
<li><p>Zwei Würfel werden geworfen. Sei <span class="math inline">\(A\)</span> das Ereignis “die Augensumme ist 10”.
Sei <span class="math inline">\(B\)</span> das Ereignis “einer der beiden Würfel (oder beide) zeigt eine 2”.
Dann ist
<span class="math display">\[
\begin{align*}
P(A|B)&amp;=\frac{P(A\cap B)}{P(B)}\\
&amp;=\frac{P(\{\})}{P(\{21,22,23,24,25,26,12,32,42,52,62\})}\\
&amp;=0.
\end{align*}
\]</span>
Die bedingte Wahrscheinlichkeit für das Ereignis <span class="math inline">\(C\)</span>: “Augensumme ist 5”
unter der Bedingung <span class="math inline">\(B\)</span> ist
<span class="math display">\[
\begin{align*}
P(C|B)&amp;=\frac{P(C\cap B)}{P(B)}\\
&amp;=\frac{P(\{23,32\})}{P(\{21,22,23,24,25,26,12,32,42,52,62\})}\\
&amp;=\frac{2}{11}.
\end{align*}
\]</span></p></li>
<li><p>Bei einer Umfrage werden Personen zufällig ausgewählt und befragt. Sei <span class="math inline">\(A\)</span>
das Ereignis “die befragte Person ist weiblich”. Sei <span class="math inline">\(B\)</span> das Ereignis
“die befragte Person arbeitet in Teilzeit”. Zwischen der (unbedingten)
Wahrscheinlichkeit <span class="math inline">\(P(B)\)</span> und der bedingten Wahrscheinlichkeit <span class="math inline">\(P(B|A)\)</span>
besteht ein Unterschied. Die Wahrscheinlichkeit <span class="math inline">\(P(B)\)</span> steht dafür, dass
eine zufällig ausgewählte Person in Teilzeit arbeitet, und diese Person
kann männlich oder weiblich sein. Dagegen ist <span class="math inline">\(P(B|A)\)</span> die
Wahrscheinlichkeit, dass eine zufällig ausgewählte Frau in Teilzeit
arbeitet.</p></li>
</ul>
<p>In den Medien werden häufig bedingte Wahrscheinlichkeiten berichtet,
ohne dass das explizit erwähnt wird. Oftmals sind die berichteten
bedingten Wahrscheinlichkeiten gar nicht die, für die man sich
eigentlich interessiert, weil die Bedingung und das Bedingte
quasi falsch herum angeordnet sind. Man interessiert sich für <span class="math inline">\(P(A|B)\)</span>,
aber berichtet wird <span class="math inline">\(P(B|A)\)</span>.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>In den Medien wird berichtet, dass ein Corona-Schnelltest mit einer
Wahrscheinlichkeit von xxx Prozent ein falsch positives Ergebnis liefert.
Falsch positiv bedeutet, die Wahrscheinlichkeit wird unter der Bedingung
angegeben, dass die getestete Person negativ ist. Wenn man Test an sich
durchführt, interessiert man sich dagegen für die bedingte
Wahrscheinlichkeit, infiziert zu sein, obwohl der Test negativ ausfällt,
und natürlich auch für die bedingte Wahrscheinlichkeit, tatsächlich
infiziert zu sein, wenn der Test positiv ausfällt.</p></li>
<li><p>In den Medien erfährt man, dass XX Prozent der Studierenden ein
“akademisches Elternhaus” haben. Es handelt sich um die bedingte
Wahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person
studiert. Für die Bewertung der Chancengleichheit ist hingegen die
bedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert, wenn
es aus einem akademischen Elternhaus stammt - und zwar im Vergleich
zu der bedingten Wahrscheinlichkeit, dass ein Kind studiert, wenn
es aus einem nicht-akademischen Elternhaus stammt.</p></li>
</ul>
<p>Wie kann man eine bedingte Wahrscheinlichkeit “umdrehen”? Wie verhält sich
<span class="math inline">\(P(B|A)\)</span> zu <span class="math inline">\(P(A|B)\)</span>? Das sehen wir im folgenden Abschnitt <a href="wahrscheinlichkeit.html#bayes">2.6</a>.</p>
<p>Bedingte Wahrscheinlichkeiten sind auch nützlich, um die Wahrscheinlichkeit
zu berechnen, dass mehrere Ereignisse gemeinsam passieren. Dazu formen wir
die Definition der bedingten Wahrscheinlichkeit einfach um. Aus
<span class="math display">\[
P(A|B)=\frac{P(A\cap B)}{P(B)}
\]</span>
wird
<span class="math display">\[
P(A\cap B)={P(B)P(A|B)}
\]</span></p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>Eins</p></li>
<li><p>Zwei</p></li>
</ul>
<p>Das lässt sich auf mehr als zwei Ereignisse erweitern:
<span class="math display">\[
\begin{align*}
P(A\cap B \cap C)&amp;=P(C)P(B|C)P(A|B\cap C)\\
P(A\cap B\cap C\cap D)&amp;=P(D)P(C|D)P(B|C\cap D)P(A|B\cap C\cap D)
\end{align*}
\]</span>
Diese Formeln sind bei näherer Betrachtung intuitiv: Die Wahrscheinlichkeit,
dass <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> und <span class="math inline">\(C\)</span> gemeinsam eintreten, ergibt sich, indem man zuerst
eine der unbedingten Wahrscheinlichkeiten nimmt (z.B. <span class="math inline">\(P(C)\)</span>).
Nun ist <span class="math inline">\(C\)</span> quasi eingetreten und wir arbeiten unter der Bedingung <span class="math inline">\(C\)</span>
weiter. Die Wahrscheinlichkeit <span class="math inline">\(P(C)\)</span> wird jetzt mit der
bedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,
also <span class="math inline">\(P(B|C)\)</span>. Nun sind <span class="math inline">\(B\)</span> und <span class="math inline">\(C\)</span> eingetreten, und auf der nächsten
Stufe multiplizieren wir deshalb mit <span class="math inline">\(P(A|B\cap C)\)</span>. Eine andere
Reihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B.
<span class="math display">\[
P(A\cap B\cap C)=P(A)P(B|A)P(C|A\cap B).
\]</span></p>
</div>
<div id="total" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Totale Wahrscheinlichkeit<a class="anchor" aria-label="anchor" href="#total"><i class="fas fa-link"></i></a>
</h2>
<p>Aus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte
Wahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum <span class="math inline">\(\Omega\)</span>
in eine Partition. Unter einer Partition versteht man eine Zerlegung
in disjunkte Mengen <span class="math inline">\(A_1, A_2, \ldots, A_n\)</span>, so dass die Vereinigungsmenge
der <span class="math inline">\(A_1,\ldots, A_n\)</span> wieder <span class="math inline">\(\Omega\)</span> ergibt. Die “Partitionierung eines
Rinds” könnte etwa so aussehen:</p>
<div class="inline-figure"><img src="images/partitionrind.jpg" width="100%"></div>
<p>Das Rind wird also vollständig Teilmengen zerlegt, und alle Teilmengen
zusammen ergeben wieder das gesamte Rind.</p>
<p>Nun nehmen wir an, dass für jedes Ereignis <span class="math inline">\(A_i\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, die
bedingte Wahrscheinlichkeit <span class="math inline">\(P(B|A_i\)</span> gegeben ist, wobei <span class="math inline">\(B\)</span> irgendein
Ereignis ist. Dann gilt für die unbedingte Wahrscheinlichkeit von <span class="math inline">\(B\)</span>:
<span class="math display">\[
P(B)=\sum_{i=1}^n P(B|A_i)P(A_i).
\]</span>
Die unbedingte Wahrscheinlichkeit ergibt sich also als gewichtete Summe
aller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch die
Wahrscheinlichkeiten der bedingenden Ereignisse.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>Eins</p></li>
<li><p>Zwei</p></li>
</ul>
</div>
<div id="bayes" class="section level2" number="2.6">
<h2>
<span class="header-section-number">2.6</span> Satz von Bayes<a class="anchor" aria-label="anchor" href="#bayes"><i class="fas fa-link"></i></a>
</h2>
<p>Der Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten
<span class="math inline">\(P(A|B)\)</span> und <span class="math inline">\(P(B|A)\)</span> zueinander in Beziehung. Es gilt
<span class="math display">\[
P(A|B)=\frac{P(B|A)P(A)}{P(B)}.
\]</span>
Um die Bedingung und das Bedingte zu vertauschen, braucht man also beide
unbedingte Wahrscheinlichkeiten. Die Herleitung des Satzes von Bayes
ergibt sich aus der Definition der bedingten Wahrscheinlichkeit.
Wegen
<span class="math display">\[
\begin{align*}
P(A|B) &amp;= \frac{P(A\cap B)}{P(B)}\\
P(B|A) &amp;= \frac{P(A\cap B)}{P(A)}
\end{align*}
\]</span>
gilt
<span class="math display">\[
P(A|B)P(B)=P(B|A)P(A).
\]</span>
Daraus folgt durch Umstellen unmittelbar der Satz von Bayes.</p>
<p><strong>Beispiele:</strong></p>
<ul>
<li><p>Eins</p></li>
<li><p>Zwei</p></li>
</ul>
</div>
<div id="unabhängigkeit" class="section level2" number="2.7">
<h2>
<span class="header-section-number">2.7</span> Unabhängigkeit<a class="anchor" aria-label="anchor" href="#unabh%C3%A4ngigkeit"><i class="fas fa-link"></i></a>
</h2>
<p>Zwei Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> heißen unabhängig (oder auch stochastisch
unabhängig), wenn
<span class="math display">\[
P(A\cap B)=P(A)\cdot P(B).
\]</span>
Eine alternative (fast äquivalente) Definition der Unabhängigkeit lautet so:
Die Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> heißen unabhängig, wenn gilt
<span class="math display">\[
P(A|B)=P(A)
\]</span>
bzw. wenn
<span class="math display">\[
P(B|A)=P(B).
\]</span>
Die Definition über die bedingten Wahrscheinlichkeiten ist etwas intuitiver.
Wenn das Wissen darüber, ob <span class="math inline">\(A\)</span> eingetreten ist oder nicht, für die
Wahrscheinlichkeit von <span class="math inline">\(B\)</span> keine Rolle spielt (und umgekehrt),
dann sind <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> unabhängig.</p>
<p>Achten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln.
Zur Erinnerung: Die Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> sind disjunkt, wenn
<span class="math inline">\(A\cap B=\emptyset\)</span>, so dass <span class="math inline">\(P(A\cap B)=0\)</span>. Disjunkte Ereignisse
können also nicht beide zusammen eintreten. Wenn <span class="math inline">\(A\)</span> eintritt, kann
man sicher sein, dass <span class="math inline">\(B\)</span> nicht eintritt. Und wenn <span class="math inline">\(B\)</span> eintritt,
kann man sicher sein, dass <span class="math inline">\(A\)</span> nicht eintritt. Unabhängige Ereignisse
können hingegen sehr wohl zusammen eintreten.</p>
<p>Wie lässt sich die Definition von Unabhängigkeit auf mehr als
zwei Ereignisse verallgemeinern? Das ist etwas komplizierter, als man es
vielleicht zunächst vermutet. Intuitiv gesprochen liegt bei nur zwei
Ereignissen (<span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span>) Unabhängigkeit dann vor, wenn Informationen über die
Ausprägung eines Ereignisses die Wahrscheinlichkeit für das andere Ereignis nicht
verändern. Bei drei Ereignissen (<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>) kann es passieren, dass die
Information über <span class="math inline">\(A\)</span> weder die Wahrscheinlichkeit von <span class="math inline">\(B\)</span> noch die von
<span class="math inline">\(C\)</span> verändert, dass aber die Information über <span class="math inline">\(A\)</span> <em>und</em> <span class="math inline">\(B\)</span> dennoch
die Wahrscheinlichkeit von <span class="math inline">\(C\)</span> verändert. Sehen wir uns dazu ein einfaches
Beispiel an:</p>
<p><strong>Beispiel:</strong></p>
<p>Zwei faire Würfel werden geworfen (mit der üblichen Ergebnismenge). Wir definieren
die folgenden drei Ereignisse:</p>
<p><span class="math inline">\(A\)</span>: Die Augenzahl des ersten Würfels ist ungerade</p>
<p><span class="math inline">\(B\)</span>: Die Augenzahl des zweiten Würfels ist ungerade</p>
<p><span class="math inline">\(C\)</span>: Die Summe der Augenzahlen ist ungerade</p>
<p>Wenn man zwei beliebige dieser Ereignisse auswählt, dann sind sie
unabhängig voneinander. Das dritte Ereignis ist aber von den beiden
anderen abhängig, weil die Summe von zwei ungeraden Zahlen niemals
ungerade ist. Weil die Situation bei mehr als zwei Ereignissen
also komplexer ist, wird Unabhängigkeit für mehrere Ereignisse
so definiert:</p>
<blockquote>
<p>Seien <span class="math inline">\(A_1,\ldots,A_n\)</span> Ereignisse. Sie heißen <em>global unabhängig</em>,
wenn für jedes <span class="math inline">\(m\in \{2,\ldots,n\}\)</span> und alle möglichen
Indizes <span class="math inline">\(1\le i_1&lt;\ldots&lt;i_m\le n\)</span> gilt
<span class="math display">\[P(A_{i_1}\cap\ldots\cap A_{i_m})=P(A_{i_1})\cdot\ldots\cdot P(A_{i_m})\]</span>.</p>
</blockquote>
<p>Die formale Definition ist nicht ganz leicht zu durchschauen. Letztlich
sagt sie aber einfach aus: Bei globaler Unabhängigkeit ist die Wahrscheinlichkeit,
dass beliebig viele beliebig ausgewählte Ereignisse gemeinsam eintreten,
gerade dem Produkt der einzelnen Wahrscheinlichkeiten entspricht.</p>
</div>
<div id="vier-felder-tafeln" class="section level2" number="2.8">
<h2>
<span class="header-section-number">2.8</span> Vier-Felder-Tafeln<a class="anchor" aria-label="anchor" href="#vier-felder-tafeln"><i class="fas fa-link"></i></a>
</h2>
<p>Die Vier-Felder-Tafel ist ein besonders einfaches, aber nützliches
Werkzeug für Fragen zu Wahrscheinlichkeiten, wenn zwei genau Ereignisse
(und ihre Gegenereignisse) eine Rolle spielen, z.B. <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> (und
<span class="math inline">\(\bar A\)</span> und <span class="math inline">\(\bar B\)</span>). Jedes der beiden Ereignisse kann eintrefen oder nicht,
so dass es insgesamt vier Kombinationsmöglichkeiten gibt. Die Wahrscheinlichkeiten
dieser vier Möglichkeiten werden wie folgt angeordnet:
<span class="math display">\[
\begin{array}{c|cc|}
&amp; A &amp; \bar A&amp; \\ \hline
B &amp; P(A\cap B) &amp; P(\bar A\cap B)\\
\bar B &amp; P(A\cap \bar B) &amp; P(\bar A\cap \bar B)\\ \hline
\end{array}
\]</span>
Da <span class="math inline">\(P((A\cap B)\cup (A\cap \bar B))=P(A)\)</span> ist (und entsprechend für
die anderen Zeilen und Spalten), kann man die Ränder ergänzen:
<span class="math display">\[
\begin{array}{c|cc|c}
&amp; A &amp; \bar A&amp; \\ \hline
B &amp; P(A\cap B) &amp; P(\bar A\cap B) &amp; P(B)\\
\bar B &amp; P(A\cap \bar B) &amp; P(\bar A\cap \bar B) &amp; P(\bar B)\\ \hline
&amp; P(A) &amp; P(\bar A) &amp; 1
\end{array}
\]</span>
Weil die Zeilen und Spalten sich immer zu den Randwerten addieren müssen,
kann man Felder leicht ergänzen, wenn in einer Zeilen oder
Spalte ein Wert fehlt.</p>
<p>Will man eine Vier-Felder-Tafel ausfüllen, dann ist es wichtig darauf zu
achten, ob es sich bei den vorgegebenen Wahrscheinlichkeiten um gemeinsame
Wahrscheinlichkeiten (z.B. <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> bzw. <span class="math inline">\(P(A\cap B)\)</span>) handelt, oder
um bedingte Wahrscheinlichkeiten (also z.B. <span class="math inline">\(P(A|B)\)</span>). Letztere können
leicht umgeformt werden, wenn man den Zusammenhang
<span class="math display">\[
P(A\cap B)=P(A|B)P(B)
\]</span>
beachtet (oder die analogen Zusammenhänge für andere bedingte
Wahrscheinlichkeiten).</p>
<p><strong>Beispiele:</strong></p>
</div>
<div id="wahrscheinlichkeitsbäume" class="section level2" number="2.9">
<h2>
<span class="header-section-number">2.9</span> Wahrscheinlichkeitsbäume<a class="anchor" aria-label="anchor" href="#wahrscheinlichkeitsb%C3%A4ume"><i class="fas fa-link"></i></a>
</h2>
<p>Vier-Felder-Tafeln sind ein nützliches Werkzeug, um einfaches
Wahrscheinlichkeitsprobleme zu lösen, in denen nur zwei Ereignisse und
ihre Gegenwahrscheinlichkeiten vorkommen. Ein Werkzeug, mit dem man auch
allgemeine und etwas komplexere Probleme übersichtlich bearbeiten kann,
sind Wahrscheinlichkeitsbäume. Die Ereignisse - es können hier durch
aus mehr als zwei sein - bilden die Äste des Baums.</p>
<p><strong>Beispiel:</strong></p>
<p>Definition der Ereignisse:
<span class="math display">\[
\begin{eqnarray*}
A &amp;\text{:}&amp;\text{ Die Firma hat Erfolg} \\
B &amp;\text{:}&amp;\text{ Der Manager strengt sich an}
\end{eqnarray*}
\]</span>
Gesucht sei die bedingte Wahrscheinlichkeit <span class="math inline">\(P(B|A)=P(A\cap B)/P(A)\)</span>.
Für die Auszahlung von Boni an einen Manager ist es wichtig zu wissen,
ob er eine gute Leistung erbracht hat. Wir wollen bestimmen, wie hoch die
Wahrscheinlichkeit ist, dass der Manager sich tatsächlich für den Erfolg
angestrengt hat, wenn wir beobachten, dass die Firma erfolgreich ist. Oder
anders gesagt, ist der Erfolg der Firma wirklich ein Indikator für eine
gute Leistung?</p>
<p>Wir nehmen an (auch wenn es in der Praxis unrealistisch ist, diese
Werte zu kennen):
<span class="math display">\[
\begin{eqnarray*}
P\left( A|B\right) &amp;=&amp;0.8 \\
P(A|\bar{B}) &amp;=&amp;0.5 \\
P\left( B\right) &amp;=&amp;0.6
\end{eqnarray*}
\]</span>
Hier handelt es sich um eine Fragestellung, die auch mit einer Vier-Felder-Tafel
beantwortet werden könnte. Trotzdem nutzen wir nun einen Wahrscheinlichkeitsbaum.
Auf der ersten Ebene verzweigt der Baum in die beiden Ereignisse
<span class="math inline">\(B\)</span> und <span class="math inline">\(\bar B\)</span>, denn wir kennen bedingte Wahrscheinlichkeiten, die diese
beiden Ereignisse als Bedingung haben. An die beiden Zweige schreiben wir
die Ereignisse und ihre Wahrscheinlichkeiten. Die Wahrscheinlichkeit von 0.4
für das Ereignis <span class="math inline">\(\bar B\)</span> ergibt sich aus der Gegenwahrscheinlichkeit von <span class="math inline">\(B\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>,t<span class="op">=</span><span class="st">"n"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,axes<span class="op">=</span><span class="cn">F</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0.4</span>,<span class="fl">0.4</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>,<span class="fl">0.2</span>,<span class="fl">0.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.4</span>,<span class="fl">0</span>,<span class="fl">0.4</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>,<span class="fl">0.5</span>,<span class="fl">0.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">0.2</span>,<span class="fl">0.7</span>,<span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">0.35</span>,<span class="fl">0.83</span>,<span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">0.35</span>,<span class="fl">0.17</span>,<span class="fl">0.4</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span><span class="fl">0.2</span>,<span class="fl">0.29</span>,<span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">bar</span><span class="op">(</span><span class="va">B</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="02-wahrscheinlichkeit_files/figure-html/unnamed-chunk-3-1.png" width="672">
Auf der zweiten Stufe werden nun die bedingten Ereignisse und Wahrscheinlichkeiten
ergänzt, einmal unter der Bedingung, dass <span class="math inline">\(B\)</span> eingetreten ist, einmal unter
der Bedingung, dass <span class="math inline">\(\bar B\)</span> eingetreten ist. An die neuen Zweige werden
die zugehörigen bedingten Wahrscheinlichkeiten geschrieben. Dabei nutzen wir
wiederum die Gegenwahrscheinlichkeiten der bedingten Ereignisse.
<img src="02-wahrscheinlichkeit_files/figure-html/unnamed-chunk-4-1.png" width="672">
Die Wahrscheinlichkeiten für <span class="math inline">\(A\cap B\)</span>, <span class="math inline">\(A\cap \bar B\)</span>, <span class="math inline">\(\bar A\cap B\)</span> und
<span class="math inline">\(\bar A\cap\bar b\)</span> ergeben sich nun, indem man entlang der Äste die
Wahrscheinlichkeiten multipliziert. Man erhält
<span class="math display">\[
\begin{align*}
P(A\cap B)&amp;=0.6\cdot 0.8 = 0.48\\
P(\bar A\cap B)&amp;=0.6\cdot 0.2 = 0.12\\
P(A\cap \bar B)&amp;=0.4\cdot 0.5 = 0.2\\
P(\bar A\cap \bar B)&amp;=0.4\cdot 0.5 = 0.2
\end{align*}
\]</span>
Die (unbedingten) Wahrscheinlichkeiten <span class="math inline">\(P(A)\)</span> und <span class="math inline">\(P(\bar A)\)</span> ergeben sich,
indem man die Wahrscheinlichkeiten aller Pfade aufaddiert, die einen <span class="math inline">\(A\)</span>-Ast
(bzw. einen <span class="math inline">\(\bar A\)</span>-Ast) haben. Es ergibt sich <span class="math inline">\(P(A)=0.68\)</span> und
<span class="math inline">\(P(\bar A)=0.32\)</span>.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="zufall.html"><span class="header-section-number">1</span> Zufallsvorgänge</a></div>
<div class="next"><a href="zufallsvariablen.html"><span class="header-section-number">3</span> Zufallsvariablen</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="In diesem Kapitel:"><h2>Kapitelinhalt:</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#wahrscheinlichkeit"><span class="header-section-number">2</span> Wahrscheinlichkeit</a></li>
<li><a class="nav-link" href="#def:wahrscheinlichkeit"><span class="header-section-number">2.1</span> Definition</a></li>
<li><a class="nav-link" href="#venn"><span class="header-section-number">2.2</span> Venn-Diagramme</a></li>
<li><a class="nav-link" href="#laplace"><span class="header-section-number">2.3</span> Laplace-Experimente</a></li>
<li><a class="nav-link" href="#bedingt"><span class="header-section-number">2.4</span> Bedingte Wahrscheinlichkeit</a></li>
<li><a class="nav-link" href="#total"><span class="header-section-number">2.5</span> Totale Wahrscheinlichkeit</a></li>
<li><a class="nav-link" href="#bayes"><span class="header-section-number">2.6</span> Satz von Bayes</a></li>
<li><a class="nav-link" href="#unabh%C3%A4ngigkeit"><span class="header-section-number">2.7</span> Unabhängigkeit</a></li>
<li><a class="nav-link" href="#vier-felder-tafeln"><span class="header-section-number">2.8</span> Vier-Felder-Tafeln</a></li>
<li><a class="nav-link" href="#wahrscheinlichkeitsb%C3%A4ume"><span class="header-section-number">2.9</span> Wahrscheinlichkeitsbäume</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/rstudio/bookdown-demo/blob/master/02-wahrscheinlichkeit.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/rstudio/bookdown-demo/edit/master/02-wahrscheinlichkeit.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Science 2: Statistische Inferenz</strong>" wurde von Mark Trede geschrieben. Diese Version ist vom 2023-05-25.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Dieses Buch wurde mit dem R-Paket <a class="text-light" href="https://bookdown.org">bookdown</a> verfasst.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
