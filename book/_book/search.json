[{"path":"index.html","id":"einleitung","chapter":"Einleitung","heading":"Einleitung","text":"\nden Wirtschaftswissenschaften spielen Zufall und Unsicherheit eine\nwichtige Rolle. Zum einen, weil die ökonomische Theorie deskriptive\nund normative Aussagen darüber macht, wie ökonomische Akteure sich\nunter Unsicherheit verhalten und wie sie sich rational verhalten\nsollten. Zum anderen, weil ökonomische Modelle mit Hilfe von\nstatistischen Verfahren die Realität angepasst werden sollen\noder ökonomische Theorien anhand von emprischen Beobachtungen\nüberprüft werden sollen. dem Modul Data Science 2 lernen Sie,\nwie man mit Zufall und Unsicherheit umgeht.Das Modul lässt sich zwei große Teile gliedern, nämlich die\nGrundlagen der Wahrscheinlichkeitstheorie und die statistische\nInferenz.","code":""},{"path":"index.html","id":"wahrscheinlichkeitstheorie","chapter":"Einleitung","heading":"Wahrscheinlichkeitstheorie","text":"xxx","code":""},{"path":"index.html","id":"statistische-inferenz","chapter":"Einleitung","heading":"Statistische Inferenz","text":"xxxDer Name dieses Moduls “Data Science 2” lässt einen vermuten, dass\ndieses Modul auf dem Modul “Data Science 1” aufbaut. Diese Vermutung\nist aber falsch. Beiden Module sind inhaltlich sich abgeschlossen und\nkönnen unabhängig voneinander belegt werden.Gegensatz zu “Data Science 1” wird die Software R diesem Modul\nnur sehr sparsam und erst ab Mitte des Semesters eingesetzt. R dient\nhauptsächlich dazu, die Verfahren zu illustrieren.","code":""},{"path":"zufall.html","id":"zufall","chapter":"Kapitel 1 Zufallsvorgänge","heading":"Kapitel 1 Zufallsvorgänge","text":"diesem Kurs geht es um Zufall. Die Frage, man denn unter Zufall eigentlich\nzu verstehen hat, ist schwierig zu beantworten. Sie gehört eher den Bereich der\nPhilosophie als den der Wirtschaftswissenschaften. Wir definieren diesem Kurs\nZufall auf eine sehr pragmatische Art: Wir haben es mit einem Zufallsvorgang\nzu tun, wenn wir nicht genau wissen, passieren wird, aber wissen, alles\npassieren könnte.Mit Zufallsvorgängen hat man es also unentwegt zu tun. Wir wissen nicht, wie\nsich der DAX-Index im Laufe des nächsten Monats entwickeln wird, aber wir\nwissen, dass er einen Wert zwischen 0 und unendlich haben wird. Die\nEntwicklung des DAX-Index ist also im Sinne unserer Definition zufällig.\nWir wissen nicht, ob eine bestimmte Person im nächsten Jahr arbeitslos sein\nwird, aber wissen, dass sie es sein könnte oder auch nicht. Der\nArbeitslosigkeitsstatus dieser Person ist also ebenfalls zufällig im\nSinne unserer Defintion.den Wirtschaftswissenschaften betrachten wir oft Zufallsvorgänge, die\nnicht unter identischen Bedingungen wiederholt werden können. \ndiesem Kurs werden wir aber zumindest Beginn aus didaktischen\nGründen oft Zufallsvorgänge betrachten, die nichts mit wirtschaftlichen\nFragestellungen zu tun haben. Als Beispiele dienen oft Münzwürfe\noder Würfelwürfe. Solche Zufallsvorgänge, die sich unter praktisch\nidentischen Bedingungen wiederholen lassen, nennt man auch\nZufallsexperimente.Damit wir einen Zufallsvorgang präzise beschreiben können und vernünftig\nmit ihm arbeiten können, brauchen wir einen formalen Rahmen und\neine geeignete Terminologie.\nDie Wahrscheinlichkeitstheorie bietet einen solchen Rahmen. \nden folgenden Abschnitten lernen wir ihn kennen.","code":""},{"path":"zufall.html","id":"ergebnisraum","chapter":"Kapitel 1 Zufallsvorgänge","heading":"1.1 Ergebnisraum","text":"Um einen Zufallsvorgang formal zu erfassen, schreiben wir alle\nmöglichen Ergebnisse des Zufallsvorgangs eine Menge. Dabei spielt\nes keine Rolle, ob der Eintritt eines Ergebnisses als wahrscheinlich\noder unwahrscheinlich angesehen wird. Alle Ergebnisse, die eintreten\nkönnen, werden die Menge aufgenommen.Die Menge aller Ergebnisse nennen wir den Ergebnisraum.\nDer Ergebnisraum wird mit dem Symbol \\(\\Omega\\) (großes Omega) bezeichnet.\nDie Elemente des Ergebnisraums werden allgemein als\n\\(\\omega_1,\\omega_2,\\ldots\\) (kleine Omegas) notiert.Beispiele:Beim Werfen einer Münze kann es zu den beiden Ergebnissen “Kopf” (K)\noder “Zahl” (Z) kommen. Die Ergebnismenge hat also zwei Elemente:\n\\(\\Omega=\\{K,Z\\}\\).Beim Werfen einer Münze kann es zu den beiden Ergebnissen “Kopf” (K)\noder “Zahl” (Z) kommen. Die Ergebnismenge hat also zwei Elemente:\n\\(\\Omega=\\{K,Z\\}\\).Wenn die Münze zweimal geworfen wird (oder wenn zwei Münzen\ngeworfen werden), dann lautet die Ergebnismenge \\(\\Omega=\\{KK,KZ,ZK,ZZ\\}\\).Wenn die Münze zweimal geworfen wird (oder wenn zwei Münzen\ngeworfen werden), dann lautet die Ergebnismenge \\(\\Omega=\\{KK,KZ,ZK,ZZ\\}\\).Beim Werfen eines Würfels kann jede Augenzahl von 1 bis 6 auftreten,\ndie Ergebnismenge ist also \\(\\Omega=\\{1,2,3,4,5,6\\}\\).Beim Werfen eines Würfels kann jede Augenzahl von 1 bis 6 auftreten,\ndie Ergebnismenge ist also \\(\\Omega=\\{1,2,3,4,5,6\\}\\).Der DAX-Index von morgen kann Werte zwischen 0 und unendlich annehmen.\nDie Ergebnismenge ist diesem Fall \\(\\Omega=\\mathbb{R}^+_0\\) (die\nMenge der nicht-negativen reellen Zahlen).Der DAX-Index von morgen kann Werte zwischen 0 und unendlich annehmen.\nDie Ergebnismenge ist diesem Fall \\(\\Omega=\\mathbb{R}^+_0\\) (die\nMenge der nicht-negativen reellen Zahlen).","code":""},{"path":"zufall.html","id":"ereignisse","chapter":"Kapitel 1 Zufallsvorgänge","heading":"1.2 Ereignisse","text":"Aussagen über Wahrscheinlichkeiten beziehen sich nicht immer nur auf\neinzelne Ergebnisse aus der Ergebnismenge, sondern oft auf mehrere\nErgebnisse. Zusammenfassungen von Ergebnissen nennt man Ereignisse\n(engl. events). Sie werden meist mit großen lateinischen Buchstaben\nbezeichnet (\\(, B, \\ldots\\)).Teilmengen der Ergebnismenge \\(\\Omega\\) heißen Ereignisse.\nEin Ereignis \\(\\) tritt ein, wenn das Ergebnis \\(\\omega\\) des\nZufallsvorgangs Element des Ereignisses ist, d.h.\nwenn \\(\\omega\\\\).Zu den Ereignissen gehören auch die leere Menge \\(\\emptyset\\) und\n\\(\\Omega\\) selber. Das Ereignis \\(\\emptyset\\) nennt man das\nunmögliche Ereignis, weil es nie eintreten kann. Das Ereignis \\(\\Omega\\)\nnennt man das sichere Ereignis, weil es mit Sicherheit eintritt. Wenn ein\nEreignis nur ein einziges Ergebnis enthält (z.B. \\(=\\{\\omega\\}\\)),\ndann spricht man von einem Elementarereignis.Weil Ereignisse Mengen sind, lassen sie sich nach den normalen\nRegeln der Mengenlehre verknüpfen:Vereinigungsmenge: Das Ereignis \\(\\cup B\\) tritt ein, wenn \\(\\)\noder \\(B\\) eintritt (oder beide).Vereinigungsmenge: Das Ereignis \\(\\cup B\\) tritt ein, wenn \\(\\)\noder \\(B\\) eintritt (oder beide).Schnittmenge: Das Ereignis \\(\\cap B\\) tritt ein, wenn \\(\\) und\n\\(B\\) eintreten.Schnittmenge: Das Ereignis \\(\\cap B\\) tritt ein, wenn \\(\\) und\n\\(B\\) eintreten.Komplementärmenge: Das Ereignis \\(\\bar \\) tritt ein, wenn \\(\\)\nnicht eintritt.Komplementärmenge: Das Ereignis \\(\\bar \\) tritt ein, wenn \\(\\)\nnicht eintritt.Zwei Ereignisse heißen disjunkt (oder unvereinbar), wenn sie\nnicht gemeinsam eintreten können, d.h. wenn \\(\\cap B=\\emptyset\\) ist.Bei einem Zufallsvorgang tritt immer nur ein Ergebnis ein, da aber dieses\nErgebnis Element mehrerer Ereignisse sein kann, können durchaus mehrere\nEreignisse eintreten. Die Unterscheidung zwischen Ergebnis und Ereignis\nmag zunächst etwas haarspalterisch erscheinen, sie ist aber wichtig und\nnützlich, denn sie erleichtert später den Umgang mit Wahrscheinlichkeiten.\nÜber Wahrscheinlichkeiten wurde bisher noch nichts gesagt, das folgt\ndirekt im nächsten Kapitel.Die folgenden Rechenregeln für Mengen sind oft nützlich, wenn man\nmit Ereignissen arbeitet:Distributivgesetz:\n\\[\n\\begin{align*}\n\\cup (B\\cap C)&=(\\cup B)\\cap (\\cup C)\\\\\n\\cap (B\\cup C)&=(\\cap B)\\cup (\\cap C).\n\\end{align*}\n\\]Regel von de Morgan:\n\\[\n\\begin{align*}\n\\overline{\\cup B} &= \\overline{}\\cap \\overline{B}\\\\\n\\overline{\\cap B} &= \\overline{}\\cup \\overline{B}.\n\\end{align*}\n\\]Beispiele:Wir betrachten einen Würfelwurf mit der Ergebnismenge\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\) und definieren die folgenden drei\nEreignisse:\n\\[\n\\begin{align*}\n&= \\{2,4,6\\}\\\\\nB &= \\{1,2,3\\}\\\\\nC &= \\{2\\}\n\\end{align*}\n\\]\nDie Ereignisse lassen sich auch Worten ausdrücken: \\(\\) ist das\nEreignis “Eine gerade Zahl wird gewürfelt”, \\(B\\) ist “Eine Zahl kleiner\nals 4 wird gewürfelt” und \\(C\\) ist “Eine 2 wird gewürfelt”. Wenn bei dem\nWurf nun eine 6 gewürfelt wird, dann tritt \\(\\) ein, \\(B\\) und \\(C\\) dagegen\nnicht. Wenn eine 2 auftritt, dann treten alle drei Ereignisse ein.Wir betrachten einen Würfelwurf mit der Ergebnismenge\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\) und definieren die folgenden drei\nEreignisse:\n\\[\n\\begin{align*}\n&= \\{2,4,6\\}\\\\\nB &= \\{1,2,3\\}\\\\\nC &= \\{2\\}\n\\end{align*}\n\\]\nDie Ereignisse lassen sich auch Worten ausdrücken: \\(\\) ist das\nEreignis “Eine gerade Zahl wird gewürfelt”, \\(B\\) ist “Eine Zahl kleiner\nals 4 wird gewürfelt” und \\(C\\) ist “Eine 2 wird gewürfelt”. Wenn bei dem\nWurf nun eine 6 gewürfelt wird, dann tritt \\(\\) ein, \\(B\\) und \\(C\\) dagegen\nnicht. Wenn eine 2 auftritt, dann treten alle drei Ereignisse ein.Nun werden zwei Würfel geworfen. Die Ergebnismenge ist\n\\[\n\\begin{align*}\n\\Omega=\\{&11,12,13,14,15,16,\\\\\n&21,22,23,24,25,26,\\\\\n&31,32,33,34,35,36,\\\\\n&41,42,43,44,45,46,\\\\\n&51,52,53,54,55,56,\\\\\n&61,62,63,64,65,66\\}\n\\end{align*}\n\\]\nWir betrachten die drei Ereignisse\n\\[\n\\begin{align*}\n&= \\{11,12,13,14,15,16\\}\\\\\nB &= \\{26,35,44,53,62\\}\\\\\nC &= \\{11,22,33,44,55,66\\}\n\\end{align*}\n\\]\nWorten bedeutet \\(\\): “Der erste Würfel zeigt eine 1”,\n\\(B\\): “Die Augensumme beträgt 8” und \\(C\\): “Es liegt ein Pasch vor” (d.h. auf\nbeiden Würfel ist die gleiche Augenzahl zu sehen). Wenn nach dem Wurf das\nErgebnis “44” auftritt, dann treten \\(B\\) und \\(C\\) ein, aber \\(\\) nicht. Bei\ndem Ergebnis “25” tritt keins der drei Ereignisse ein. Bei “15” tritt nur\n\\(\\) ein.Nun werden zwei Würfel geworfen. Die Ergebnismenge ist\n\\[\n\\begin{align*}\n\\Omega=\\{&11,12,13,14,15,16,\\\\\n&21,22,23,24,25,26,\\\\\n&31,32,33,34,35,36,\\\\\n&41,42,43,44,45,46,\\\\\n&51,52,53,54,55,56,\\\\\n&61,62,63,64,65,66\\}\n\\end{align*}\n\\]\nWir betrachten die drei Ereignisse\n\\[\n\\begin{align*}\n&= \\{11,12,13,14,15,16\\}\\\\\nB &= \\{26,35,44,53,62\\}\\\\\nC &= \\{11,22,33,44,55,66\\}\n\\end{align*}\n\\]\nWorten bedeutet \\(\\): “Der erste Würfel zeigt eine 1”,\n\\(B\\): “Die Augensumme beträgt 8” und \\(C\\): “Es liegt ein Pasch vor” (d.h. auf\nbeiden Würfel ist die gleiche Augenzahl zu sehen). Wenn nach dem Wurf das\nErgebnis “44” auftritt, dann treten \\(B\\) und \\(C\\) ein, aber \\(\\) nicht. Bei\ndem Ergebnis “25” tritt keins der drei Ereignisse ein. Bei “15” tritt nur\n\\(\\) ein.Wir betrachten die Tagesrendite einer Aktie mit der Ergebnismenge\n\\(\\Omega=[-100, \\infty]\\). Außerdem definieren wir die Ereignisse\n\\[\n\\begin{align*}\n&= [-100, -20]\\\\\nB &= [-2,2]\\\\\nC &= [-100,-10]\\cup [10,\\infty].\n\\end{align*}\n\\]\nWorten könnte man diese Ereignisse ausdrücken: \\(\\): “Ein katastrophaler\nAbsturz der Aktie”, \\(B\\): “Eine normale Tagesrendite” und \\(C\\): “Eine sehr\nstarke Schwankung”. Wenn die realisierte Tagesrendite dann 1 Prozent ist,\ntritt nur \\(B\\) ein. Wenn der Kurs um 25 Prozent fällt, treten \\(\\) und \\(C\\)\nauf.Wir betrachten die Tagesrendite einer Aktie mit der Ergebnismenge\n\\(\\Omega=[-100, \\infty]\\). Außerdem definieren wir die Ereignisse\n\\[\n\\begin{align*}\n&= [-100, -20]\\\\\nB &= [-2,2]\\\\\nC &= [-100,-10]\\cup [10,\\infty].\n\\end{align*}\n\\]\nWorten könnte man diese Ereignisse ausdrücken: \\(\\): “Ein katastrophaler\nAbsturz der Aktie”, \\(B\\): “Eine normale Tagesrendite” und \\(C\\): “Eine sehr\nstarke Schwankung”. Wenn die realisierte Tagesrendite dann 1 Prozent ist,\ntritt nur \\(B\\) ein. Wenn der Kurs um 25 Prozent fällt, treten \\(\\) und \\(C\\)\nauf.Die Kardinalität der Ergebnismenge bezeichnen wir mit \\(|\\Omega|\\). Wenn\n\\(\\Omega\\) nur endlich viele Elemente hat, ist die Kardinalität schlicht und\neinfach die Anzahl der Elemente. Es ist aber auch möglich (und vielen\nökonomischen Anwendungen auch üblich), dass die Ergebnismenge unendlich\nviele Elemente enthält. solchen Fällen kann es zu einigen\nmathematisch-technischen Schwierigkeiten kommen, die wir diesem Kurs\naber einfach ignorieren. Eine sorgfältigere Behandlung findet dem\nWahlpflichtmodul “Advanced Statistics” statt.","code":""},{"path":"wahrscheinlichkeit.html","id":"wahrscheinlichkeit","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"Kapitel 2 Wahrscheinlichkeit","text":"","code":""},{"path":"wahrscheinlichkeit.html","id":"def:wahrscheinlichkeit","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.1 Definition","text":"Manche Ereignisse sind weniger wahrscheinlich als andere. Wie hoch\ndie Wahrscheinlichkeit eines Ereignisses ist, wird durch eine\nAbbildung (Funktion) angegeben.Eine Abbildung \\(P: \\mapsto P()\\)\nheißt Wahrscheinlichkeitsmaß, wenn giltNichtnegativität: \\(P()\\ge 0\\) für alle Ereignisse \\(\\)Nichtnegativität: \\(P()\\ge 0\\) für alle Ereignisse \\(\\)Normierung: \\(P(\\Omega)=1\\)Normierung: \\(P(\\Omega)=1\\)Additivität: Für disjunkte Ereignisse \\(\\) und \\(B\\)\n(d.h. \\(\\cap B=\\emptyset\\)) ist \\(P(\\cup B)=P()+P(B)\\)Additivität: Für disjunkte Ereignisse \\(\\) und \\(B\\)\n(d.h. \\(\\cap B=\\emptyset\\)) ist \\(P(\\cup B)=P()+P(B)\\)Diese Eigenschaften, die eine Wahrscheinlichkeitsabbildung erfüllen\nmuss, sind intuitiv sinnvoll und können nicht aus irgendwelchen Fakten\nhergeleitet werden, daher spricht man auch von Axiomen. Etwas allgemeiner\nund präziser wurden diese Axiome 1933 von Andrey Kolmogorov\nfür eine saubere mathematische Fundierung der Wahrscheinlichkeitstheorie\neingeführt.Aus diesen Axiomen lassen sich einige Rechenregeln ableiten, zum BeispielMonotonität: Wenn \\(\\subseteq B\\), dann ist \\(P()\\le P(B)\\)Monotonität: Wenn \\(\\subseteq B\\), dann ist \\(P()\\le P(B)\\)Komplementärereignis: \\(P(\\bar )=1-P()\\)Komplementärereignis: \\(P(\\bar )=1-P()\\)Additionssatz: \\(P(\\cup B)=P()+P(B)-P(\\cap B)\\)Additionssatz: \\(P(\\cup B)=P()+P(B)-P(\\cap B)\\)","code":""},{"path":"wahrscheinlichkeit.html","id":"venn","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.2 Venn-Diagramme","text":"Ereignisse lassen sich - wie andere Mengen der Mengenlehre auch - \nForm von Venn-Diagrammen darstellen. Wenn die Ergebnismenge \\(\\Omega\\)\ndurch ein Rechteck repräsentiert wird, dann kann man Ereignisse\nals Teilmengen des Rechtecks darstellen. dem folgenden Bild ist zum\nBeispiel das Ereignis \\(\\) der kleine Kreis der Mitte.\ndiesem Venn-Diagramm wird deutlich, dass die Ereignisse \\(\\) und \\(C\\)\nnicht gleichzeitig eintreten können, \\(\\) und \\(B\\) hingegen schon, denn\nihre Schnittmenge ist nicht leer. Außerdem\nerkennt man, dass \\(\\) eine kleinere Wahrscheinlichkeit hat als \\(B\\) und \\(C\\).\nDer Additionssatz \\(P(\\cup B)=P()+P(B)-P(\\cap B)\\)\nwird dieser grafischen Sichtweise sehr einfach klar:\nDie Wahrscheinlichkeit von \\(\\cup B\\) (also \\(\\) oder \\(B\\)) ergibt sich als\ndie Summe aus den beiden Kreisflächen \\(\\) und \\(B\\) abzüglich der Fläche\nder Schnittmenge, die sonst doppelt gezählt werden würde.","code":""},{"path":"wahrscheinlichkeit.html","id":"laplace","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.3 Laplace-Experimente","text":"Eine besonders einfache Art von Zufallsvorgängen sind die sogenannten\nLaplace-Experimente.Ein Zufallsvorgang heißt Laplace-Experiment, wenn es nur endlich viele\nErgebnisse gibt (d.h. wenn \\(|\\Omega|=n\\)) und wenn alle\nElementarereignisse als gleich wahrscheinlich angenommen werden können.Bei einem Laplace-Experiment ist die Wahrscheinlichkeit, dass ein\nErgebnis \\(\\) eintritt leicht zu ermitteln. Sie beträgt\n\\[\nP()=\\frac{||}{|\\Omega|},\n\\]\nalso die Anzahl der Ergebnisse \\(\\) dividiert durch die Anzahl aller\nErgebnisse. Um Aussagen über Wahrscheinlichkeiten zu treffen, muss man\nalso abzählen, wie viele Ergebnisse den Mengen sind. Das ist manchmal\nsehr einfach, kann aber bei großen Mengen kompliziert sein. solchen\nFällen hilft der Teilbereich der Mathematik weiter, den man “Kombinatorik”\nnennt. Wir gehen diesem Kurs jedoch nicht näher auf\nkombinatorische Probleme ein, sondern beschränken uns auf Mengen, bei\ndenen es einfach ist, die Anzahl ihrer Elemente zu bestimmen.Beispiele:Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.\nDie Anzahl der Ergebnisse \\(\\Omega\\) beträgt 6. Sei \\(\\) das\nEreignis “Eine gerade Zahl wird geworfen”, also \\(=\\{2,4,6\\}\\).\nDann ist\n\\[\n\\begin{align*}\nP()&=\\frac{||}{|\\Omega|}\\\\\n&=\\frac{3}{6}\\\\\n&=0.5.\n\\end{align*}\n\\]Ein Würfel wird geworfen. Es handelt sich um ein Laplace-Experiment.\nDie Anzahl der Ergebnisse \\(\\Omega\\) beträgt 6. Sei \\(\\) das\nEreignis “Eine gerade Zahl wird geworfen”, also \\(=\\{2,4,6\\}\\).\nDann ist\n\\[\n\\begin{align*}\nP()&=\\frac{||}{|\\Omega|}\\\\\n&=\\frac{3}{6}\\\\\n&=0.5.\n\\end{align*}\n\\]Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine Rolle,\nwie man die Ergebnismenge festlegt. Wir wählen\n\\[\n\\begin{align*}\n\\Omega=\\{&11,12,13,14,15,16,\\\\\n&21,22,23,24,25,26,\\\\\n&31,32,33,34,35,36,\\\\\n&41,42,43,44,45,46,\\\\\n&51,52,53,54,55,56,\\\\\n&61,62,63,64,65,66\\}\n\\end{align*}\n\\]\nweil es sich dann um ein Laplace-Experiment handelt. Die Ergebnisse\n“24” und “42” sind zwar nicht unterscheidbar, wenn die Würfel gleich\naussehen, aber wir können den zuerst geworfenen Würfel (oder den\nweiter links liegenden) als ersten Würfel bezeichnen. Dass alle 36\nErgebnisse gleich wahrscheinlich sind, können wir mit unserem\nAlltagswissen begründen, nicht aus der Mathematik heraus.\nSei \\(\\) das Ereignis “Die Augenzahlen der beiden Würfel unterscheiden\nsich um 2”, d.h. \\(=\\{13,24,31,35,42,46,53,64\\}\\). Dann gilt\n\\[\n\\begin{align*}\nP()&=\\frac{||}{|\\Omega|}\\\\\n&=\\frac{8}{36}\\\\\n&=\\frac{2}{9}\\\\\n&\\approx 0.2222.\n\\end{align*}\n\\]\nAls Ergebnismenge wäre auch\n\\[\n\\begin{align*}\n\\Omega=\\{\n&11,12,13,14,15,16,\\\\\n&22,23,24,25,26,\\\\\n&33,34,35,36,\\\\\n&44,45,46,\\\\\n&55,56,\\\\\n&66\\}\n\\end{align*}\n\\]\nmöglich gewesen, aber dann wäre die Annahme eines Laplace-Experiments\nnicht plausibel gewesen. Die Elementarereignisse “11” und “12” wären\nbeispielsweise nicht gleich wahrscheinlich (ein Pasch tritt\nerfahrungsgemäß seltener auf).\nDie Wahl der Ergebnismenge sollte immer erfolgen, dass das weitere\nVorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge\ngewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte\nman das tun.Zwei gleich aussehende Würfel werden geworfen. Nun spielt es eine Rolle,\nwie man die Ergebnismenge festlegt. Wir wählen\n\\[\n\\begin{align*}\n\\Omega=\\{&11,12,13,14,15,16,\\\\\n&21,22,23,24,25,26,\\\\\n&31,32,33,34,35,36,\\\\\n&41,42,43,44,45,46,\\\\\n&51,52,53,54,55,56,\\\\\n&61,62,63,64,65,66\\}\n\\end{align*}\n\\]\nweil es sich dann um ein Laplace-Experiment handelt. Die Ergebnisse\n“24” und “42” sind zwar nicht unterscheidbar, wenn die Würfel gleich\naussehen, aber wir können den zuerst geworfenen Würfel (oder den\nweiter links liegenden) als ersten Würfel bezeichnen. Dass alle 36\nErgebnisse gleich wahrscheinlich sind, können wir mit unserem\nAlltagswissen begründen, nicht aus der Mathematik heraus.Sei \\(\\) das Ereignis “Die Augenzahlen der beiden Würfel unterscheiden\nsich um 2”, d.h. \\(=\\{13,24,31,35,42,46,53,64\\}\\). Dann gilt\n\\[\n\\begin{align*}\nP()&=\\frac{||}{|\\Omega|}\\\\\n&=\\frac{8}{36}\\\\\n&=\\frac{2}{9}\\\\\n&\\approx 0.2222.\n\\end{align*}\n\\]\nAls Ergebnismenge wäre auch\n\\[\n\\begin{align*}\n\\Omega=\\{\n&11,12,13,14,15,16,\\\\\n&22,23,24,25,26,\\\\\n&33,34,35,36,\\\\\n&44,45,46,\\\\\n&55,56,\\\\\n&66\\}\n\\end{align*}\n\\]\nmöglich gewesen, aber dann wäre die Annahme eines Laplace-Experiments\nnicht plausibel gewesen. Die Elementarereignisse “11” und “12” wären\nbeispielsweise nicht gleich wahrscheinlich (ein Pasch tritt\nerfahrungsgemäß seltener auf).Die Wahl der Ergebnismenge sollte immer erfolgen, dass das weitere\nVorgehen möglichst einfach und elegant ist. Wenn die Ergebnismenge\ngewählt werden kann, dass ein Laplace-Experiment vorliegt, sollte\nman das tun.","code":""},{"path":"wahrscheinlichkeit.html","id":"bedingt","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.4 Bedingte Wahrscheinlichkeit","text":"Manchmal gibt es begrenzte Informationen über einen Zufallsvorgang.\nDann kennt man nicht das realisierte Ergebnis, kann aber die Menge der\nmöglichen Ergebnisse eingrenzen. Dadurch ändern sich die\nWahrscheinlichkeiten für Ereignisse.Wir betrachten zwei Ereignsse \\(\\) und \\(B\\) mit \\(P(B)>0\\).\nDann heißt\n\\[\nP(|B)=\\frac{P(\\cap B)}{P(B)}\n\\]\ndie bedingte Wahrscheinlichkeit von \\(\\) gegeben \\(B\\)Die Notation \\(|B\\) steht nicht für ein bestimmtes Ereignis, sondern\nzeigt , dass wir eine neue Art von Wahrscheinlichkeit betrachten,\nnämlich die bedingte Wahrscheinlichkeit. Beim Sprechen über\nWahrscheinlichkeiten ist es nicht immer einfach (aber wichtig!),\nzwischen der Wahrscheinlichkeit \\(P(\\cap B)\\) und der bedingten\nWahrscheinlichkeit \\(P(|B)\\) zu unterscheiden. Wenn man \\(P(\\cap B)\\)\nmeint, spricht man von der Wahrscheinlichkeit für \\(\\) und \\(B\\).\nWenn man \\(P(|B)\\) meint, sagt man \\(\\) gegeben \\(B\\), oder: \\(\\) wenn \\(B\\),\noder: \\(\\) unter der Bedingung \\(B\\). Wenn man ausdrücklich angeben\nmöchte, dass eine Wahrscheinlichkeit keine bedingte Wahrscheinlichkeit\nist, nennt man sie auch eine unbedingte Wahrscheinlichkeit.Beispiele:Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse\n\\(=\\{2,4,6\\}\\) (“gerade Zahl”), und \\(B=\\{4,5,6\\}\\) (“eine Zahl größer als 3”).\nDie bedingte Wahrscheinlichkeit von \\(\\) gegeben \\(B\\) beträgt\n\\[\n\\begin{align*}\nP(|B)&=\\frac{P(\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{4,5,6\\})}\\\\\n&=\\frac{2}{3}\n\\end{align*}\n\\]\nund die bedingte Wahrscheinlichkeit von \\(B\\) gegeben \\(\\) lautet\n\\[\n\\begin{align*}\nP(B|)&=\\frac{P(B\\cap )}{P()}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{2,4,6\\})}\\\\\n&=\\frac{2}{3}.\n\\end{align*}\n\\]Ein Würfel wird geworfen. Wir betrachten die beiden Ereignisse\n\\(=\\{2,4,6\\}\\) (“gerade Zahl”), und \\(B=\\{4,5,6\\}\\) (“eine Zahl größer als 3”).\nDie bedingte Wahrscheinlichkeit von \\(\\) gegeben \\(B\\) beträgt\n\\[\n\\begin{align*}\nP(|B)&=\\frac{P(\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{4,5,6\\})}\\\\\n&=\\frac{2}{3}\n\\end{align*}\n\\]\nund die bedingte Wahrscheinlichkeit von \\(B\\) gegeben \\(\\) lautet\n\\[\n\\begin{align*}\nP(B|)&=\\frac{P(B\\cap )}{P()}\\\\\n&=\\frac{P(\\{4,6\\})}{P(\\{2,4,6\\})}\\\\\n&=\\frac{2}{3}.\n\\end{align*}\n\\]Zwei Würfel werden geworfen. Sei \\(\\) das Ereignis “die Augensumme ist 10”.\nSei \\(B\\) das Ereignis “einer der beiden Würfel (oder beide) zeigt eine 2”.\nDann ist\n\\[\n\\begin{align*}\nP(|B)&=\\frac{P(\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n&=0.\n\\end{align*}\n\\]\nDie bedingte Wahrscheinlichkeit für das Ereignis \\(C\\): “Augensumme ist 5”\nunter der Bedingung \\(B\\) ist\n\\[\n\\begin{align*}\nP(C|B)&=\\frac{P(C\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{23,32\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n&=\\frac{2}{11}.\n\\end{align*}\n\\]Zwei Würfel werden geworfen. Sei \\(\\) das Ereignis “die Augensumme ist 10”.\nSei \\(B\\) das Ereignis “einer der beiden Würfel (oder beide) zeigt eine 2”.\nDann ist\n\\[\n\\begin{align*}\nP(|B)&=\\frac{P(\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n&=0.\n\\end{align*}\n\\]\nDie bedingte Wahrscheinlichkeit für das Ereignis \\(C\\): “Augensumme ist 5”\nunter der Bedingung \\(B\\) ist\n\\[\n\\begin{align*}\nP(C|B)&=\\frac{P(C\\cap B)}{P(B)}\\\\\n&=\\frac{P(\\{23,32\\})}{P(\\{21,22,23,24,25,26,12,32,42,52,62\\})}\\\\\n&=\\frac{2}{11}.\n\\end{align*}\n\\]Bei einer Umfrage werden Personen zufällig ausgewählt und befragt. Sei \\(\\)\ndas Ereignis “die befragte Person ist weiblich”. Sei \\(B\\) das Ereignis\n“die befragte Person arbeitet Teilzeit”. Zwischen der (unbedingten)\nWahrscheinlichkeit \\(P(B)\\) und der bedingten Wahrscheinlichkeit \\(P(B|)\\)\nbesteht ein Unterschied. Die Wahrscheinlichkeit \\(P(B)\\) steht dafür, dass\neine zufällig ausgewählte Person Teilzeit arbeitet, und diese Person\nkann männlich oder weiblich sein. Dagegen ist \\(P(B|)\\) die\nWahrscheinlichkeit, dass eine zufällig ausgewählte Frau Teilzeit\narbeitet.Bei einer Umfrage werden Personen zufällig ausgewählt und befragt. Sei \\(\\)\ndas Ereignis “die befragte Person ist weiblich”. Sei \\(B\\) das Ereignis\n“die befragte Person arbeitet Teilzeit”. Zwischen der (unbedingten)\nWahrscheinlichkeit \\(P(B)\\) und der bedingten Wahrscheinlichkeit \\(P(B|)\\)\nbesteht ein Unterschied. Die Wahrscheinlichkeit \\(P(B)\\) steht dafür, dass\neine zufällig ausgewählte Person Teilzeit arbeitet, und diese Person\nkann männlich oder weiblich sein. Dagegen ist \\(P(B|)\\) die\nWahrscheinlichkeit, dass eine zufällig ausgewählte Frau Teilzeit\narbeitet.den Medien werden häufig bedingte Wahrscheinlichkeiten berichtet,\nohne dass das explizit erwähnt wird. Oftmals sind die berichteten\nbedingten Wahrscheinlichkeiten gar nicht die, für die man sich\neigentlich interessiert, weil die Bedingung und das Bedingte\nquasi falsch herum angeordnet sind. Man interessiert sich für \\(P(|B)\\),\naber berichtet wird \\(P(B|)\\).Beispiele:den Medien wird berichtet, dass ein Corona-Schnelltest mit einer\nWahrscheinlichkeit von xxx Prozent ein falsch positives Ergebnis liefert.\nFalsch positiv bedeutet, die Wahrscheinlichkeit wird unter der Bedingung\nangegeben, dass die getestete Person negativ ist. Wenn man Test sich\ndurchführt, interessiert man sich dagegen für die bedingte\nWahrscheinlichkeit, infiziert zu sein, obwohl der Test negativ ausfällt,\nund natürlich auch für die bedingte Wahrscheinlichkeit, tatsächlich\ninfiziert zu sein, wenn der Test positiv ausfällt.den Medien wird berichtet, dass ein Corona-Schnelltest mit einer\nWahrscheinlichkeit von xxx Prozent ein falsch positives Ergebnis liefert.\nFalsch positiv bedeutet, die Wahrscheinlichkeit wird unter der Bedingung\nangegeben, dass die getestete Person negativ ist. Wenn man Test sich\ndurchführt, interessiert man sich dagegen für die bedingte\nWahrscheinlichkeit, infiziert zu sein, obwohl der Test negativ ausfällt,\nund natürlich auch für die bedingte Wahrscheinlichkeit, tatsächlich\ninfiziert zu sein, wenn der Test positiv ausfällt.den Medien erfährt man, dass XX Prozent der Studierenden ein\n“akademisches Elternhaus” haben. Es handelt sich um die bedingte\nWahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person\nstudiert. Für die Bewertung der Chancengleichheit ist hingegen die\nbedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert, wenn\nes aus einem akademischen Elternhaus stammt - und zwar im Vergleich\nzu der bedingten Wahrscheinlichkeit, dass ein Kind studiert, wenn\nes aus einem nicht-akademischen Elternhaus stammt.den Medien erfährt man, dass XX Prozent der Studierenden ein\n“akademisches Elternhaus” haben. Es handelt sich um die bedingte\nWahrscheinlichkeit eines akademischen Elternhauses, wenn eine Person\nstudiert. Für die Bewertung der Chancengleichheit ist hingegen die\nbedingte Wahrscheinlichkeit interessanter, dass ein Kind studiert, wenn\nes aus einem akademischen Elternhaus stammt - und zwar im Vergleich\nzu der bedingten Wahrscheinlichkeit, dass ein Kind studiert, wenn\nes aus einem nicht-akademischen Elternhaus stammt.Wie kann man eine bedingte Wahrscheinlichkeit “umdrehen”? Wie verhält sich\n\\(P(B|)\\) zu \\(P(|B)\\)? Das sehen wir im folgenden Abschnitt 2.6.Bedingte Wahrscheinlichkeiten sind auch nützlich, um die Wahrscheinlichkeit\nzu berechnen, dass mehrere Ereignisse gemeinsam passieren. Dazu formen wir\ndie Definition der bedingten Wahrscheinlichkeit einfach um. Aus\n\\[\nP(|B)=\\frac{P(\\cap B)}{P(B)}\n\\]\nwird\n\\[\nP(\\cap B)={P(B)P(|B)}\n\\]Beispiele:EinsEinsZweiZweiDas lässt sich auf mehr als zwei Ereignisse erweitern:\n\\[\n\\begin{align*}\nP(\\cap B \\cap C)&=P(C)P(B|C)P(|B\\cap C)\\\\\nP(\\cap B\\cap C\\cap D)&=P(D)P(C|D)P(B|C\\cap D)P(|B\\cap C\\cap D)\n\\end{align*}\n\\]\nDiese Formeln sind bei näherer Betrachtung intuitiv: Die Wahrscheinlichkeit,\ndass \\(\\), \\(B\\) und \\(C\\) gemeinsam eintreten, ergibt sich, indem man zuerst\neine der unbedingten Wahrscheinlichkeiten nimmt (z.B. \\(P(C)\\)).\nNun ist \\(C\\) quasi eingetreten und wir arbeiten unter der Bedingung \\(C\\)\nweiter. Die Wahrscheinlichkeit \\(P(C)\\) wird jetzt mit der\nbedingten Wahrscheinlichkeit des nächsten Ereignisses multipliziert,\nalso \\(P(B|C)\\). Nun sind \\(B\\) und \\(C\\) eingetreten, und auf der nächsten\nStufe multiplizieren wir deshalb mit \\(P(|B\\cap C)\\). Eine andere\nReihenfolge der Bedingungen wäre natürlich ebenfalls möglich, z.B.\n\\[\nP(\\cap B\\cap C)=P()P(B|)P(C|\\cap B).\n\\]","code":""},{"path":"wahrscheinlichkeit.html","id":"total","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.5 Totale Wahrscheinlichkeit","text":"Aus mehreren bedingten Wahrscheinlichkeiten lässt sich eine unbedingte\nWahrscheinlichkeit errechnen. Dazu zerlegen wir den Ergebnisraum \\(\\Omega\\)\neine Partition. Unter einer Partition versteht man eine Zerlegung\ndisjunkte Mengen \\(A_1, A_2, \\ldots, A_n\\), dass die Vereinigungsmenge\nder \\(A_1,\\ldots, A_n\\) wieder \\(\\Omega\\) ergibt. Die “Partitionierung eines\nRinds” könnte etwa aussehen:Das Rind wird also vollständig Teilmengen zerlegt, und alle Teilmengen\nzusammen ergeben wieder das gesamte Rind.Nun nehmen wir , dass für jedes Ereignis \\(A_i\\), \\(=1,\\ldots,n\\), die\nbedingte Wahrscheinlichkeit \\(P(B|A_i\\) gegeben ist, wobei \\(B\\) irgendein\nEreignis ist. Dann gilt für die unbedingte Wahrscheinlichkeit von \\(B\\):\n\\[\nP(B)=\\sum_{=1}^n P(B|A_i)P(A_i).\n\\]\nDie unbedingte Wahrscheinlichkeit ergibt sich also als gewichtete Summe\naller bedingten Wahrscheinlichkeiten, die Gewichtung erfolgt durch die\nWahrscheinlichkeiten der bedingenden Ereignisse.Beispiele:EinsEinsZweiZwei","code":""},{"path":"wahrscheinlichkeit.html","id":"bayes","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.6 Satz von Bayes","text":"Der Satz von Bayes setzt die beiden bedingten Wahrscheinlichkeiten\n\\(P(|B)\\) und \\(P(B|)\\) zueinander Beziehung. Es gilt\n\\[\nP(|B)=\\frac{P(B|)P()}{P(B)}.\n\\]\nUm die Bedingung und das Bedingte zu vertauschen, braucht man also beide\nunbedingte Wahrscheinlichkeiten. Die Herleitung des Satzes von Bayes\nergibt sich aus der Definition der bedingten Wahrscheinlichkeit.\nWegen\n\\[\n\\begin{align*}\nP(|B) &= \\frac{P(\\cap B)}{P(B)}\\\\\nP(B|) &= \\frac{P(\\cap B)}{P()}\n\\end{align*}\n\\]\ngilt\n\\[\nP(|B)P(B)=P(B|)P().\n\\]\nDaraus folgt durch Umstellen unmittelbar der Satz von Bayes.Beispiele:EinsEinsZweiZwei","code":""},{"path":"wahrscheinlichkeit.html","id":"unabhängigkeit","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.7 Unabhängigkeit","text":"Zwei Ereignisse \\(\\) und \\(B\\) heißen unabhängig (oder auch stochastisch\nunabhängig), wenn\n\\[\nP(\\cap B)=P()\\cdot P(B).\n\\]\nEine alternative (fast äquivalente) Definition der Unabhängigkeit lautet :\nDie Ereignisse \\(\\) und \\(B\\) heißen unabhängig, wenn gilt\n\\[\nP(|B)=P()\n\\]\nbzw. wenn\n\\[\nP(B|)=P(B).\n\\]\nDie Definition über die bedingten Wahrscheinlichkeiten ist etwas intuitiver.\nWenn das Wissen darüber, ob \\(\\) eingetreten ist oder nicht, für die\nWahrscheinlichkeit von \\(B\\) keine Rolle spielt (und umgekehrt),\ndann sind \\(\\) und \\(B\\) unabhängig.Achten Sie darauf, Unabhängigkeit nicht mit Disjunktheit zu verwechseln.\nZur Erinnerung: Die Ereignisse \\(\\) und \\(B\\) sind disjunkt, wenn\n\\(\\cap B=\\emptyset\\), dass \\(P(\\cap B)=0\\). Disjunkte Ereignisse\nkönnen also nicht beide zusammen eintreten. Wenn \\(\\) eintritt, kann\nman sicher sein, dass \\(B\\) nicht eintritt. Und wenn \\(B\\) eintritt,\nkann man sicher sein, dass \\(\\) nicht eintritt. Unabhängige Ereignisse\nkönnen hingegen sehr wohl zusammen eintreten.Wie lässt sich die Definition von Unabhängigkeit auf mehr als\nzwei Ereignisse verallgemeinern? Das ist etwas komplizierter, als man es\nvielleicht zunächst vermutet. Intuitiv gesprochen liegt bei nur zwei\nEreignissen (\\(\\) und \\(B\\)) Unabhängigkeit dann vor, wenn Informationen über die\nAusprägung eines Ereignisses die Wahrscheinlichkeit für das andere Ereignis nicht\nverändern. Bei drei Ereignissen (\\(\\), \\(B\\), \\(C\\)) kann es passieren, dass die\nInformation über \\(\\) weder die Wahrscheinlichkeit von \\(B\\) noch die von\n\\(C\\) verändert, dass aber die Information über \\(\\) und \\(B\\) dennoch\ndie Wahrscheinlichkeit von \\(C\\) verändert. Sehen wir uns dazu ein einfaches\nBeispiel :Beispiel:Zwei faire Würfel werden geworfen (mit der üblichen Ergebnismenge). Wir definieren\ndie folgenden drei Ereignisse:\\(\\): Die Augenzahl des ersten Würfels ist ungerade\\(B\\): Die Augenzahl des zweiten Würfels ist ungerade\\(C\\): Die Summe der Augenzahlen ist ungeradeWenn man zwei beliebige dieser Ereignisse auswählt, dann sind sie\nunabhängig voneinander. Das dritte Ereignis ist aber von den beiden\nanderen abhängig, weil die Summe von zwei ungeraden Zahlen niemals\nungerade ist. Weil die Situation bei mehr als zwei Ereignissen\nalso komplexer ist, wird Unabhängigkeit für mehrere Ereignisse\ndefiniert:Seien \\(A_1,\\ldots,A_n\\) Ereignisse. Sie heißen global unabhängig,\nwenn für jedes \\(m\\\\{2,\\ldots,n\\}\\) und alle möglichen\nIndizes \\(1\\le i_1<\\ldots<i_m\\le n\\) gilt\n\\[P(A_{i_1}\\cap\\ldots\\cap A_{i_m})=P(A_{i_1})\\cdot\\ldots\\cdot P(A_{i_m})\\].Die formale Definition ist nicht ganz leicht zu durchschauen. Letztlich\nsagt sie aber einfach aus: Bei globaler Unabhängigkeit ist die Wahrscheinlichkeit,\ndass beliebig viele beliebig ausgewählte Ereignisse gemeinsam eintreten,\ngerade dem Produkt der einzelnen Wahrscheinlichkeiten entspricht.","code":""},{"path":"wahrscheinlichkeit.html","id":"vier-felder-tafeln","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.8 Vier-Felder-Tafeln","text":"Die Vier-Felder-Tafel ist ein besonders einfaches, aber nützliches\nWerkzeug für Fragen zu Wahrscheinlichkeiten, wenn zwei genau Ereignisse\n(und ihre Gegenereignisse) eine Rolle spielen, z.B. \\(\\) und \\(B\\) (und\n\\(\\bar \\) und \\(\\bar B\\)). Jedes der beiden Ereignisse kann eintrefen oder nicht,\ndass es insgesamt vier Kombinationsmöglichkeiten gibt. Die Wahrscheinlichkeiten\ndieser vier Möglichkeiten werden wie folgt angeordnet:\n\\[\n\\begin{array}{c|cc|}\n& & \\bar & \\\\ \\hline\nB & P(\\cap B) & P(\\bar \\cap B)\\\\\n\\bar B & P(\\cap \\bar B) & P(\\bar \\cap \\bar B)\\\\ \\hline\n\\end{array}\n\\]\nDa \\(P((\\cap B)\\cup (\\cap \\bar B))=P()\\) ist (und entsprechend für\ndie anderen Zeilen und Spalten), kann man die Ränder ergänzen:\n\\[\n\\begin{array}{c|cc|c}\n& & \\bar & \\\\ \\hline\nB & P(\\cap B) & P(\\bar \\cap B) & P(B)\\\\\n\\bar B & P(\\cap \\bar B) & P(\\bar \\cap \\bar B) & P(\\bar B)\\\\ \\hline\n& P() & P(\\bar ) & 1\n\\end{array}\n\\]\nWeil die Zeilen und Spalten sich immer zu den Randwerten addieren müssen,\nkann man Felder leicht ergänzen, wenn einer Zeilen oder\nSpalte ein Wert fehlt.man eine Vier-Felder-Tafel ausfüllen, dann ist es wichtig darauf zu\nachten, ob es sich bei den vorgegebenen Wahrscheinlichkeiten um gemeinsame\nWahrscheinlichkeiten (z.B. \\(\\) und \\(B\\) bzw. \\(P(\\cap B)\\)) handelt, oder\num bedingte Wahrscheinlichkeiten (also z.B. \\(P(|B)\\)). Letztere können\nleicht umgeformt werden, wenn man den Zusammenhang\n\\[\nP(\\cap B)=P(|B)P(B)\n\\]\nbeachtet (oder die analogen Zusammenhänge für andere bedingte\nWahrscheinlichkeiten).Beispiele:","code":""},{"path":"wahrscheinlichkeit.html","id":"wahrscheinlichkeitsbäume","chapter":"Kapitel 2 Wahrscheinlichkeit","heading":"2.9 Wahrscheinlichkeitsbäume","text":"Vier-Felder-Tafeln sind ein nützliches Werkzeug, um einfaches\nWahrscheinlichkeitsprobleme zu lösen, denen nur zwei Ereignisse und\nihre Gegenwahrscheinlichkeiten vorkommen. Ein Werkzeug, mit dem man auch\nallgemeine und etwas komplexere Probleme übersichtlich bearbeiten kann,\nsind Wahrscheinlichkeitsbäume. Die Ereignisse - es können hier durch\naus mehr als zwei sein - bilden die Äste des Baums.Beispiel:Definition der Ereignisse:\n\\[\n\\begin{eqnarray*}\n&\\text{:}&\\text{ Die Firma hat Erfolg} \\\\\nB &\\text{:}&\\text{ Der Manager strengt sich }\n\\end{eqnarray*}\n\\]\nGesucht sei die bedingte Wahrscheinlichkeit \\(P(B|)=P(\\cap B)/P()\\).\nFür die Auszahlung von Boni einen Manager ist es wichtig zu wissen,\nob er eine gute Leistung erbracht hat. Wir wollen bestimmen, wie hoch die\nWahrscheinlichkeit ist, dass der Manager sich tatsächlich für den Erfolg\nangestrengt hat, wenn wir beobachten, dass die Firma erfolgreich ist. Oder\nanders gesagt, ist der Erfolg der Firma wirklich ein Indikator für eine\ngute Leistung?Wir nehmen (auch wenn es der Praxis unrealistisch ist, diese\nWerte zu kennen):\n\\[\n\\begin{eqnarray*}\nP\\left( |B\\right) &=&0.8 \\\\\nP(|\\bar{B}) &=&0.5 \\\\\nP\\left( B\\right) &=&0.6\n\\end{eqnarray*}\n\\]\nHier handelt es sich um eine Fragestellung, die auch mit einer Vier-Felder-Tafel\nbeantwortet werden könnte. Trotzdem nutzen wir nun einen Wahrscheinlichkeitsbaum.\nAuf der ersten Ebene verzweigt der Baum die beiden Ereignisse\n\\(B\\) und \\(\\bar B\\), denn wir kennen bedingte Wahrscheinlichkeiten, die diese\nbeiden Ereignisse als Bedingung haben. die beiden Zweige schreiben wir\ndie Ereignisse und ihre Wahrscheinlichkeiten. Die Wahrscheinlichkeit von 0.4\nfür das Ereignis \\(\\bar B\\) ergibt sich aus der Gegenwahrscheinlichkeit von \\(B\\).\nAuf der zweiten Stufe werden nun die bedingten Ereignisse und Wahrscheinlichkeiten\nergänzt, einmal unter der Bedingung, dass \\(B\\) eingetreten ist, einmal unter\nder Bedingung, dass \\(\\bar B\\) eingetreten ist. die neuen Zweige werden\ndie zugehörigen bedingten Wahrscheinlichkeiten geschrieben. Dabei nutzen wir\nwiederum die Gegenwahrscheinlichkeiten der bedingten Ereignisse.\n\nDie Wahrscheinlichkeiten für \\(\\cap B\\), \\(\\cap \\bar B\\), \\(\\bar \\cap B\\) und\n\\(\\bar \\cap\\bar b\\) ergeben sich nun, indem man entlang der Äste die\nWahrscheinlichkeiten multipliziert. Man erhält\n\\[\n\\begin{align*}\nP(\\cap B)&=0.6\\cdot 0.8 = 0.48\\\\\nP(\\bar \\cap B)&=0.6\\cdot 0.2 = 0.12\\\\\nP(\\cap \\bar B)&=0.4\\cdot 0.5 = 0.2\\\\\nP(\\bar \\cap \\bar B)&=0.4\\cdot 0.5 = 0.2\n\\end{align*}\n\\]\nDie (unbedingten) Wahrscheinlichkeiten \\(P()\\) und \\(P(\\bar )\\) ergeben sich,\nindem man die Wahrscheinlichkeiten aller Pfade aufaddiert, die einen \\(\\)-Ast\n(bzw. einen \\(\\bar \\)-Ast) haben. Es ergibt sich \\(P()=0.68\\) und\n\\(P(\\bar )=0.32\\).","code":"\nplot(c(0,1),c(0,1),t=\"n\",xlab=\"\",ylab=\"\",axes=F)\npoints(c(0,0.4,0.4),c(0.5,0.2,0.8))\nlines(c(0.4,0,0.4),c(0.2,0.5,0.8))\ntext(0.2,0.7,expression(B))\ntext(0.35,0.83,0.6)\ntext(0.35,0.17,0.4)\ntext(0.2,0.29,expression(bar(B)))"},{"path":"zufallsvariablen.html","id":"zufallsvariablen","chapter":"Kapitel 3 Zufallsvariablen","heading":"Kapitel 3 Zufallsvariablen","text":"","code":""},{"path":"zufallsvariablen.html","id":"verteilungsfunktion","chapter":"Kapitel 3 Zufallsvariablen","heading":"3.1 Verteilungsfunktion","text":"","code":""},{"path":"zufallsvariablen.html","id":"quantilfunktion","chapter":"Kapitel 3 Zufallsvariablen","heading":"3.2 Quantilfunktion","text":"","code":""},{"path":"zufallsvariablen.html","id":"erwartungswert","chapter":"Kapitel 3 Zufallsvariablen","heading":"3.3 Erwartungswert","text":"","code":""},{"path":"zufallsvariablen.html","id":"varianz","chapter":"Kapitel 3 Zufallsvariablen","heading":"3.4 Varianz","text":"","code":""},{"path":"zufallsvektoren.html","id":"zufallsvektoren","chapter":"Kapitel 4 Zufallsvektoren","heading":"Kapitel 4 Zufallsvektoren","text":"","code":""},{"path":"zufallsvektoren.html","id":"verteilungsfunktion-1","chapter":"Kapitel 4 Zufallsvektoren","heading":"4.1 Verteilungsfunktion","text":"","code":""},{"path":"zufallsvektoren.html","id":"bedingte-verteilungen","chapter":"Kapitel 4 Zufallsvektoren","heading":"4.2 Bedingte Verteilungen","text":"","code":""},{"path":"zufallsvektoren.html","id":"kovarianz","chapter":"Kapitel 4 Zufallsvektoren","heading":"4.3 Kovarianz","text":"","code":""},{"path":"stichproben.html","id":"stichproben","chapter":"Kapitel 5 Stichproben","heading":"Kapitel 5 Stichproben","text":"","code":""},{"path":"punktschaetzung.html","id":"punktschaetzung","chapter":"Kapitel 6 Punktschätzung","heading":"Kapitel 6 Punktschätzung","text":"","code":""},{"path":"intervallschaetzung.html","id":"intervallschaetzung","chapter":"Kapitel 7 Intervallschätzung","heading":"Kapitel 7 Intervallschätzung","text":"","code":""},{"path":"hypothesentests.html","id":"hypothesentests","chapter":"Kapitel 8 Hypothesentests","heading":"Kapitel 8 Hypothesentests","text":"","code":""}]
